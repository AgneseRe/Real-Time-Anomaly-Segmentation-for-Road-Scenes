{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgneseRe/Real-Time-Anomaly-Segmentation-for-Road-Scenes/blob/main/AML_AnomalySegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjyXaByVOtbd"
      },
      "source": [
        "# **Real-time Anomaly Segmentation for Road Scenes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpweY-zTOyvF"
      },
      "source": [
        "Existing deep neural networks, when deployed in open-world settings, perform poorly on unknown, anomaly, out-of-distribution (OoD) objects that were not present during the training. The goal of this project is to build tiny anomaly segmentation models to segment anomaly patterns. Models must be able to fit in small devices, which represents a realistic memory constraint for an edge application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxuFk3f4uqwU"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SOBJKZAfEwAZ"
      },
      "outputs": [],
      "source": [
        "!rm -r sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wed7tVmrP222",
        "outputId": "388f232d-dd55-416d-a018-d6d448434aec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.6/473.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# download required packages and import useful modules\n",
        "!pip3 install --quiet numpy\n",
        "!pip3 install --quiet Pillow\n",
        "\n",
        "!pip3 install --quiet gdown\n",
        "!pip3 install --quiet torchvision\n",
        "!pip3 install --quiet ood_metrics\n",
        "!pip3 install --quiet cityscapesscripts\n",
        "\n",
        "!pip3 install --quiet matplotlib\n",
        "!pip3 install --quiet visdom\n",
        "\n",
        "import os, sys, subprocess, torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aha0ydTyO4IR"
      },
      "source": [
        "The following function is implemented to download the *Cityscapes* dataset in two different ways: via Google Drive (using `gdown`) or directly from the Cityscapes official website (using `csDownload`). Although the first option is preferable as it is definitely faster, direct download from the website is provided as an alternative. `gdown` may in fact raise the error *Failed to retrieve the file url* if the file we are attempting to download is exceptionally large (*e.g.* 11G), there are numerous users simultaneously trying to download it programmatically or we download it many times in a limited time. Regardless of the method used, use the conversor (available [here](https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/preparation/createTrainIdLabelImgs.py)) to generate labelTrainIds from labelIds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-cQRn0hfGLlV"
      },
      "outputs": [],
      "source": [
        "def download_cityscapes():\n",
        "\n",
        "    if not os.path.isdir('/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/cityscapes'):\n",
        "        print(\"Attempting to download cityscapes dataset using gdown...\")\n",
        "\n",
        "        try:\n",
        "            # If check is true, and the process exits with a non-zero exit code, a CalledProcessError exception will be raised.\n",
        "            subprocess.run([\"gdown\", \"https://drive.google.com/uc?id=11gSQ9UcLCnIqmY7srG2S6EVwV3paOMEq\"], check=True)\n",
        "            print(\"Dataset downloaded successfully using gdown. Unzipping...\")\n",
        "            subprocess.run([\"unzip\", \"-q\", \"cityscapes.zip\"], check=True)\n",
        "            # Use the conversor to generate labelTrainIds from labelIds\n",
        "            print(\"Generating trainIds from labelIds...\")\n",
        "            !CITYSCAPES_DATASET='cityscapes/' csCreateTrainIdLabelImgs\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(\"gdown failed. Attempting to download cityscapes dataset from the official website...\")\n",
        "            try:\n",
        "              # Cityscapes credentials: (agnesere, FCSBwcVMi-u9-Zn)\n",
        "              !csDownload leftImg8bit_trainvaltest.zip\n",
        "              !csDownload gtFine_trainvaltest.zip\n",
        "\n",
        "              print(\"Dataset downloaded successfully from the official website. Unzipping...\")\n",
        "              !unzip -q 'leftImg8bit_trainvaltest.zip' -d 'cityscapes'\n",
        "              !unzip -o -q 'gtFine_trainvaltest.zip' -d 'cityscapes'\n",
        "\n",
        "              print(\"Generating trainIds from labelIds...\")\n",
        "              !CITYSCAPES_DATASET='cityscapes/' csCreateTrainIdLabelImgs\n",
        "\n",
        "              print(\"Cityscapes dataset ready\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(\"Failed to download the dataset using both methods.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFHf3bXySHFf"
      },
      "source": [
        "Download and unzip the validation dataset (*FS_LostFound_full*, *RoadAnomaly*, *RoadAnomaly21*, *RoadObsticle21*, *fs_static*), clone or update the GitHub repository (*Real-Time-Anomaly-Segmentation-for-Road-Scenes*) and download the *Cityscapes* dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Bl3WGlWfon",
        "outputId": "e9fd8280-7da5-4c80-fc7a-f86801b1c57a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
            "From (redirected): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta&confirm=t&uuid=ad5b1b1c-32f6-4a19-8df3-d3c28ce3f694\n",
            "To: /content/Validation_Dataset.zip\n",
            "100% 329M/329M [00:05<00:00, 55.9MB/s]\n",
            "Cloning into 'Real-Time-Anomaly-Segmentation-for-Road-Scenes'...\n",
            "remote: Enumerating objects: 946, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 946 (delta 59), reused 66 (delta 45), pack-reused 854 (from 3)\u001b[K\n",
            "Receiving objects: 100% (946/946), 280.24 MiB | 21.85 MiB/s, done.\n",
            "Resolving deltas: 100% (539/539), done.\n",
            "Updating files: 100% (140/140), done.\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes\n"
          ]
        }
      ],
      "source": [
        "# download and unzip validation dataset\n",
        "if not os.path.isdir('/content/validation_dataset'):\n",
        "  !gdown 'https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta'\n",
        "  !unzip -q 'Validation_Dataset.zip'\n",
        "  !mkdir validation_dataset && cp -pR Validation_Dataset/* validation_dataset/ && rm -R Validation_Dataset/\n",
        "  !rm 'Validation_Dataset.zip'\n",
        "\n",
        "# clone the github repo and pull command\n",
        "if not os.path.isdir('content/Real-Time-Anomaly-Segmentation-for-Road-Scenes'):\n",
        "  !git clone https://github.com/AgneseRe/Real-Time-Anomaly-Segmentation-for-Road-Scenes.git\n",
        "else: # if folder already present\n",
        "  !git pull\n",
        "\n",
        "%cd Real-Time-Anomaly-Segmentation-for-Road-Scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtojfyyxIpaw",
        "outputId": "40c7c725-799b-4383-bf7e-b9b8720c9de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download cityscapes dataset using gdown...\n",
            "Dataset downloaded successfully using gdown. Unzipping...\n",
            "Generating trainIds from labelIds...\n",
            "Processing 5000 annotation files\n",
            "Progress: 100.0 % "
          ]
        }
      ],
      "source": [
        "# download cityscapes dataset\n",
        "download_cityscapes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_czbWkaraAdh"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smRtjvZQu0R4"
      },
      "source": [
        "### Step 2A\n",
        "### Compute AuPRC & FPR95TPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9ewyGcSZ__2",
        "outputId": "34ad7ea5-e2dc-4dc5-a35e-862c154744ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval\n"
          ]
        }
      ],
      "source": [
        "%cd eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MdLxJaGP1FyY"
      },
      "outputs": [],
      "source": [
        "# datasets = os.listdir(\"../../validation_dataset\")\n",
        "datasets = {\n",
        "    \"SMIYC RA-21\": \"RoadAnomaly21\",\n",
        "    \"SMIYC RO-21\": \"RoadObsticle21\",\n",
        "    \"FS L&F\": \"FS_LostFound_full\",\n",
        "    \" FS Static\": \"fs_static\",\n",
        "    \"Road Anomaly\": \"RoadAnomaly\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOPyLTmNxhR"
      },
      "source": [
        "Perform various anomaly inferences using the pre-trained **ErfNet** model and anomaly segmentation test dataset provided. Different techniques are used (MSP, MaxLogit and MaxEntropy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoH7P5ukYN8",
        "outputId": "aae1ced0-7f6a-48d8-8a62-5614bfcfaa84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 29.100 | FPR@TPR95: 62.511\n",
            " - MaxLogit   | AUPRC score: 38.320 | FPR@TPR95: 59.337\n",
            " - MaxEntropy | AUPRC score: 31.005 | FPR@TPR95: 62.593\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 2.712 | FPR@TPR95: 64.974\n",
            " - MaxLogit   | AUPRC score: 4.627 | FPR@TPR95: 48.443\n",
            " - MaxEntropy | AUPRC score: 3.052 | FPR@TPR95: 65.600\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score: 1.748 | FPR@TPR95: 50.763\n",
            " - MaxLogit   | AUPRC score: 3.301 | FPR@TPR95: 45.495\n",
            " - MaxEntropy | AUPRC score: 2.582 | FPR@TPR95: 50.368\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score: 7.470 | FPR@TPR95: 41.823\n",
            " - MaxLogit   | AUPRC score: 9.499 | FPR@TPR95: 40.300\n",
            " - MaxEntropy | AUPRC score: 8.826 | FPR@TPR95: 41.523\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 12.426 | FPR@TPR95: 82.492\n",
            " - MaxLogit   | AUPRC score: 15.582 | FPR@TPR95: 73.248\n",
            " - MaxEntropy | AUPRC score: 12.678 | FPR@TPR95: 82.632\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "methods = [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]\n",
        "\n",
        "for dataset, folder in datasets.items():\n",
        "  print(f\"Dataset {dataset}\")\n",
        "\n",
        "  for method in methods:\n",
        "    print(f\" - {method:<10} \", end = \"\")\n",
        "    input_path = f\"../../validation_dataset/{folder}/images/*.*\"\n",
        "    plot_dir_path = f\"../plots/baselines/{folder}_{method}\"\n",
        "    if torch.cuda.is_available():\n",
        "      !python evalAnomaly.py --input={input_path} --method={method} --plotdir={plot_dir_path}\n",
        "    else:\n",
        "      !python evalAnomaly.py --input={input_path} --method={method} --plotdir={plot_dir_path} --cpu\n",
        "\n",
        "  print(\"=\" * 55, end = \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vXNPPsR8YtJ"
      },
      "source": [
        "If you want to save the baselines folder in your local machine, create a ZIP file with the following command and then download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDemCzdK8WJh"
      },
      "outputs": [],
      "source": [
        " # !zip -r baselines.zip baselines/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UaIqk89NPw3"
      },
      "source": [
        "### Compute mIoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1AKrBWFNc_Y",
        "outputId": "019224a8-d724-420a-9d97-bf4950ce81bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/cityscapes/leftImg8bit/val /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/cityscapes/gtFine/val\n",
            "---------------------------------------\n",
            "Took  78.16403198242188 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.31\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.96\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.40\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  !python eval_iou.py --loadDir ../trained_models/ --datadir /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/cityscapes\n",
        "else:\n",
        "  !python eval_iou.py --loadDir ../trained_models/ --datadir /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/cityscapes --cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJodGToyvAwX"
      },
      "source": [
        "### Step 2B - Compute AuPRC & FPR95TPR with temperature scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5oNs1EqDL6B",
        "outputId": "a81273d6-0076-4cbc-9198-3af1b3b3ba4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset SMIYC RA-21\n",
            " - 0.5        | AUPRC score: 27.061 | FPR@TPR95: 62.731\n",
            " - 0.75       | AUPRC score: 28.156 | FPR@TPR95: 62.479\n",
            " - 1.0        | AUPRC score: 29.100 | FPR@TPR95: 62.511\n",
            " - 1.1        | AUPRC score: 29.410 | FPR@TPR95: 62.590\n",
            " - 1.2        | AUPRC score: 29.678 | FPR@TPR95: 62.724\n",
            " - 1.5        | AUPRC score: 30.258 | FPR@TPR95: 63.318\n",
            " - 2.0        | AUPRC score: 30.679 | FPR@TPR95: 64.721\n",
            " - 5.0        | AUPRC score: 30.196 | FPR@TPR95: 71.594\n",
            " - 10.0       | AUPRC score: 29.526 | FPR@TPR95: 75.757\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - 0.5        | AUPRC score: 2.420 | FPR@TPR95: 63.225\n",
            " - 0.75       | AUPRC score: 2.567 | FPR@TPR95: 64.053\n",
            " - 1.0        | AUPRC score: 2.712 | FPR@TPR95: 64.974\n",
            " - 1.1        | AUPRC score: 2.766 | FPR@TPR95: 65.524\n",
            " - 1.2        | AUPRC score: 2.816 | FPR@TPR95: 66.033\n",
            " - 1.5        | AUPRC score: 2.937 | FPR@TPR95: 67.928\n",
            " - 2.0        | AUPRC score: 3.026 | FPR@TPR95: 71.459\n",
            " - 5.0        | AUPRC score: 2.841 | FPR@TPR95: 83.111\n",
            " - 10.0       | AUPRC score: 2.644 | FPR@TPR95: 88.146\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - 0.5        | AUPRC score: 1.280 | FPR@TPR95: 66.737\n",
            " - 0.75       | AUPRC score: 1.493 | FPR@TPR95: 51.848\n",
            " - 1.0        | AUPRC score: 1.748 | FPR@TPR95: 50.763\n",
            " - 1.1        | AUPRC score: 1.860 | FPR@TPR95: 50.387\n",
            " - 1.2        | AUPRC score: 1.972 | FPR@TPR95: 50.150\n",
            " - 1.5        | AUPRC score: 2.286 | FPR@TPR95: 49.456\n",
            " - 2.0        | AUPRC score: 2.677 | FPR@TPR95: 48.324\n",
            " - 5.0        | AUPRC score: 3.252 | FPR@TPR95: 45.396\n",
            " - 10.0       | AUPRC score: 3.344 | FPR@TPR95: 44.121\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - 0.5        | AUPRC score: 6.601 | FPR@TPR95: 43.476\n",
            " - 0.75       | AUPRC score: 6.991 | FPR@TPR95: 42.493\n",
            " - 1.0        | AUPRC score: 7.470 | FPR@TPR95: 41.823\n",
            " - 1.1        | AUPRC score: 7.687 | FPR@TPR95: 41.587\n",
            " - 1.2        | AUPRC score: 7.910 | FPR@TPR95: 41.406\n",
            " - 1.5        | AUPRC score: 8.580 | FPR@TPR95: 41.096\n",
            " - 2.0        | AUPRC score: 9.508 | FPR@TPR95: 41.018\n",
            " - 5.0        | AUPRC score: 11.175 | FPR@TPR95: 45.514\n",
            " - 10.0       | AUPRC score: 11.462 | FPR@TPR95: 51.164\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - 0.5        | AUPRC score: 12.188 | FPR@TPR95: 82.022\n",
            " - 0.75       | AUPRC score: 12.319 | FPR@TPR95: 82.285\n",
            " - 1.0        | AUPRC score: 12.426 | FPR@TPR95: 82.492\n",
            " - 1.1        | AUPRC score: 12.466 | FPR@TPR95: 82.621\n",
            " - 1.2        | AUPRC score: 12.502 | FPR@TPR95: 82.757\n",
            " - 1.5        | AUPRC score: 12.591 | FPR@TPR95: 83.244\n",
            " - 2.0        | AUPRC score: 12.676 | FPR@TPR95: 84.150\n",
            " - 5.0        | AUPRC score: 12.672 | FPR@TPR95: 87.713\n",
            " - 10.0       | AUPRC score: 12.575 | FPR@TPR95: 89.477\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "temperatures = [0.5, 0.75, 1.0, 1.1, 1.2, 1.5, 2.0, 5.0, 10.0]\n",
        "\n",
        "for dataset, folder in datasets.items():\n",
        "  print(f\"Dataset {dataset}\")\n",
        "\n",
        "  for temperature in temperatures:\n",
        "    print(f\" - {temperature:<10} \", end = \"\")\n",
        "    input_path = f\"../../validation_dataset/{folder}/images/*.*\"\n",
        "    # plot_dir_path = f\"../plots/temperature/{folder}_MSP\"\n",
        "    if torch.cuda.is_available():\n",
        "      !python evalAnomaly.py --input={input_path} --method=\"MSP\" --temperature={temperature}\n",
        "    else:\n",
        "      !python evalAnomaly.py --input={input_path} --method=\"MSP\" --temperature={temperature} --cpu\n",
        "\n",
        "  print(\"=\" * 55, end = \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ylMHF05vI-a"
      },
      "source": [
        "### Step 3 - Train models with void classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"../train\"\n",
        "data_dir = \"../cityscapes\""
      ],
      "metadata": {
        "id": "xx43qM7vVoGk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uvJ8DqNPGxg3"
      },
      "outputs": [],
      "source": [
        "def train_model(model: str, num_epochs: int, batch_size: int, stop_epoch: int, pretrained: bool = False, resume: bool = False) -> None:\n",
        "  # if model == \"bisenet\":\n",
        "  #     !gdown \"https://drive.usercontent.google.com/download?id=1Gj4eZrmdygA5c_y7N0KrmSRThoYjfjk-\" -O \"checkpoint.pth.tar\"\n",
        "  # dict_keys(['epoch', 'arch', 'state_dict', 'best_acc', 'optimizer'])\n",
        "\n",
        "  state_flag = f\"--state ../trained_models/custom/{model}_pretrained.pth\" if pretrained else \"\"\n",
        "  resume_flag = \"--resume\" if resume else \"\"\n",
        "\n",
        "  !cd {base_dir} && python -W ignore main_v2.py \\\n",
        "    --savedir {model}_training_void \\\n",
        "    --datadir {data_dir} \\\n",
        "    --model {model} \\\n",
        "    --cuda \\\n",
        "    --num-epochs={num_epochs} \\\n",
        "    --epochs-save=1 \\\n",
        "    --batch-size={batch_size} \\\n",
        "    --stop-epoch={stop_epoch} \\\n",
        "    {state_flag} \\\n",
        "    {resume_flag}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"bisenet\", 20, 6) # BiSeNet 20 epoch 6 batch_size\n",
        "train_model(\"bisenet\", 20, 6, pretrained=True) # BiSeNet 20 epoch 6 batch_size with pretrained weights\n",
        "train_model(\"bisenet\", 20, 6, resume=True) # BiSeNet 20 epoch 6 batch_size with resume training\n",
        "train_model(\"bisenet\", 20, 6, pretrained=True, resume=True) # BiSeNet 20 epoch 6 batch_size with pretrained weights and resume training"
      ],
      "metadata": {
        "id": "SyIIKcbuV0Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxoP9g7BF45G"
      },
      "outputs": [],
      "source": [
        "!cd {base_dir} && python -W ignore main_v2.py --savedir \"bisenet_training_void_1\" --datadir {data_dir} --model \"bisenet\" --cuda --num-epochs=20 --epochs-save=1 --batch-size=6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ERFNet Fine-tuning"
      ],
      "metadata": {
        "id": "jMpBXEAotIdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd {base_dir} && python -W ignore main_v2.py --savedir \"erfnet_training_void\" --datadir {data_dir} --model \"erfnet\" --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --loadWeights=\"erfnet_pretrained.pth\""
      ],
      "metadata": {
        "id": "kSucMOVKQmBN",
        "outputId": "5467e1b3-a665-4c5e-f9f2-f604fc9c2d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "ERFNet criterion: <class 'utils.losses.ce_loss.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 0.3277 (epoch: 1, step: 0) // Avg time/img: 0.1345 s\n",
            "loss: 0.3739 (epoch: 1, step: 50) // Avg time/img: 0.0380 s\n",
            "loss: 0.3833 (epoch: 1, step: 100) // Avg time/img: 0.0370 s\n",
            "loss: 0.3838 (epoch: 1, step: 150) // Avg time/img: 0.0369 s\n",
            "loss: 0.3784 (epoch: 1, step: 200) // Avg time/img: 0.0372 s\n",
            "loss: 0.3808 (epoch: 1, step: 250) // Avg time/img: 0.0374 s\n",
            "loss: 0.3803 (epoch: 1, step: 300) // Avg time/img: 0.0374 s\n",
            "loss: 0.3776 (epoch: 1, step: 350) // Avg time/img: 0.0374 s\n",
            "loss: 0.3748 (epoch: 1, step: 400) // Avg time/img: 0.0374 s\n",
            "loss: 0.3732 (epoch: 1, step: 450) // Avg time/img: 0.0375 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.4784 (epoch: 1, step: 0) // Avg time/img: 0.0324 s\n",
            "VAL loss: 0.5761 (epoch: 1, step: 50) // Avg time/img: 0.0337 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.82\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 0.4813 (epoch: 2, step: 0) // Avg time/img: 0.0455 s\n",
            "loss: 0.355 (epoch: 2, step: 50) // Avg time/img: 0.0376 s\n",
            "loss: 0.3503 (epoch: 2, step: 100) // Avg time/img: 0.0377 s\n",
            "loss: 0.3499 (epoch: 2, step: 150) // Avg time/img: 0.0376 s\n",
            "loss: 0.3419 (epoch: 2, step: 200) // Avg time/img: 0.0375 s\n",
            "loss: 0.3443 (epoch: 2, step: 250) // Avg time/img: 0.0376 s\n",
            "loss: 0.3406 (epoch: 2, step: 300) // Avg time/img: 0.0376 s\n",
            "loss: 0.3371 (epoch: 2, step: 350) // Avg time/img: 0.0376 s\n",
            "loss: 0.3337 (epoch: 2, step: 400) // Avg time/img: 0.0376 s\n",
            "loss: 0.3311 (epoch: 2, step: 450) // Avg time/img: 0.0376 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.4423 (epoch: 2, step: 0) // Avg time/img: 0.0346 s\n",
            "VAL loss: 0.5238 (epoch: 2, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.75\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 0.435 (epoch: 3, step: 0) // Avg time/img: 0.0394 s\n",
            "loss: 0.3073 (epoch: 3, step: 50) // Avg time/img: 0.0380 s\n",
            "loss: 0.3006 (epoch: 3, step: 100) // Avg time/img: 0.0378 s\n",
            "loss: 0.3004 (epoch: 3, step: 150) // Avg time/img: 0.0378 s\n",
            "loss: 0.2987 (epoch: 3, step: 200) // Avg time/img: 0.0379 s\n",
            "loss: 0.3011 (epoch: 3, step: 250) // Avg time/img: 0.0378 s\n",
            "loss: 0.2969 (epoch: 3, step: 300) // Avg time/img: 0.0378 s\n",
            "loss: 0.2954 (epoch: 3, step: 350) // Avg time/img: 0.0377 s\n",
            "loss: 0.2941 (epoch: 3, step: 400) // Avg time/img: 0.0377 s\n",
            "loss: 0.2913 (epoch: 3, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.4015 (epoch: 3, step: 0) // Avg time/img: 0.0347 s\n",
            "VAL loss: 0.4804 (epoch: 3, step: 50) // Avg time/img: 0.0337 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.48\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 0.2747 (epoch: 4, step: 0) // Avg time/img: 0.0432 s\n",
            "loss: 0.2677 (epoch: 4, step: 50) // Avg time/img: 0.0380 s\n",
            "loss: 0.2657 (epoch: 4, step: 100) // Avg time/img: 0.0380 s\n",
            "loss: 0.2683 (epoch: 4, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2687 (epoch: 4, step: 200) // Avg time/img: 0.0378 s\n",
            "loss: 0.2652 (epoch: 4, step: 250) // Avg time/img: 0.0379 s\n",
            "loss: 0.2644 (epoch: 4, step: 300) // Avg time/img: 0.0378 s\n",
            "loss: 0.2618 (epoch: 4, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2589 (epoch: 4, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2588 (epoch: 4, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.3684 (epoch: 4, step: 0) // Avg time/img: 0.0454 s\n",
            "VAL loss: 0.4525 (epoch: 4, step: 50) // Avg time/img: 0.0339 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.51\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 0.2267 (epoch: 5, step: 0) // Avg time/img: 0.0374 s\n",
            "loss: 0.2425 (epoch: 5, step: 50) // Avg time/img: 0.0383 s\n",
            "loss: 0.2433 (epoch: 5, step: 100) // Avg time/img: 0.0381 s\n",
            "loss: 0.2429 (epoch: 5, step: 150) // Avg time/img: 0.0380 s\n",
            "loss: 0.2431 (epoch: 5, step: 200) // Avg time/img: 0.0378 s\n",
            "loss: 0.2424 (epoch: 5, step: 250) // Avg time/img: 0.0379 s\n",
            "loss: 0.2408 (epoch: 5, step: 300) // Avg time/img: 0.0379 s\n",
            "loss: 0.2428 (epoch: 5, step: 350) // Avg time/img: 0.0379 s\n",
            "loss: 0.244 (epoch: 5, step: 400) // Avg time/img: 0.0379 s\n",
            "loss: 0.2423 (epoch: 5, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.352 (epoch: 5, step: 0) // Avg time/img: 0.0316 s\n",
            "VAL loss: 0.4335 (epoch: 5, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.60\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 0.2844 (epoch: 6, step: 0) // Avg time/img: 0.0477 s\n",
            "loss: 0.2347 (epoch: 6, step: 50) // Avg time/img: 0.0377 s\n",
            "loss: 0.235 (epoch: 6, step: 100) // Avg time/img: 0.0376 s\n",
            "loss: 0.2303 (epoch: 6, step: 150) // Avg time/img: 0.0378 s\n",
            "loss: 0.2329 (epoch: 6, step: 200) // Avg time/img: 0.0377 s\n",
            "loss: 0.2326 (epoch: 6, step: 250) // Avg time/img: 0.0378 s\n",
            "loss: 0.232 (epoch: 6, step: 300) // Avg time/img: 0.0377 s\n",
            "loss: 0.232 (epoch: 6, step: 350) // Avg time/img: 0.0376 s\n",
            "loss: 0.2309 (epoch: 6, step: 400) // Avg time/img: 0.0376 s\n",
            "loss: 0.2312 (epoch: 6, step: 450) // Avg time/img: 0.0376 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.3293 (epoch: 6, step: 0) // Avg time/img: 0.0355 s\n",
            "VAL loss: 0.427 (epoch: 6, step: 50) // Avg time/img: 0.0336 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.38\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.206 (epoch: 7, step: 0) // Avg time/img: 0.0528 s\n",
            "loss: 0.2233 (epoch: 7, step: 50) // Avg time/img: 0.0386 s\n",
            "loss: 0.2282 (epoch: 7, step: 100) // Avg time/img: 0.0382 s\n",
            "loss: 0.2281 (epoch: 7, step: 150) // Avg time/img: 0.0382 s\n",
            "loss: 0.2276 (epoch: 7, step: 200) // Avg time/img: 0.0380 s\n",
            "loss: 0.2275 (epoch: 7, step: 250) // Avg time/img: 0.0380 s\n",
            "loss: 0.2274 (epoch: 7, step: 300) // Avg time/img: 0.0379 s\n",
            "loss: 0.2272 (epoch: 7, step: 350) // Avg time/img: 0.0379 s\n",
            "loss: 0.2274 (epoch: 7, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2263 (epoch: 7, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.3464 (epoch: 7, step: 0) // Avg time/img: 0.0346 s\n",
            "VAL loss: 0.4162 (epoch: 7, step: 50) // Avg time/img: 0.0341 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.59\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 0.2091 (epoch: 8, step: 0) // Avg time/img: 0.0460 s\n",
            "loss: 0.2252 (epoch: 8, step: 50) // Avg time/img: 0.0386 s\n",
            "loss: 0.2217 (epoch: 8, step: 100) // Avg time/img: 0.0381 s\n",
            "loss: 0.2197 (epoch: 8, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2211 (epoch: 8, step: 200) // Avg time/img: 0.0378 s\n",
            "loss: 0.2231 (epoch: 8, step: 250) // Avg time/img: 0.0378 s\n",
            "loss: 0.2235 (epoch: 8, step: 300) // Avg time/img: 0.0378 s\n",
            "loss: 0.2225 (epoch: 8, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2218 (epoch: 8, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2226 (epoch: 8, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.3086 (epoch: 8, step: 0) // Avg time/img: 0.0313 s\n",
            "VAL loss: 0.4065 (epoch: 8, step: 50) // Avg time/img: 0.0340 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.19\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.1442 (epoch: 9, step: 0) // Avg time/img: 0.0461 s\n",
            "loss: 0.2164 (epoch: 9, step: 50) // Avg time/img: 0.0376 s\n",
            "loss: 0.22 (epoch: 9, step: 100) // Avg time/img: 0.0374 s\n",
            "loss: 0.2211 (epoch: 9, step: 150) // Avg time/img: 0.0375 s\n",
            "loss: 0.222 (epoch: 9, step: 200) // Avg time/img: 0.0375 s\n",
            "loss: 0.222 (epoch: 9, step: 250) // Avg time/img: 0.0374 s\n",
            "loss: 0.2213 (epoch: 9, step: 300) // Avg time/img: 0.0375 s\n",
            "loss: 0.22 (epoch: 9, step: 350) // Avg time/img: 0.0375 s\n",
            "loss: 0.2206 (epoch: 9, step: 400) // Avg time/img: 0.0375 s\n",
            "loss: 0.2199 (epoch: 9, step: 450) // Avg time/img: 0.0376 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.3396 (epoch: 9, step: 0) // Avg time/img: 0.0433 s\n",
            "VAL loss: 0.4066 (epoch: 9, step: 50) // Avg time/img: 0.0342 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.78\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 0.1874 (epoch: 10, step: 0) // Avg time/img: 0.0387 s\n",
            "loss: 0.2153 (epoch: 10, step: 50) // Avg time/img: 0.0377 s\n",
            "loss: 0.2154 (epoch: 10, step: 100) // Avg time/img: 0.0377 s\n",
            "loss: 0.215 (epoch: 10, step: 150) // Avg time/img: 0.0378 s\n",
            "loss: 0.2151 (epoch: 10, step: 200) // Avg time/img: 0.0379 s\n",
            "loss: 0.2161 (epoch: 10, step: 250) // Avg time/img: 0.0380 s\n",
            "loss: 0.2155 (epoch: 10, step: 300) // Avg time/img: 0.0380 s\n",
            "loss: 0.2158 (epoch: 10, step: 350) // Avg time/img: 0.0380 s\n",
            "loss: 0.2148 (epoch: 10, step: 400) // Avg time/img: 0.0379 s\n",
            "loss: 0.2151 (epoch: 10, step: 450) // Avg time/img: 0.0380 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.3279 (epoch: 10, step: 0) // Avg time/img: 0.0443 s\n",
            "VAL loss: 0.4018 (epoch: 10, step: 50) // Avg time/img: 0.0339 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.64\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.185 (epoch: 11, step: 0) // Avg time/img: 0.0386 s\n",
            "loss: 0.2135 (epoch: 11, step: 50) // Avg time/img: 0.0384 s\n",
            "loss: 0.2147 (epoch: 11, step: 100) // Avg time/img: 0.0381 s\n",
            "loss: 0.2139 (epoch: 11, step: 150) // Avg time/img: 0.0380 s\n",
            "loss: 0.2148 (epoch: 11, step: 200) // Avg time/img: 0.0380 s\n",
            "loss: 0.216 (epoch: 11, step: 250) // Avg time/img: 0.0379 s\n",
            "loss: 0.2147 (epoch: 11, step: 300) // Avg time/img: 0.0379 s\n",
            "loss: 0.2147 (epoch: 11, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2152 (epoch: 11, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2149 (epoch: 11, step: 450) // Avg time/img: 0.0377 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.3262 (epoch: 11, step: 0) // Avg time/img: 0.0352 s\n",
            "VAL loss: 0.3957 (epoch: 11, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.96\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-011.pth (epoch: 11)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 0.1922 (epoch: 12, step: 0) // Avg time/img: 0.0453 s\n",
            "loss: 0.2089 (epoch: 12, step: 50) // Avg time/img: 0.0379 s\n",
            "loss: 0.2085 (epoch: 12, step: 100) // Avg time/img: 0.0380 s\n",
            "loss: 0.211 (epoch: 12, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2115 (epoch: 12, step: 200) // Avg time/img: 0.0379 s\n",
            "loss: 0.2103 (epoch: 12, step: 250) // Avg time/img: 0.0379 s\n",
            "loss: 0.2118 (epoch: 12, step: 300) // Avg time/img: 0.0378 s\n",
            "loss: 0.2121 (epoch: 12, step: 350) // Avg time/img: 0.0377 s\n",
            "loss: 0.212 (epoch: 12, step: 400) // Avg time/img: 0.0377 s\n",
            "loss: 0.2124 (epoch: 12, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.3124 (epoch: 12, step: 0) // Avg time/img: 0.0363 s\n",
            "VAL loss: 0.4002 (epoch: 12, step: 50) // Avg time/img: 0.0333 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.79\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.2475 (epoch: 13, step: 0) // Avg time/img: 0.0482 s\n",
            "loss: 0.2097 (epoch: 13, step: 50) // Avg time/img: 0.0383 s\n",
            "loss: 0.2112 (epoch: 13, step: 100) // Avg time/img: 0.0383 s\n",
            "loss: 0.2114 (epoch: 13, step: 150) // Avg time/img: 0.0380 s\n",
            "loss: 0.2105 (epoch: 13, step: 200) // Avg time/img: 0.0379 s\n",
            "loss: 0.2109 (epoch: 13, step: 250) // Avg time/img: 0.0379 s\n",
            "loss: 0.2113 (epoch: 13, step: 300) // Avg time/img: 0.0378 s\n",
            "loss: 0.212 (epoch: 13, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2116 (epoch: 13, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2118 (epoch: 13, step: 450) // Avg time/img: 0.0379 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.3242 (epoch: 13, step: 0) // Avg time/img: 0.0347 s\n",
            "VAL loss: 0.3928 (epoch: 13, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.64\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 0.2718 (epoch: 14, step: 0) // Avg time/img: 0.0410 s\n",
            "loss: 0.2124 (epoch: 14, step: 50) // Avg time/img: 0.0381 s\n",
            "loss: 0.2131 (epoch: 14, step: 100) // Avg time/img: 0.0381 s\n",
            "loss: 0.2141 (epoch: 14, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2125 (epoch: 14, step: 200) // Avg time/img: 0.0379 s\n",
            "loss: 0.2123 (epoch: 14, step: 250) // Avg time/img: 0.0379 s\n",
            "loss: 0.2115 (epoch: 14, step: 300) // Avg time/img: 0.0377 s\n",
            "loss: 0.2116 (epoch: 14, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2111 (epoch: 14, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2111 (epoch: 14, step: 450) // Avg time/img: 0.0377 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.3071 (epoch: 14, step: 0) // Avg time/img: 0.0365 s\n",
            "VAL loss: 0.3915 (epoch: 14, step: 50) // Avg time/img: 0.0335 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.64\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.2065 (epoch: 15, step: 0) // Avg time/img: 0.0459 s\n",
            "loss: 0.2157 (epoch: 15, step: 50) // Avg time/img: 0.0380 s\n",
            "loss: 0.213 (epoch: 15, step: 100) // Avg time/img: 0.0379 s\n",
            "loss: 0.2118 (epoch: 15, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2123 (epoch: 15, step: 200) // Avg time/img: 0.0378 s\n",
            "loss: 0.2118 (epoch: 15, step: 250) // Avg time/img: 0.0377 s\n",
            "loss: 0.2117 (epoch: 15, step: 300) // Avg time/img: 0.0377 s\n",
            "loss: 0.2105 (epoch: 15, step: 350) // Avg time/img: 0.0377 s\n",
            "loss: 0.2107 (epoch: 15, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2108 (epoch: 15, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.3213 (epoch: 15, step: 0) // Avg time/img: 0.0359 s\n",
            "VAL loss: 0.394 (epoch: 15, step: 50) // Avg time/img: 0.0336 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.65\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 0.2148 (epoch: 16, step: 0) // Avg time/img: 0.0374 s\n",
            "loss: 0.2039 (epoch: 16, step: 50) // Avg time/img: 0.0380 s\n",
            "loss: 0.2105 (epoch: 16, step: 100) // Avg time/img: 0.0377 s\n",
            "loss: 0.2086 (epoch: 16, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2085 (epoch: 16, step: 200) // Avg time/img: 0.0378 s\n",
            "loss: 0.207 (epoch: 16, step: 250) // Avg time/img: 0.0379 s\n",
            "loss: 0.2071 (epoch: 16, step: 300) // Avg time/img: 0.0378 s\n",
            "loss: 0.2082 (epoch: 16, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2088 (epoch: 16, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2094 (epoch: 16, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.3094 (epoch: 16, step: 0) // Avg time/img: 0.0340 s\n",
            "VAL loss: 0.3902 (epoch: 16, step: 50) // Avg time/img: 0.0337 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.38\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.15 (epoch: 17, step: 0) // Avg time/img: 0.0397 s\n",
            "loss: 0.2115 (epoch: 17, step: 50) // Avg time/img: 0.0378 s\n",
            "loss: 0.2124 (epoch: 17, step: 100) // Avg time/img: 0.0379 s\n",
            "loss: 0.2104 (epoch: 17, step: 150) // Avg time/img: 0.0378 s\n",
            "loss: 0.2101 (epoch: 17, step: 200) // Avg time/img: 0.0377 s\n",
            "loss: 0.2099 (epoch: 17, step: 250) // Avg time/img: 0.0377 s\n",
            "loss: 0.2093 (epoch: 17, step: 300) // Avg time/img: 0.0376 s\n",
            "loss: 0.2088 (epoch: 17, step: 350) // Avg time/img: 0.0377 s\n",
            "loss: 0.2084 (epoch: 17, step: 400) // Avg time/img: 0.0377 s\n",
            "loss: 0.2094 (epoch: 17, step: 450) // Avg time/img: 0.0377 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.2986 (epoch: 17, step: 0) // Avg time/img: 0.0354 s\n",
            "VAL loss: 0.3868 (epoch: 17, step: 50) // Avg time/img: 0.0341 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.58\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 0.2023 (epoch: 18, step: 0) // Avg time/img: 0.0360 s\n",
            "loss: 0.215 (epoch: 18, step: 50) // Avg time/img: 0.0378 s\n",
            "loss: 0.2116 (epoch: 18, step: 100) // Avg time/img: 0.0377 s\n",
            "loss: 0.2116 (epoch: 18, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2108 (epoch: 18, step: 200) // Avg time/img: 0.0379 s\n",
            "loss: 0.2112 (epoch: 18, step: 250) // Avg time/img: 0.0378 s\n",
            "loss: 0.2101 (epoch: 18, step: 300) // Avg time/img: 0.0377 s\n",
            "loss: 0.2102 (epoch: 18, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2093 (epoch: 18, step: 400) // Avg time/img: 0.0377 s\n",
            "loss: 0.2096 (epoch: 18, step: 450) // Avg time/img: 0.0377 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.3004 (epoch: 18, step: 0) // Avg time/img: 0.0350 s\n",
            "VAL loss: 0.3911 (epoch: 18, step: 50) // Avg time/img: 0.0340 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.68\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.1953 (epoch: 19, step: 0) // Avg time/img: 0.0451 s\n",
            "loss: 0.2037 (epoch: 19, step: 50) // Avg time/img: 0.0383 s\n",
            "loss: 0.2064 (epoch: 19, step: 100) // Avg time/img: 0.0380 s\n",
            "loss: 0.2048 (epoch: 19, step: 150) // Avg time/img: 0.0379 s\n",
            "loss: 0.2058 (epoch: 19, step: 200) // Avg time/img: 0.0378 s\n",
            "loss: 0.2058 (epoch: 19, step: 250) // Avg time/img: 0.0378 s\n",
            "loss: 0.206 (epoch: 19, step: 300) // Avg time/img: 0.0378 s\n",
            "loss: 0.2066 (epoch: 19, step: 350) // Avg time/img: 0.0379 s\n",
            "loss: 0.2077 (epoch: 19, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2075 (epoch: 19, step: 450) // Avg time/img: 0.0378 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.3189 (epoch: 19, step: 0) // Avg time/img: 0.0357 s\n",
            "VAL loss: 0.3891 (epoch: 19, step: 50) // Avg time/img: 0.0344 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.79\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.1774 (epoch: 20, step: 0) // Avg time/img: 0.0398 s\n",
            "loss: 0.2024 (epoch: 20, step: 50) // Avg time/img: 0.0381 s\n",
            "loss: 0.209 (epoch: 20, step: 100) // Avg time/img: 0.0377 s\n",
            "loss: 0.2087 (epoch: 20, step: 150) // Avg time/img: 0.0377 s\n",
            "loss: 0.2094 (epoch: 20, step: 200) // Avg time/img: 0.0377 s\n",
            "loss: 0.2076 (epoch: 20, step: 250) // Avg time/img: 0.0377 s\n",
            "loss: 0.2068 (epoch: 20, step: 300) // Avg time/img: 0.0377 s\n",
            "loss: 0.207 (epoch: 20, step: 350) // Avg time/img: 0.0378 s\n",
            "loss: 0.2072 (epoch: 20, step: 400) // Avg time/img: 0.0378 s\n",
            "loss: 0.2077 (epoch: 20, step: 450) // Avg time/img: 0.0377 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.3113 (epoch: 20, step: 0) // Avg time/img: 0.0343 s\n",
            "VAL loss: 0.3884 (epoch: 20, step: 50) // Avg time/img: 0.0338 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.84\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "Z9wFyUaR1ANs",
        "outputId": "e49a2264-1f03-4bdc-d483-ded193d91aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../save"
      ],
      "metadata": {
        "id": "sCWGOj8H1D55",
        "outputId": "56e92f76-1ffa-4d39-b47d-e85bfa0ebdc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r bisenet_training_void.zip bisenet_training_void_1/"
      ],
      "metadata": {
        "id": "LeSYcbj0UK2s",
        "outputId": "41298891-5a7a-4736-bd68-91a49b25eda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: bisenet_training_void_1/ (stored 0%)\n",
            "  adding: bisenet_training_void_1/model-020.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-005.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-002.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-014.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-004.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/opts.txt (deflated 38%)\n",
            "  adding: bisenet_training_void_1/model-008.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model_best.pth.tar (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-003.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-012.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model.txt (deflated 91%)\n",
            "  adding: bisenet_training_void_1/model-011.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/checkpoint.pth.tar (deflated 7%)\n",
            "  adding: bisenet_training_void_1/automated_log.txt (deflated 59%)\n",
            "  adding: bisenet_training_void_1/model-017.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-015.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-013.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model_best.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-006.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-016.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/best.txt (stored 0%)\n",
            "  adding: bisenet_training_void_1/model-010.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/bisenet.py (deflated 82%)\n",
            "  adding: bisenet_training_void_1/model-019.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-009.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-018.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-007.pth (deflated 7%)\n",
            "  adding: bisenet_training_void_1/model-001.pth (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BiSeNet training"
      ],
      "metadata": {
        "id": "rTIaOpXWEYV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start training BiSeNet for a total of 60 epochs. In the first run, due to GPU time limitations on Google Colab, the training is intentionally interrupted after 20 epochs by setting the parameter `stop_epoch` equal to 20. Remember to set `num_epochs` to 60 from the beginning to ensure that the learning rate scheduler behaves correctly across the full training process. The process is then resumed in the following runs from epoch 21 using the `--resume` flag."
      ],
      "metadata": {
        "id": "R-5bPbOzoCpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r \"../save/bisenet_training_void\""
      ],
      "metadata": {
        "id": "GkAhszC1GPPW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"bisenet\", num_epochs=60, batch_size=6, stop_epoch=20)"
      ],
      "metadata": {
        "id": "tv3wuAZREcGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"bisenet\", num_epochs=60, batch_size=6, stop_epoch=40, resume=True)"
      ],
      "metadata": {
        "id": "9ZVo5aGbEmG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"bisenet\", num_epochs=60, batch_size=6, stop_epoch=60, resume=True)"
      ],
      "metadata": {
        "id": "j8GVIXm-rNVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UozW3CHhS7U9"
      },
      "source": [
        "# REVISE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8ATRy0qGNmA"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmjgpr8lGQ78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for model in models:\n",
        "  print(\"----------------------------\")\n",
        "  for dataset_dir in datasets_list:\n",
        "\n",
        "    if no_execute:\n",
        "      break\n",
        "\n",
        "    load_dir = f'content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/{net}_training_void'\n",
        "    weights = f'/model_best.pth'\n",
        "    format_file = os.listdir(f'/content/validation_dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "    input =f'/content/validation_dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "    print(f\"\\nDataset: {dataset_dir} net: {net}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      !python  content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} | tail -n 2\n",
        "    else:\n",
        "      !python  content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu | tail -n 2\n",
        "\n",
        "    if just_once:\n",
        "      no_execute = True\n",
        "      just_once = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJKUFjWIbQh"
      },
      "source": [
        "**mIoU Void Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tiSQNBxIl3r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "for model in models:\n",
        "  print(\"----------------------------\")\n",
        "\n",
        "  if no_execute:\n",
        "      break\n",
        "  print(f\"-----------{model}-------------\")\n",
        "  loadDir = f'content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/{model}_training_void'\n",
        "  weights = f'/model_best.pth'\n",
        "  if torch.cuda.is_available():\n",
        "    !python  content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/eval_iou.py --loadDir {loadDir} --loadWeights {weights} --datadir /content/cityscapes/ --model {model} | tail -n 25\n",
        "  else:\n",
        "    !python  content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/eval_iou.py  --loadDir {loadDir} --loadWeights {weights} --datadir /content/cityscapes/  --model {model}  --cpu | tail -n 25\n",
        "\n",
        "\n",
        "  if just_once:\n",
        "    no_execute = True\n",
        "    just_once = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n2SPIAyvU5g"
      },
      "source": [
        "### Step 4 - Analyze the Effect of Training Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkwRPEbyVkKW"
      },
      "source": [
        "Analyze the effect of the training model along with losses that are specifically made for anomaly detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEaQplsFKGqJ"
      },
      "source": [
        "**Losses**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACEmk07kKSzF"
      },
      "source": [
        "**Fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eM-3aNyKYrS"
      },
      "outputs": [],
      "source": [
        "# Fine tune ERFNET with different losses\n",
        "\"\"\"\n",
        "Training:\n",
        "1. Focal loss\n",
        "2. LogitNorm+CrossEntropy loss\n",
        "3. IsoMaxPlus+CrossEntropy loss\n",
        "4. LogitNorm+Focal loss\n",
        "5. IsoMaxPlus+Focal loss\n",
        "\"\"\"\n",
        "titles = [\"Focal\", \"LogitNorm+CrossEntropy\", \"IsoMaxPlus+CrossEntropy\", \"LogitNorm+Focal\", \"IsoMaxPlus+Focal\"]\n",
        "losses = [\"Focal\", \"CrossEntropy\", \"CrossEntropy\", \"Focal\", \"Focal\"]\n",
        "models = [\"erfnet\", \"erfnet\", \"erfnet_isomaxplus\", \"erfnet\", \"erfnet_isomaxplus\"]\n",
        "savedirs = [\"erfnet_training_focal_loss\", \"erfnet_training_logitnorm_cross_entropy_loss\", \"erfnet_training_isomaxplus_cross_entropy_loss\", \"erfnet_training_logitnorm_focal_loss\", \"erfnet_training_isomaxplus_focal_loss\"]\n",
        "logit_normalization_flags = [False, True, False, True, False]\n",
        "epochs = 20\n",
        "\n",
        "# Base directory of the project\n",
        "base_dir = \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/train\"\n",
        "# Dataset directory\n",
        "data_dir = \"/content/cityscapes\"\n",
        "pretrained_weights = \"erfnet_pretrained.pth\"\n",
        "\n",
        "# Loop to execute fine-tuning\n",
        "for title, loss, model, savedir, logit_normalization_flag in zip(titles, losses, models, savedirs, logit_normalization_flags):\n",
        "    print(f\"\\n\\n----- Fine-tuning with {title} loss -----\")\n",
        "    !cd {base_dir} && python -W ignore main.py --savedir {savedir} --loss {loss} --logit_normalization {logit_normalization_flag} --datadir {data_dir} --model {model} --cuda --num-epochs=20 --epochs-save=1 --FineTune --decoder --loadWeights={pretrained_weights}\n",
        "    print(f\"Model saved in /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/{savedir}\")\n",
        "    # zip folder\n",
        "    !zip -r save_{savedir}.zip /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/{savedir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zIp0mbgKafh"
      },
      "source": [
        "**Extension Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eopXW5mKKeR9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "no_execute = False\n",
        "just_once = False\n",
        "\n",
        "losses = [\"CrossEntropy\", \"Focal\", \"LogitNorm+CrossEntropy\", \"IsoMaxPlus+CrossEntropy\", \"LogitNorm+FocalLoss\", \"IsoMaxPlus+FocalLoss\"]\n",
        "models = [\"erfnet\", \"erfnet\", \"erfnet\", \"erfnet_isomaxplus\", \"erfnet\", \"erfnet_isomaxplus\"]\n",
        "load_dirs = [\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/trained_models/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_focal_loss/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_logitnorm_cross_entropy_loss/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_isomaxplus_cross_entropy_loss/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_logitnorm_focal_loss/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_isomaxplus_focal_loss/\"]\n",
        "weights = [\"erfnet_pretrained.pth\", \"model_best.pth\", \"model_best.pth\", \"model_best.pth\", \"model_best.pth\", \"model_best.pth\"]\n",
        "\n",
        "for loss, model, load_dir, weight in zip(losses, models, load_dirs, weights):\n",
        "  print(f\"------ Evaluating loss: {loss} ------\\n\")\n",
        "  for dataset_dir in ['RoadAnomaly21', 'RoadObsticle21', 'FS_LostFound_full', 'fs_static', 'RoadAnomaly']:\n",
        "    for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\", \"Mahalanobis\"]:\n",
        "\n",
        "      if no_execute:\n",
        "        break\n",
        "\n",
        "      format_file = os.listdir(f'/content/Validation_Dataset/{dataset_dir}/images')[0].split(\".\")[1]\n",
        "      input =f'/content/Validation_Dataset/{dataset_dir}/images/\\*.{format_file}'\n",
        "\n",
        "      print(f\"\\nDataset: {dataset_dir} method: {method} loss: {loss}\")\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "        !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method  {method} --model {model} --loadDir {load_dir} --loadWeights {weight} | tail -n 2\n",
        "      else:\n",
        "        !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method {method}  --model {model} --loadDir {load_dir} --loadWeights {weight} --cpu | tail -n 2\n",
        "\n",
        "      print(\"----------------------------\")\n",
        "      if just_once:\n",
        "        no_execute = True\n",
        "        just_once = False\n",
        "    print(\"----------------------------\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1JdbXZnKfyH"
      },
      "source": [
        "##Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh4XfFbnKkuk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Example image to color\n",
        "# Nice images: RoadAnomaly/images/28, RoadAnomaly/images/58\n",
        "input = '/content/Validation_Dataset/RoadAnomaly/images/58.jpg'\n",
        "\n",
        "### Baseline models ###\n",
        "for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\", \"Mahalanobis\"]:\n",
        "  print(f\"Method: {method}\")\n",
        "  save_image_path = f'/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/baseline/{method}'\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method  {method} --save-colored {save_image_path}  | tail -n 2\n",
        "  else:\n",
        "    !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method {method} --save-colored {save_image_path} --cpu | tail -n 2\n",
        "\n",
        "!python /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/visualization.py --name_dir=\"/ccontent/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/baseline\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/baseline_visualization.png\"\n",
        "\n",
        "### Temperature scaling ###\n",
        "for t in [0.5, 0.75, 1.1]:\n",
        "  print(f\"Method: MSP, Temperature: {t}\")\n",
        "  save_image_path = f'/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/temperature/t={t}'\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method 'MSP' --temperature {t} --save-colored {save_image_path} | tail -n 2\n",
        "  else:\n",
        "    !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method 'MSP' --cpu --temperature {t} --save-colored {save_image_path} | tail -n 2\n",
        "\n",
        "!python /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/visualization.py --name_dir=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/temperature\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/temperature_visualization.png\"\n",
        "\n",
        "### Finetuned models with void ###\n",
        "for net in [\"erfnet\", \"enet\", \"bisenet\"]:\n",
        "  save_image_path = f'/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/void/{net}'\n",
        "  load_dir = f'/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/{net}_training_void'\n",
        "  weights = f'/model_best.pth'\n",
        "  print(f\"Finetuned network: {net}\")\n",
        "  if torch.cuda.is_available():\n",
        "    !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --save-colored {save_image_path} | tail -n 2\n",
        "  else:\n",
        "    !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --void --model {net} --loadDir {load_dir} --loadWeights {weights} --cpu --save-colored {save_image_path} | tail -n 2\n",
        "\n",
        "!python /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/visualization.py --name_dir=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/void\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/void_visualization.png\"\n",
        "\n",
        "### Losses ###\n",
        "losses = [\"CrossEntropy\", \"Focal\", \"LogitNorm\", \"IsoMaxPlus\"]\n",
        "models = [\"erfnet\", \"erfnet\", \"erfnet\", \"erfnet_isomaxplus\"]\n",
        "load_dirs = [\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/trained_models/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_focal_loss/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_logitnorm_loss/\", \"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/save/erfnet_training_isomaxplus_loss/\"]\n",
        "weights = [\"erfnet_pretrained.pth\", \"model_best.pth\", \"model_best.pth\", \"model_best.pth\"]\n",
        "for loss, model, load_dir, weight in zip(losses, models, load_dirs, weights):\n",
        "  for method in [\"MSP\", \"MaxLogit\", \"MaxEntropy\", \"Mahalanobis\"]:\n",
        "    save_image_path = f'/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses/{loss}/{method}'\n",
        "    print(f\"Method: {method}, loss: {loss}\")\n",
        "    if torch.cuda.is_available():\n",
        "      !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method  {method} --model {model} --loadDir {load_dir} --loadWeights {weight} --save-colored {save_image_path} | tail -n 2\n",
        "    else:\n",
        "      !python  /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method {method}  --model {model} --loadDir {load_dir} --loadWeights {weight} --save-colored {save_image_path} --cpu | tail -n 2\n",
        "\n",
        "!python /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/visualization.py --name_dir=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses/CrossEntropy\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses_CrossEntropy_visualization.png\"\n",
        "!python /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/visualization.py --name_dir=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses/Focal\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses_Focal_visualization.png\"\n",
        "!python /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/visualization.py --name_dir=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses/LogitNorm\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses_LogitNorm_visualization.png\"\n",
        "!python /ccontent/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/visualization.py --name_dir=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses/IsoMaxPlus\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/losses_IsoMaxPlus_visualization.png\"\n",
        "\n",
        "# Zip the images\n",
        "!zip -r colored_anomalies.zip /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}