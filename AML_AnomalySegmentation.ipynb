{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgneseRe/Real-Time-Anomaly-Segmentation-for-Road-Scenes/blob/main/AML_AnomalySegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjyXaByVOtbd"
      },
      "source": [
        "# **Real-time Anomaly Segmentation for Road Scenes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpweY-zTOyvF"
      },
      "source": [
        "Existing deep neural networks, when deployed in open-world settings, perform poorly on unknown, anomaly, out-of-distribution (OoD) objects that were not present during the training. The goal of this project is to build tiny anomaly segmentation models to segment anomaly patterns. Models must be able to fit in small devices, which represents a realistic memory constraint for an edge application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxuFk3f4uqwU"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SOBJKZAfEwAZ"
      },
      "outputs": [],
      "source": [
        "!rm -r sample_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages and import useful modules."
      ],
      "metadata": {
        "id": "zYeE7QjQysQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wed7tVmrP222"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install --quiet numpy\n",
        "!pip3 install --quiet Pillow\n",
        "\n",
        "!pip3 install --quiet gdown\n",
        "!pip3 install --quiet torchvision\n",
        "!pip3 install --quiet ood_metrics\n",
        "!pip3 install --quiet cityscapesscripts\n",
        "\n",
        "!pip3 install --quiet matplotlib\n",
        "!pip3 install --quiet visdom\n",
        "\n",
        "import os, sys, subprocess, torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aha0ydTyO4IR"
      },
      "source": [
        "The following function is implemented to download the *Cityscapes* dataset in two different ways: via Google Drive (using `gdown`) or directly from the Cityscapes official website (using `csDownload`). Although the first option is preferable as it is definitely faster, direct download from the website is provided as an alternative. `gdown` may in fact raise the error *Failed to retrieve the file url* if the file we are attempting to download is exceptionally large (*e.g.* 11G), there are numerous users simultaneously trying to download it programmatically or we download it many times in a limited time. Regardless of the method used, use the conversor (available [here](https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/preparation/createTrainIdLabelImgs.py)) to generate labelTrainIds from labelIds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-cQRn0hfGLlV"
      },
      "outputs": [],
      "source": [
        "def download_cityscapes():\n",
        "\n",
        "    if not os.path.isdir('/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/cityscapes'):\n",
        "        print(\"Attempting to download cityscapes dataset using gdown...\")\n",
        "\n",
        "        try:\n",
        "            # If check is true, and the process exits with a non-zero exit code, a CalledProcessError exception will be raised.\n",
        "            subprocess.run([\"gdown\", \"https://drive.google.com/uc?id=11gSQ9UcLCnIqmY7srG2S6EVwV3paOMEq\"], check=True)\n",
        "            print(\"Dataset downloaded successfully using gdown. Unzipping...\")\n",
        "            subprocess.run([\"unzip\", \"-q\", \"cityscapes.zip\"], check=True)\n",
        "            # Use the conversor to generate labelTrainIds from labelIds\n",
        "            print(\"Generating trainIds from labelIds...\")\n",
        "            !CITYSCAPES_DATASET='cityscapes/' csCreateTrainIdLabelImgs\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(\"gdown failed. Attempting to download cityscapes dataset from the official website...\")\n",
        "            try:\n",
        "              !csDownload leftImg8bit_trainvaltest.zip\n",
        "              !csDownload gtFine_trainvaltest.zip\n",
        "\n",
        "              print(\"Dataset downloaded successfully from the official website. Unzipping...\")\n",
        "              !unzip -q 'leftImg8bit_trainvaltest.zip' -d 'cityscapes'\n",
        "              !unzip -o -q 'gtFine_trainvaltest.zip' -d 'cityscapes'\n",
        "\n",
        "              print(\"Generating trainIds from labelIds...\")\n",
        "              !CITYSCAPES_DATASET='cityscapes/' csCreateTrainIdLabelImgs\n",
        "\n",
        "              print(\"Cityscapes dataset ready\")\n",
        "\n",
        "            except Exception as e2:\n",
        "                print(\"Failed to download the dataset using both methods.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFHf3bXySHFf"
      },
      "source": [
        "Download and unzip the validation dataset (*FS_LostFound_full*, *RoadAnomaly*, *RoadAnomaly21*, *RoadObsticle21*, *fs_static*), clone or update the GitHub repository (*Real-Time-Anomaly-Segmentation-for-Road-Scenes*) and download the *Cityscapes* dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download and unzip validation dataset\n",
        "if not os.path.isdir('/content/validation_dataset'):\n",
        "  !gdown 'https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta'\n",
        "  !unzip -q 'Validation_Dataset.zip'\n",
        "  !mkdir validation_dataset && cp -pR Validation_Dataset/* validation_dataset/ && rm -R Validation_Dataset/\n",
        "  !rm 'Validation_Dataset.zip'\n",
        "\n",
        "# clone the github repo and pull command\n",
        "if not os.path.isdir('content/Real-Time-Anomaly-Segmentation-for-Road-Scenes'):\n",
        "  !git clone https://github.com/AgneseRe/Real-Time-Anomaly-Segmentation-for-Road-Scenes.git\n",
        "else: # if folder already present\n",
        "  !git pull\n",
        "\n",
        "%cd Real-Time-Anomaly-Segmentation-for-Road-Scenes"
      ],
      "metadata": {
        "id": "m_ZPejhlbRns",
        "outputId": "748b8d3d-cea6-43d1-cb27-2f7eddb22a37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta\n",
            "From (redirected): https://drive.google.com/uc?id=12YJq48XkCxQHjN3CmLc-zM5dThSak4Ta&confirm=t&uuid=c042e90b-0d5a-4a28-a862-3894735f0c3e\n",
            "To: /content/Validation_Dataset.zip\n",
            "100% 329M/329M [00:03<00:00, 94.0MB/s]\n",
            "Cloning into 'Real-Time-Anomaly-Segmentation-for-Road-Scenes'...\n",
            "remote: Enumerating objects: 1922, done.\u001b[K\n",
            "remote: Total 1922 (delta 0), reused 0 (delta 0), pack-reused 1922 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1922/1922), 2.15 GiB | 39.13 MiB/s, done.\n",
            "Resolving deltas: 100% (1040/1040), done.\n",
            "Updating files: 100% (471/471), done.\n",
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AtojfyyxIpaw",
        "outputId": "79b0ba3f-2981-4151-9f15-e710f19232ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to download cityscapes dataset using gdown...\n",
            "gdown failed. Attempting to download cityscapes dataset from the official website...\n",
            "Cityscapes username or email address: agnesere\n",
            "Cityscapes password: \n",
            "Store credentials unencrypted in '/root/.local/share/cityscapesscripts/credentials.json' [y/N]: N\n",
            "Downloading cityscapes package 'leftImg8bit_trainvaltest.zip' to './leftImg8bit_trainvaltest.zip'\n",
            "Download progress:  98% 10.8G/11.0G [08:51<00:10, 21.8MB/s]\n",
            "Cityscapes username or email address: agnesere\n",
            "Cityscapes password: \n",
            "Store credentials unencrypted in '/root/.local/share/cityscapesscripts/credentials.json' [y/N]: N\n",
            "Downloading cityscapes package 'gtFine_trainvaltest.zip' to './gtFine_trainvaltest.zip'\n",
            "Download progress: 100% 241M/241M [00:12<00:00, 19.8MB/s]\n",
            "Dataset downloaded successfully from the official website. Unzipping...\n",
            "Generating trainIds from labelIds...\n",
            "Processing 5000 annotation files\n",
            "Progress: 100.0 % Cityscapes dataset ready\n"
          ]
        }
      ],
      "source": [
        "# download cityscapes dataset - use credentials (agnesere, FCSBwcVMi-u9-Zn) if downloading from official site\n",
        "download_cityscapes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_czbWkaraAdh"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smRtjvZQu0R4"
      },
      "source": [
        "### Step 2A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute AuPRC & FPR95TPR"
      ],
      "metadata": {
        "id": "ld4klFz5M3bo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W9ewyGcSZ__2",
        "outputId": "ff99d5ab-7090-4074-e9f1-a534598c2d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval\n"
          ]
        }
      ],
      "source": [
        "%cd eval"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define datasets used for evaluation."
      ],
      "metadata": {
        "id": "CAlJ1knA6nHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MdLxJaGP1FyY"
      },
      "outputs": [],
      "source": [
        "# datasets = os.listdir(\"../../validation_dataset\")\n",
        "datasets = {\n",
        "    \"SMIYC RA-21\": \"RoadAnomaly21\",\n",
        "    \"SMIYC RO-21\": \"RoadObsticle21\",\n",
        "    \"FS L&F\": \"FS_LostFound_full\",\n",
        "    \" FS Static\": \"fs_static\",\n",
        "    \"Road Anomaly\": \"RoadAnomaly\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List anomaly detection methods used for evaluation."
      ],
      "metadata": {
        "id": "kgovQ7YG60me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"MSP\", \"MaxLogit\", \"MaxEntropy\"]"
      ],
      "metadata": {
        "id": "ZdZnNg2VeGMY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOPyLTmNxhR"
      },
      "source": [
        "Automate running anomaly detection experiments on multiple datasets using different methods. The evaluation script can be invoked with the appropriate parameters: name of the model, path to the training folder, base folder to save generated plots, directory to load the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval_anomaly(datasets, methods, model = None, training_folder = None, plot_folder = None, load_dir = None) -> None:\n",
        "\n",
        "  for dataset, folder in datasets.items():\n",
        "    print(f\"Dataset {dataset}\")\n",
        "\n",
        "    for method in methods:\n",
        "      print(f\" - {method:<10} \", end = \"\")\n",
        "      input_path = f\"../../validation_dataset/{folder}/images/*.*\"\n",
        "      plot_dir_path = f\"../plots/losses/{plot_folder}/{folder}_{method}\" if model else f\"../plots/baselines/{folder}_{method}\"\n",
        "\n",
        "      add_cmd = \"--cpu\" if not torch.cuda.is_available() else \"\"\n",
        "\n",
        "      if model:\n",
        "        !python evalAnomaly.py --input={input_path} --method={method} --loadModel={model} --loadDir={load_dir} --loadWeights={training_folder}/model_best.pth --plotdir={plot_dir_path} {add_cmd}\n",
        "      else: # ERFNet pre-trained\n",
        "        !python evalAnomaly.py --input={input_path} --method={method} --plotdir={plot_dir_path} {add_cmd}\n",
        "\n",
        "    print(\"=\" * 55, end = \"\\n\")"
      ],
      "metadata": {
        "id": "WsplUveYceKH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate a segmentation model on Cityscapes using specified weights."
      ],
      "metadata": {
        "id": "ZDhaDCyLojs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval_iou(model = \"erfnet\", load_dir = \"../trained_models/\", training_folder = \"erfnet_pretrained.pth\", void = False) -> None:\n",
        "\n",
        "  load_model = f\"{model}.py\" if model != \"erfnet_isomaxplus\" else \"erfnet.py\"\n",
        "  add_cmd = \"--cpu\" if not torch.cuda.is_available() else \"\"\n",
        "  method_flag = \"--method void\" if void else \"\" # for void classifier\n",
        "  !python eval_iou.py --model={model} --loadDir={load_dir} --loadModel={load_model} --loadWeights={training_folder} --datadir /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/cityscapes {method_flag} {add_cmd}"
      ],
      "metadata": {
        "id": "EnFkz4pL307H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform inference using the pre-trained **ERFNet** model on anomaly segmentation test datasets provided. Evaluate results with different techniques: MSP, MaxLogit and MaxEntropy."
      ],
      "metadata": {
        "id": "N3TVqRlwns_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_eval_anomaly(datasets, methods)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gipIszWHekzP",
        "outputId": "c4074ce8-21fe-4e70-a6a0-16528c49a33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 29.100 | FPR@TPR95: 62.511\n",
            " - MaxLogit   | AUPRC score: 38.320 | FPR@TPR95: 59.337\n",
            " - MaxEntropy | AUPRC score: 31.005 | FPR@TPR95: 62.593\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 2.712 | FPR@TPR95: 64.974\n",
            " - MaxLogit   | AUPRC score: 4.627 | FPR@TPR95: 48.443\n",
            " - MaxEntropy | AUPRC score: 3.052 | FPR@TPR95: 65.600\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score: 1.748 | FPR@TPR95: 50.763\n",
            " - MaxLogit   | AUPRC score: 3.301 | FPR@TPR95: 45.495\n",
            " - MaxEntropy | AUPRC score: 2.582 | FPR@TPR95: 50.368\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score: 7.470 | FPR@TPR95: 41.823\n",
            " - MaxLogit   | AUPRC score: 9.499 | FPR@TPR95: 40.300\n",
            " - MaxEntropy | AUPRC score: 8.826 | FPR@TPR95: 41.523\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 12.426 | FPR@TPR95: 82.492\n",
            " - MaxLogit   | AUPRC score: 15.582 | FPR@TPR95: 73.248\n",
            " - MaxEntropy | AUPRC score: 12.678 | FPR@TPR95: 82.632\n",
            "=======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vXNPPsR8YtJ"
      },
      "source": [
        "If you want to save the baselines folder in your local machine, create a ZIP file with the following command and then download it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDemCzdK8WJh"
      },
      "outputs": [],
      "source": [
        " # !zip -r baselines.zip baselines/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UaIqk89NPw3"
      },
      "source": [
        "#### Compute mIoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1AKrBWFNc_Y",
        "outputId": "4cce72f4-8ee8-434b-e898-f678b6c2de1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: ../trained_models/erfnet\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  80.3807921409607 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m97.62\u001b[0m Road\n",
            "\u001b[0m81.37\u001b[0m sidewalk\n",
            "\u001b[0m90.77\u001b[0m building\n",
            "\u001b[0m49.43\u001b[0m wall\n",
            "\u001b[0m54.93\u001b[0m fence\n",
            "\u001b[0m60.81\u001b[0m pole\n",
            "\u001b[0m62.60\u001b[0m traffic light\n",
            "\u001b[0m72.32\u001b[0m traffic sign\n",
            "\u001b[0m91.35\u001b[0m vegetation\n",
            "\u001b[0m60.97\u001b[0m terrain\n",
            "\u001b[0m93.38\u001b[0m sky\n",
            "\u001b[0m76.11\u001b[0m person\n",
            "\u001b[0m53.45\u001b[0m rider\n",
            "\u001b[0m92.91\u001b[0m car\n",
            "\u001b[0m72.78\u001b[0m truck\n",
            "\u001b[0m78.87\u001b[0m bus\n",
            "\u001b[0m63.86\u001b[0m train\n",
            "\u001b[0m46.41\u001b[0m motorcycle\n",
            "\u001b[0m71.89\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m72.20\u001b[0m %\n"
          ]
        }
      ],
      "source": [
        "run_eval_iou()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJodGToyvAwX"
      },
      "source": [
        "### Step 2B"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute AuPRC & FPR95TPR with temperature scaling"
      ],
      "metadata": {
        "id": "ifgHbBGzNBzJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5oNs1EqDL6B",
        "outputId": "a81273d6-0076-4cbc-9198-3af1b3b3ba4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset SMIYC RA-21\n",
            " - 0.5        | AUPRC score: 27.061 | FPR@TPR95: 62.731\n",
            " - 0.75       | AUPRC score: 28.156 | FPR@TPR95: 62.479\n",
            " - 1.0        | AUPRC score: 29.100 | FPR@TPR95: 62.511\n",
            " - 1.1        | AUPRC score: 29.410 | FPR@TPR95: 62.590\n",
            " - 1.2        | AUPRC score: 29.678 | FPR@TPR95: 62.724\n",
            " - 1.5        | AUPRC score: 30.258 | FPR@TPR95: 63.318\n",
            " - 2.0        | AUPRC score: 30.679 | FPR@TPR95: 64.721\n",
            " - 5.0        | AUPRC score: 30.196 | FPR@TPR95: 71.594\n",
            " - 10.0       | AUPRC score: 29.526 | FPR@TPR95: 75.757\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - 0.5        | AUPRC score: 2.420 | FPR@TPR95: 63.225\n",
            " - 0.75       | AUPRC score: 2.567 | FPR@TPR95: 64.053\n",
            " - 1.0        | AUPRC score: 2.712 | FPR@TPR95: 64.974\n",
            " - 1.1        | AUPRC score: 2.766 | FPR@TPR95: 65.524\n",
            " - 1.2        | AUPRC score: 2.816 | FPR@TPR95: 66.033\n",
            " - 1.5        | AUPRC score: 2.937 | FPR@TPR95: 67.928\n",
            " - 2.0        | AUPRC score: 3.026 | FPR@TPR95: 71.459\n",
            " - 5.0        | AUPRC score: 2.841 | FPR@TPR95: 83.111\n",
            " - 10.0       | AUPRC score: 2.644 | FPR@TPR95: 88.146\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - 0.5        | AUPRC score: 1.280 | FPR@TPR95: 66.737\n",
            " - 0.75       | AUPRC score: 1.493 | FPR@TPR95: 51.848\n",
            " - 1.0        | AUPRC score: 1.748 | FPR@TPR95: 50.763\n",
            " - 1.1        | AUPRC score: 1.860 | FPR@TPR95: 50.387\n",
            " - 1.2        | AUPRC score: 1.972 | FPR@TPR95: 50.150\n",
            " - 1.5        | AUPRC score: 2.286 | FPR@TPR95: 49.456\n",
            " - 2.0        | AUPRC score: 2.677 | FPR@TPR95: 48.324\n",
            " - 5.0        | AUPRC score: 3.252 | FPR@TPR95: 45.396\n",
            " - 10.0       | AUPRC score: 3.344 | FPR@TPR95: 44.121\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - 0.5        | AUPRC score: 6.601 | FPR@TPR95: 43.476\n",
            " - 0.75       | AUPRC score: 6.991 | FPR@TPR95: 42.493\n",
            " - 1.0        | AUPRC score: 7.470 | FPR@TPR95: 41.823\n",
            " - 1.1        | AUPRC score: 7.687 | FPR@TPR95: 41.587\n",
            " - 1.2        | AUPRC score: 7.910 | FPR@TPR95: 41.406\n",
            " - 1.5        | AUPRC score: 8.580 | FPR@TPR95: 41.096\n",
            " - 2.0        | AUPRC score: 9.508 | FPR@TPR95: 41.018\n",
            " - 5.0        | AUPRC score: 11.175 | FPR@TPR95: 45.514\n",
            " - 10.0       | AUPRC score: 11.462 | FPR@TPR95: 51.164\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - 0.5        | AUPRC score: 12.188 | FPR@TPR95: 82.022\n",
            " - 0.75       | AUPRC score: 12.319 | FPR@TPR95: 82.285\n",
            " - 1.0        | AUPRC score: 12.426 | FPR@TPR95: 82.492\n",
            " - 1.1        | AUPRC score: 12.466 | FPR@TPR95: 82.621\n",
            " - 1.2        | AUPRC score: 12.502 | FPR@TPR95: 82.757\n",
            " - 1.5        | AUPRC score: 12.591 | FPR@TPR95: 83.244\n",
            " - 2.0        | AUPRC score: 12.676 | FPR@TPR95: 84.150\n",
            " - 5.0        | AUPRC score: 12.672 | FPR@TPR95: 87.713\n",
            " - 10.0       | AUPRC score: 12.575 | FPR@TPR95: 89.477\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "temperatures = [0.5, 0.75, 1.0, 1.1, 1.2, 1.5, 2.0, 5.0, 10.0]\n",
        "\n",
        "for dataset, folder in datasets.items():\n",
        "  print(f\"Dataset {dataset}\")\n",
        "\n",
        "  for temperature in temperatures:\n",
        "    print(f\" - {temperature:<10} \", end = \"\")\n",
        "    input_path = f\"../../validation_dataset/{folder}/images/*.*\"\n",
        "    if torch.cuda.is_available():\n",
        "      !python evalAnomaly.py --input={input_path} --method=\"MSP\" --temperature={temperature}\n",
        "    else:\n",
        "      !python evalAnomaly.py --input={input_path} --method=\"MSP\" --temperature={temperature} --cpu\n",
        "\n",
        "  print(\"=\" * 55, end = \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ylMHF05vI-a"
      },
      "source": [
        "### Training models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We explored two training strategies:\n",
        "  - **Training from scratch** on the Cityscapes dataset.\n",
        "  - **Fine-tuning** a pretrained model. Leverage pretrained versions of the models, and further fine-tune them to adapt to our task."
      ],
      "metadata": {
        "id": "_vgngXFSqHKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "2wBN43GVNJ4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"../train\"\n",
        "data_dir = \"../cityscapes\""
      ],
      "metadata": {
        "id": "xx43qM7vVoGk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uvJ8DqNPGxg3"
      },
      "outputs": [],
      "source": [
        "def train_model(model: str, num_epochs: int, batch_size: int, stop_epoch: int = 20, pretrained: bool = False, resume: bool = False, fineTune: bool = False) -> None:\n",
        "\n",
        "  state_flag = f\"--state ../trained_models/{model}_pretrained.pth\" if pretrained else \"\"\n",
        "  resume_flag = \"--resume\" if resume else \"\"\n",
        "  finetune_flag = f\"--FineTune --loadWeights ../trained_models/{model}_pretrained.pth\" if fineTune else \"\"\n",
        "\n",
        "  # if model == \"bisenet\":\n",
        "  #     !gdown \"https://drive.usercontent.google.com/download?id=1Gj4eZrmdygA5c_y7N0KrmSRThoYjfjk-\" -O \"checkpoint.pth.tar\"\n",
        "\n",
        "  if fineTune:\n",
        "    savedir_name = f\"{model}_training_void_ft\"\n",
        "  else:\n",
        "    savedir_name = f\"{model}_training_void\"\n",
        "\n",
        "  !cd {base_dir} && python -W ignore main_v2.py \\\n",
        "    --savedir {savedir_name}\\\n",
        "    --datadir {data_dir} \\\n",
        "    --model {model} \\\n",
        "    --cuda \\\n",
        "    --num-epochs={num_epochs} \\\n",
        "    --epochs-save=1 \\\n",
        "    --batch-size={batch_size} \\\n",
        "    --stop-epoch={stop_epoch} \\\n",
        "    --decoder \\\n",
        "    {finetune_flag} \\\n",
        "    {state_flag} \\\n",
        "    {resume_flag}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ERFNet Training"
      ],
      "metadata": {
        "id": "PZu4VDaaqcLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resumed ERFNet training from a saved checkpoint due to Colab's session time limitations. The encoder and the first few decoder epochs were trained in a previous session. The training of the remaining part of the decoder is detailed below.\n",
        "\n",
        "The encoder was trained for 20 epochs, followed by the decoder, also trained for 20 epochs."
      ],
      "metadata": {
        "id": "9fY9IdXXkNKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"erfnet\", num_epochs=20, batch_size=6)"
      ],
      "metadata": {
        "id": "ZA-bn-VKrri-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"erfnet\", num_epochs=20, batch_size=6, resume=True)\n",
        "# %cd ../save\n",
        "# !zip -r erfnet_training_void.zip erfnet_training_void/"
      ],
      "metadata": {
        "id": "z6khSGrrijJv",
        "outputId": "72b35c79-820e-4244-ea3a-1177d5bdae3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== TRAINING ===========\n",
            "========== DECODER TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "Criterion: CrossEntropyLoss2d\n",
            "=> Loaded checkpoint at epoch 7)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.00036270892346860996\n",
            "loss: 0.7219 (epoch: 7, step: 0) // Avg time/img: 0.4481 s\n",
            "loss: 0.4191 (epoch: 7, step: 50) // Avg time/img: 0.1129 s\n",
            "loss: 0.4132 (epoch: 7, step: 100) // Avg time/img: 0.1105 s\n",
            "loss: 0.4109 (epoch: 7, step: 150) // Avg time/img: 0.1108 s\n",
            "loss: 0.4086 (epoch: 7, step: 200) // Avg time/img: 0.1111 s\n",
            "loss: 0.4134 (epoch: 7, step: 250) // Avg time/img: 0.1118 s\n",
            "loss: 0.4095 (epoch: 7, step: 300) // Avg time/img: 0.1124 s\n",
            "loss: 0.4031 (epoch: 7, step: 350) // Avg time/img: 0.1128 s\n",
            "loss: 0.4017 (epoch: 7, step: 400) // Avg time/img: 0.1131 s\n",
            "loss: 0.4001 (epoch: 7, step: 450) // Avg time/img: 0.1134 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.4987 (epoch: 7, step: 0) // Avg time/img: 0.0358 s\n",
            "VAL loss: 0.5223 (epoch: 7, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m46.21\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-007.pth (epoch: 7)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0003393063796290625\n",
            "loss: 0.5231 (epoch: 8, step: 0) // Avg time/img: 0.1299 s\n",
            "loss: 0.4124 (epoch: 8, step: 50) // Avg time/img: 0.1170 s\n",
            "loss: 0.3983 (epoch: 8, step: 100) // Avg time/img: 0.1157 s\n",
            "loss: 0.3952 (epoch: 8, step: 150) // Avg time/img: 0.1153 s\n",
            "loss: 0.3896 (epoch: 8, step: 200) // Avg time/img: 0.1154 s\n",
            "loss: 0.3846 (epoch: 8, step: 250) // Avg time/img: 0.1152 s\n",
            "loss: 0.3829 (epoch: 8, step: 300) // Avg time/img: 0.1153 s\n",
            "loss: 0.3819 (epoch: 8, step: 350) // Avg time/img: 0.1153 s\n",
            "loss: 0.3837 (epoch: 8, step: 400) // Avg time/img: 0.1153 s\n",
            "loss: 0.3859 (epoch: 8, step: 450) // Avg time/img: 0.1153 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.3988 (epoch: 8, step: 0) // Avg time/img: 0.0331 s\n",
            "VAL loss: 0.4829 (epoch: 8, step: 50) // Avg time/img: 0.0325 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m48.26\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-008.pth (epoch: 8)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.00031572293374467766\n",
            "loss: 0.4132 (epoch: 9, step: 0) // Avg time/img: 0.1257 s\n",
            "loss: 0.3681 (epoch: 9, step: 50) // Avg time/img: 0.1160 s\n",
            "loss: 0.3749 (epoch: 9, step: 100) // Avg time/img: 0.1155 s\n",
            "loss: 0.3666 (epoch: 9, step: 150) // Avg time/img: 0.1154 s\n",
            "loss: 0.3673 (epoch: 9, step: 200) // Avg time/img: 0.1154 s\n",
            "loss: 0.3674 (epoch: 9, step: 250) // Avg time/img: 0.1154 s\n",
            "loss: 0.3686 (epoch: 9, step: 300) // Avg time/img: 0.1155 s\n",
            "loss: 0.3723 (epoch: 9, step: 350) // Avg time/img: 0.1155 s\n",
            "loss: 0.377 (epoch: 9, step: 400) // Avg time/img: 0.1155 s\n",
            "loss: 0.3788 (epoch: 9, step: 450) // Avg time/img: 0.1155 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.3929 (epoch: 9, step: 0) // Avg time/img: 0.0332 s\n",
            "VAL loss: 0.4944 (epoch: 9, step: 50) // Avg time/img: 0.0327 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m49.98\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-009.pth (epoch: 9)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.00029194189645999016\n",
            "loss: 0.3715 (epoch: 10, step: 0) // Avg time/img: 0.1275 s\n",
            "loss: 0.3543 (epoch: 10, step: 50) // Avg time/img: 0.1155 s\n",
            "loss: 0.3543 (epoch: 10, step: 100) // Avg time/img: 0.1151 s\n",
            "loss: 0.3461 (epoch: 10, step: 150) // Avg time/img: 0.1151 s\n",
            "loss: 0.3442 (epoch: 10, step: 200) // Avg time/img: 0.1152 s\n",
            "loss: 0.3497 (epoch: 10, step: 250) // Avg time/img: 0.1152 s\n",
            "loss: 0.3524 (epoch: 10, step: 300) // Avg time/img: 0.1152 s\n",
            "loss: 0.3532 (epoch: 10, step: 350) // Avg time/img: 0.1152 s\n",
            "loss: 0.3549 (epoch: 10, step: 400) // Avg time/img: 0.1153 s\n",
            "loss: 0.358 (epoch: 10, step: 450) // Avg time/img: 0.1153 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.5204 (epoch: 10, step: 0) // Avg time/img: 0.0339 s\n",
            "VAL loss: 0.4853 (epoch: 10, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m50.79\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-010.pth (epoch: 10)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0002679433656340733\n",
            "loss: 0.4089 (epoch: 11, step: 0) // Avg time/img: 0.1157 s\n",
            "loss: 0.3426 (epoch: 11, step: 50) // Avg time/img: 0.1151 s\n",
            "loss: 0.3364 (epoch: 11, step: 100) // Avg time/img: 0.1154 s\n",
            "loss: 0.3358 (epoch: 11, step: 150) // Avg time/img: 0.1155 s\n",
            "loss: 0.3348 (epoch: 11, step: 200) // Avg time/img: 0.1154 s\n",
            "loss: 0.335 (epoch: 11, step: 250) // Avg time/img: 0.1154 s\n",
            "loss: 0.3383 (epoch: 11, step: 300) // Avg time/img: 0.1154 s\n",
            "loss: 0.3373 (epoch: 11, step: 350) // Avg time/img: 0.1154 s\n",
            "loss: 0.3387 (epoch: 11, step: 400) // Avg time/img: 0.1154 s\n",
            "loss: 0.3387 (epoch: 11, step: 450) // Avg time/img: 0.1154 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.3815 (epoch: 11, step: 0) // Avg time/img: 0.0510 s\n",
            "VAL loss: 0.4501 (epoch: 11, step: 50) // Avg time/img: 0.0329 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m51.25\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-011.pth (epoch: 11)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.00024370321958949772\n",
            "loss: 0.3604 (epoch: 12, step: 0) // Avg time/img: 0.1223 s\n",
            "loss: 0.3294 (epoch: 12, step: 50) // Avg time/img: 0.1160 s\n",
            "loss: 0.3217 (epoch: 12, step: 100) // Avg time/img: 0.1159 s\n",
            "loss: 0.3226 (epoch: 12, step: 150) // Avg time/img: 0.1158 s\n",
            "loss: 0.3232 (epoch: 12, step: 200) // Avg time/img: 0.1157 s\n",
            "loss: 0.3253 (epoch: 12, step: 250) // Avg time/img: 0.1157 s\n",
            "loss: 0.3251 (epoch: 12, step: 300) // Avg time/img: 0.1156 s\n",
            "loss: 0.3267 (epoch: 12, step: 350) // Avg time/img: 0.1156 s\n",
            "loss: 0.3261 (epoch: 12, step: 400) // Avg time/img: 0.1155 s\n",
            "loss: 0.3268 (epoch: 12, step: 450) // Avg time/img: 0.1156 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.4224 (epoch: 12, step: 0) // Avg time/img: 0.0388 s\n",
            "VAL loss: 0.4508 (epoch: 12, step: 50) // Avg time/img: 0.0322 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m52.43\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-012.pth (epoch: 12)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.00021919164527704348\n",
            "loss: 0.2532 (epoch: 13, step: 0) // Avg time/img: 0.1312 s\n",
            "loss: 0.3355 (epoch: 13, step: 50) // Avg time/img: 0.1162 s\n",
            "loss: 0.3207 (epoch: 13, step: 100) // Avg time/img: 0.1157 s\n",
            "loss: 0.3188 (epoch: 13, step: 150) // Avg time/img: 0.1156 s\n",
            "loss: 0.3192 (epoch: 13, step: 200) // Avg time/img: 0.1155 s\n",
            "loss: 0.316 (epoch: 13, step: 250) // Avg time/img: 0.1156 s\n",
            "loss: 0.3209 (epoch: 13, step: 300) // Avg time/img: 0.1155 s\n",
            "loss: 0.3238 (epoch: 13, step: 350) // Avg time/img: 0.1155 s\n",
            "loss: 0.3248 (epoch: 13, step: 400) // Avg time/img: 0.1155 s\n",
            "loss: 0.3245 (epoch: 13, step: 450) // Avg time/img: 0.1155 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.4047 (epoch: 13, step: 0) // Avg time/img: 0.0327 s\n",
            "VAL loss: 0.4417 (epoch: 13, step: 50) // Avg time/img: 0.0325 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m52.78\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-013.pth (epoch: 13)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.00019437089939938174\n",
            "loss: 0.2807 (epoch: 14, step: 0) // Avg time/img: 0.1183 s\n",
            "loss: 0.313 (epoch: 14, step: 50) // Avg time/img: 0.1159 s\n",
            "loss: 0.3095 (epoch: 14, step: 100) // Avg time/img: 0.1153 s\n",
            "loss: 0.3061 (epoch: 14, step: 150) // Avg time/img: 0.1154 s\n",
            "loss: 0.3067 (epoch: 14, step: 200) // Avg time/img: 0.1153 s\n",
            "loss: 0.3035 (epoch: 14, step: 250) // Avg time/img: 0.1153 s\n",
            "loss: 0.3002 (epoch: 14, step: 300) // Avg time/img: 0.1153 s\n",
            "loss: 0.3024 (epoch: 14, step: 350) // Avg time/img: 0.1153 s\n",
            "loss: 0.3041 (epoch: 14, step: 400) // Avg time/img: 0.1154 s\n",
            "loss: 0.3028 (epoch: 14, step: 450) // Avg time/img: 0.1154 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.3501 (epoch: 14, step: 0) // Avg time/img: 0.0339 s\n",
            "VAL loss: 0.4112 (epoch: 14, step: 50) // Avg time/img: 0.0330 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m56.10\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-014.pth (epoch: 14)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.00016919173095082495\n",
            "loss: 0.4052 (epoch: 15, step: 0) // Avg time/img: 0.1182 s\n",
            "loss: 0.2887 (epoch: 15, step: 50) // Avg time/img: 0.1161 s\n",
            "loss: 0.2932 (epoch: 15, step: 100) // Avg time/img: 0.1159 s\n",
            "loss: 0.2898 (epoch: 15, step: 150) // Avg time/img: 0.1156 s\n",
            "loss: 0.2941 (epoch: 15, step: 200) // Avg time/img: 0.1155 s\n",
            "loss: 0.2933 (epoch: 15, step: 250) // Avg time/img: 0.1155 s\n",
            "loss: 0.2908 (epoch: 15, step: 300) // Avg time/img: 0.1154 s\n",
            "loss: 0.2887 (epoch: 15, step: 350) // Avg time/img: 0.1155 s\n",
            "loss: 0.29 (epoch: 15, step: 400) // Avg time/img: 0.1155 s\n",
            "loss: 0.2909 (epoch: 15, step: 450) // Avg time/img: 0.1155 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.3678 (epoch: 15, step: 0) // Avg time/img: 0.0308 s\n",
            "VAL loss: 0.4405 (epoch: 15, step: 50) // Avg time/img: 0.0326 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m55.36\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.00014358729437462936\n",
            "loss: 0.2386 (epoch: 16, step: 0) // Avg time/img: 0.1259 s\n",
            "loss: 0.2901 (epoch: 16, step: 50) // Avg time/img: 0.1160 s\n",
            "loss: 0.2864 (epoch: 16, step: 100) // Avg time/img: 0.1156 s\n",
            "loss: 0.2888 (epoch: 16, step: 150) // Avg time/img: 0.1156 s\n",
            "loss: 0.287 (epoch: 16, step: 200) // Avg time/img: 0.1156 s\n",
            "loss: 0.2846 (epoch: 16, step: 250) // Avg time/img: 0.1155 s\n",
            "loss: 0.2822 (epoch: 16, step: 300) // Avg time/img: 0.1155 s\n",
            "loss: 0.2829 (epoch: 16, step: 350) // Avg time/img: 0.1155 s\n",
            "loss: 0.2834 (epoch: 16, step: 400) // Avg time/img: 0.1155 s\n",
            "loss: 0.2837 (epoch: 16, step: 450) // Avg time/img: 0.1156 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.3128 (epoch: 16, step: 0) // Avg time/img: 0.0333 s\n",
            "VAL loss: 0.4063 (epoch: 16, step: 50) // Avg time/img: 0.0332 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m56.11\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-016.pth (epoch: 16)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.00011746189430880188\n",
            "loss: 0.2711 (epoch: 17, step: 0) // Avg time/img: 0.1172 s\n",
            "loss: 0.2614 (epoch: 17, step: 50) // Avg time/img: 0.1155 s\n",
            "loss: 0.2643 (epoch: 17, step: 100) // Avg time/img: 0.1154 s\n",
            "loss: 0.2713 (epoch: 17, step: 150) // Avg time/img: 0.1160 s\n",
            "loss: 0.2715 (epoch: 17, step: 200) // Avg time/img: 0.1158 s\n",
            "loss: 0.2707 (epoch: 17, step: 250) // Avg time/img: 0.1158 s\n",
            "loss: 0.2721 (epoch: 17, step: 300) // Avg time/img: 0.1157 s\n",
            "loss: 0.2733 (epoch: 17, step: 350) // Avg time/img: 0.1156 s\n",
            "loss: 0.2731 (epoch: 17, step: 400) // Avg time/img: 0.1155 s\n",
            "loss: 0.2727 (epoch: 17, step: 450) // Avg time/img: 0.1155 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.3424 (epoch: 17, step: 0) // Avg time/img: 0.0325 s\n",
            "VAL loss: 0.4011 (epoch: 17, step: 50) // Avg time/img: 0.0322 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m56.48\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-017.pth (epoch: 17)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683728e-05\n",
            "loss: 0.2505 (epoch: 18, step: 0) // Avg time/img: 0.1193 s\n",
            "loss: 0.2686 (epoch: 18, step: 50) // Avg time/img: 0.1156 s\n",
            "loss: 0.268 (epoch: 18, step: 100) // Avg time/img: 0.1155 s\n",
            "loss: 0.2641 (epoch: 18, step: 150) // Avg time/img: 0.1154 s\n",
            "loss: 0.2657 (epoch: 18, step: 200) // Avg time/img: 0.1155 s\n",
            "loss: 0.2629 (epoch: 18, step: 250) // Avg time/img: 0.1155 s\n",
            "loss: 0.2624 (epoch: 18, step: 300) // Avg time/img: 0.1155 s\n",
            "loss: 0.2622 (epoch: 18, step: 350) // Avg time/img: 0.1155 s\n",
            "loss: 0.2618 (epoch: 18, step: 400) // Avg time/img: 0.1154 s\n",
            "loss: 0.2627 (epoch: 18, step: 450) // Avg time/img: 0.1154 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.2776 (epoch: 18, step: 0) // Avg time/img: 0.0330 s\n",
            "VAL loss: 0.3936 (epoch: 18, step: 50) // Avg time/img: 0.0332 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m58.48\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-018.pth (epoch: 18)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-05\n",
            "loss: 0.2165 (epoch: 19, step: 0) // Avg time/img: 0.1349 s\n",
            "loss: 0.2504 (epoch: 19, step: 50) // Avg time/img: 0.1169 s\n",
            "loss: 0.2477 (epoch: 19, step: 100) // Avg time/img: 0.1160 s\n",
            "loss: 0.249 (epoch: 19, step: 150) // Avg time/img: 0.1159 s\n",
            "loss: 0.2509 (epoch: 19, step: 200) // Avg time/img: 0.1158 s\n",
            "loss: 0.2532 (epoch: 19, step: 250) // Avg time/img: 0.1156 s\n",
            "loss: 0.2521 (epoch: 19, step: 300) // Avg time/img: 0.1157 s\n",
            "loss: 0.2522 (epoch: 19, step: 350) // Avg time/img: 0.1157 s\n",
            "loss: 0.255 (epoch: 19, step: 400) // Avg time/img: 0.1156 s\n",
            "loss: 0.2533 (epoch: 19, step: 450) // Avg time/img: 0.1156 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.2827 (epoch: 19, step: 0) // Avg time/img: 0.0474 s\n",
            "VAL loss: 0.3932 (epoch: 19, step: 50) // Avg time/img: 0.0331 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m59.74\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void/model-019.pth (epoch: 19)\n",
            "save: ../save/erfnet_training_void/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-05\n",
            "loss: 0.2424 (epoch: 20, step: 0) // Avg time/img: 0.1235 s\n",
            "loss: 0.2455 (epoch: 20, step: 50) // Avg time/img: 0.1137 s\n",
            "loss: 0.2453 (epoch: 20, step: 100) // Avg time/img: 0.1144 s\n",
            "loss: 0.2419 (epoch: 20, step: 150) // Avg time/img: 0.1146 s\n",
            "loss: 0.2421 (epoch: 20, step: 200) // Avg time/img: 0.1147 s\n",
            "loss: 0.2425 (epoch: 20, step: 250) // Avg time/img: 0.1147 s\n",
            "loss: 0.2433 (epoch: 20, step: 300) // Avg time/img: 0.1149 s\n",
            "loss: 0.2435 (epoch: 20, step: 350) // Avg time/img: 0.1149 s\n",
            "loss: 0.2441 (epoch: 20, step: 400) // Avg time/img: 0.1149 s\n",
            "loss: 0.2444 (epoch: 20, step: 450) // Avg time/img: 0.1150 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.352 (epoch: 20, step: 0) // Avg time/img: 0.0373 s\n",
            "VAL loss: 0.3984 (epoch: 20, step: 50) // Avg time/img: 0.0328 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m59.03\u001b[0m %\n",
            "save: ../save/erfnet_training_void/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ERFNet Fine-Tuning"
      ],
      "metadata": {
        "id": "jMpBXEAotIdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initiate the fine-tuning process of ERFNet over 20 epochs with a batch size of 6, setting the parameter `fineTune=True`. The pretrained ERFNet model on the Cityscapes dataset is fine-tuned including the 20th void class, which represents background or unannotated regions."
      ],
      "metadata": {
        "id": "D-xHAiLbcOZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"erfnet\", num_epochs=20, batch_size=6, fineTune=True)\n",
        "# %cd ../save\n",
        "# !zip -r erfnet_training_void_ft.zip erfnet_training_void_ft/"
      ],
      "metadata": {
        "id": "bbeaR8ZBor7Q",
        "outputId": "ed4faf0c-cb33-4235-d580-682e773fd146",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model erfnet with weights ../trained_models/erfnet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "Criterion: <class 'utils.losses.ce_loss.CrossEntropyLoss2d'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 0.5082 (epoch: 1, step: 0) // Avg time/img: 0.3150 s\n",
            "loss: 0.3931 (epoch: 1, step: 50) // Avg time/img: 0.0396 s\n",
            "loss: 0.3815 (epoch: 1, step: 100) // Avg time/img: 0.0371 s\n",
            "loss: 0.3817 (epoch: 1, step: 150) // Avg time/img: 0.0363 s\n",
            "loss: 0.3752 (epoch: 1, step: 200) // Avg time/img: 0.0360 s\n",
            "loss: 0.3759 (epoch: 1, step: 250) // Avg time/img: 0.0357 s\n",
            "loss: 0.3744 (epoch: 1, step: 300) // Avg time/img: 0.0355 s\n",
            "loss: 0.3768 (epoch: 1, step: 350) // Avg time/img: 0.0353 s\n",
            "loss: 0.3742 (epoch: 1, step: 400) // Avg time/img: 0.0353 s\n",
            "loss: 0.3715 (epoch: 1, step: 450) // Avg time/img: 0.0352 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.4713 (epoch: 1, step: 0) // Avg time/img: 0.0335 s\n",
            "VAL loss: 0.5808 (epoch: 1, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.79\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void_ft/model-001.pth (epoch: 1)\n",
            "save: ../save/erfnet_training_void_ft/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  4.774426908107499e-05\n",
            "loss: 0.3874 (epoch: 2, step: 0) // Avg time/img: 0.0440 s\n",
            "loss: 0.3255 (epoch: 2, step: 50) // Avg time/img: 0.0349 s\n",
            "loss: 0.3436 (epoch: 2, step: 100) // Avg time/img: 0.0347 s\n",
            "loss: 0.3431 (epoch: 2, step: 150) // Avg time/img: 0.0348 s\n",
            "loss: 0.3414 (epoch: 2, step: 200) // Avg time/img: 0.0348 s\n",
            "loss: 0.3376 (epoch: 2, step: 250) // Avg time/img: 0.0348 s\n",
            "loss: 0.3352 (epoch: 2, step: 300) // Avg time/img: 0.0348 s\n",
            "loss: 0.3338 (epoch: 2, step: 350) // Avg time/img: 0.0348 s\n",
            "loss: 0.3301 (epoch: 2, step: 400) // Avg time/img: 0.0349 s\n",
            "loss: 0.3285 (epoch: 2, step: 450) // Avg time/img: 0.0348 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.4181 (epoch: 2, step: 0) // Avg time/img: 0.0424 s\n",
            "VAL loss: 0.5248 (epoch: 2, step: 50) // Avg time/img: 0.0318 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.61\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  4.547662880414811e-05\n",
            "loss: 0.2253 (epoch: 3, step: 0) // Avg time/img: 0.0437 s\n",
            "loss: 0.3145 (epoch: 3, step: 50) // Avg time/img: 0.0351 s\n",
            "loss: 0.3047 (epoch: 3, step: 100) // Avg time/img: 0.0350 s\n",
            "loss: 0.3005 (epoch: 3, step: 150) // Avg time/img: 0.0349 s\n",
            "loss: 0.2968 (epoch: 3, step: 200) // Avg time/img: 0.0349 s\n",
            "loss: 0.2961 (epoch: 3, step: 250) // Avg time/img: 0.0348 s\n",
            "loss: 0.2941 (epoch: 3, step: 300) // Avg time/img: 0.0348 s\n",
            "loss: 0.2932 (epoch: 3, step: 350) // Avg time/img: 0.0347 s\n",
            "loss: 0.2926 (epoch: 3, step: 400) // Avg time/img: 0.0347 s\n",
            "loss: 0.2912 (epoch: 3, step: 450) // Avg time/img: 0.0348 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.4007 (epoch: 3, step: 0) // Avg time/img: 0.0337 s\n",
            "VAL loss: 0.4817 (epoch: 3, step: 50) // Avg time/img: 0.0308 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.70\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  4.319634861514096e-05\n",
            "loss: 0.4063 (epoch: 4, step: 0) // Avg time/img: 0.0359 s\n",
            "loss: 0.2784 (epoch: 4, step: 50) // Avg time/img: 0.0349 s\n",
            "loss: 0.276 (epoch: 4, step: 100) // Avg time/img: 0.0345 s\n",
            "loss: 0.2732 (epoch: 4, step: 150) // Avg time/img: 0.0348 s\n",
            "loss: 0.2682 (epoch: 4, step: 200) // Avg time/img: 0.0348 s\n",
            "loss: 0.2671 (epoch: 4, step: 250) // Avg time/img: 0.0348 s\n",
            "loss: 0.2667 (epoch: 4, step: 300) // Avg time/img: 0.0348 s\n",
            "loss: 0.2634 (epoch: 4, step: 350) // Avg time/img: 0.0348 s\n",
            "loss: 0.2608 (epoch: 4, step: 400) // Avg time/img: 0.0348 s\n",
            "loss: 0.2598 (epoch: 4, step: 450) // Avg time/img: 0.0348 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.3634 (epoch: 4, step: 0) // Avg time/img: 0.0346 s\n",
            "VAL loss: 0.4455 (epoch: 4, step: 50) // Avg time/img: 0.0313 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.78\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  4.090260730254292e-05\n",
            "loss: 0.2218 (epoch: 5, step: 0) // Avg time/img: 0.0365 s\n",
            "loss: 0.2376 (epoch: 5, step: 50) // Avg time/img: 0.0351 s\n",
            "loss: 0.2437 (epoch: 5, step: 100) // Avg time/img: 0.0352 s\n",
            "loss: 0.2443 (epoch: 5, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.246 (epoch: 5, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2455 (epoch: 5, step: 250) // Avg time/img: 0.0351 s\n",
            "loss: 0.2433 (epoch: 5, step: 300) // Avg time/img: 0.0351 s\n",
            "loss: 0.2439 (epoch: 5, step: 350) // Avg time/img: 0.0351 s\n",
            "loss: 0.2423 (epoch: 5, step: 400) // Avg time/img: 0.0351 s\n",
            "loss: 0.2414 (epoch: 5, step: 450) // Avg time/img: 0.0351 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.3386 (epoch: 5, step: 0) // Avg time/img: 0.0343 s\n",
            "VAL loss: 0.4376 (epoch: 5, step: 50) // Avg time/img: 0.0320 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.69\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  3.859447533617852e-05\n",
            "loss: 0.1717 (epoch: 6, step: 0) // Avg time/img: 0.0352 s\n",
            "loss: 0.2323 (epoch: 6, step: 50) // Avg time/img: 0.0349 s\n",
            "loss: 0.2348 (epoch: 6, step: 100) // Avg time/img: 0.0351 s\n",
            "loss: 0.2352 (epoch: 6, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.2342 (epoch: 6, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2342 (epoch: 6, step: 250) // Avg time/img: 0.0351 s\n",
            "loss: 0.2342 (epoch: 6, step: 300) // Avg time/img: 0.0350 s\n",
            "loss: 0.2325 (epoch: 6, step: 350) // Avg time/img: 0.0350 s\n",
            "loss: 0.2332 (epoch: 6, step: 400) // Avg time/img: 0.0350 s\n",
            "loss: 0.2319 (epoch: 6, step: 450) // Avg time/img: 0.0350 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.3283 (epoch: 6, step: 0) // Avg time/img: 0.0325 s\n",
            "VAL loss: 0.4263 (epoch: 6, step: 50) // Avg time/img: 0.0311 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.55\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  3.6270892346861e-05\n",
            "loss: 0.2248 (epoch: 7, step: 0) // Avg time/img: 0.0364 s\n",
            "loss: 0.2269 (epoch: 7, step: 50) // Avg time/img: 0.0347 s\n",
            "loss: 0.2269 (epoch: 7, step: 100) // Avg time/img: 0.0349 s\n",
            "loss: 0.2269 (epoch: 7, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.2255 (epoch: 7, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 0.2264 (epoch: 7, step: 250) // Avg time/img: 0.0352 s\n",
            "loss: 0.2271 (epoch: 7, step: 300) // Avg time/img: 0.0352 s\n",
            "loss: 0.228 (epoch: 7, step: 350) // Avg time/img: 0.0352 s\n",
            "loss: 0.228 (epoch: 7, step: 400) // Avg time/img: 0.0352 s\n",
            "loss: 0.2272 (epoch: 7, step: 450) // Avg time/img: 0.0351 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.3223 (epoch: 7, step: 0) // Avg time/img: 0.0344 s\n",
            "VAL loss: 0.4164 (epoch: 7, step: 50) // Avg time/img: 0.0313 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.22\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  3.393063796290625e-05\n",
            "loss: 0.2813 (epoch: 8, step: 0) // Avg time/img: 0.0466 s\n",
            "loss: 0.2228 (epoch: 8, step: 50) // Avg time/img: 0.0360 s\n",
            "loss: 0.2253 (epoch: 8, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 0.2218 (epoch: 8, step: 150) // Avg time/img: 0.0358 s\n",
            "loss: 0.2224 (epoch: 8, step: 200) // Avg time/img: 0.0358 s\n",
            "loss: 0.2218 (epoch: 8, step: 250) // Avg time/img: 0.0358 s\n",
            "loss: 0.2216 (epoch: 8, step: 300) // Avg time/img: 0.0358 s\n",
            "loss: 0.2223 (epoch: 8, step: 350) // Avg time/img: 0.0357 s\n",
            "loss: 0.2217 (epoch: 8, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2212 (epoch: 8, step: 450) // Avg time/img: 0.0356 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.3405 (epoch: 8, step: 0) // Avg time/img: 0.0289 s\n",
            "VAL loss: 0.408 (epoch: 8, step: 50) // Avg time/img: 0.0319 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.33\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-008.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  3.157229337446777e-05\n",
            "loss: 0.2076 (epoch: 9, step: 0) // Avg time/img: 0.0449 s\n",
            "loss: 0.2213 (epoch: 9, step: 50) // Avg time/img: 0.0359 s\n",
            "loss: 0.2209 (epoch: 9, step: 100) // Avg time/img: 0.0357 s\n",
            "loss: 0.2216 (epoch: 9, step: 150) // Avg time/img: 0.0358 s\n",
            "loss: 0.2211 (epoch: 9, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 0.221 (epoch: 9, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 0.2209 (epoch: 9, step: 300) // Avg time/img: 0.0356 s\n",
            "loss: 0.2202 (epoch: 9, step: 350) // Avg time/img: 0.0356 s\n",
            "loss: 0.2199 (epoch: 9, step: 400) // Avg time/img: 0.0356 s\n",
            "loss: 0.2198 (epoch: 9, step: 450) // Avg time/img: 0.0355 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.3174 (epoch: 9, step: 0) // Avg time/img: 0.0318 s\n",
            "VAL loss: 0.4065 (epoch: 9, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.59\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  2.9194189645999014e-05\n",
            "loss: 0.1964 (epoch: 10, step: 0) // Avg time/img: 0.0440 s\n",
            "loss: 0.2212 (epoch: 10, step: 50) // Avg time/img: 0.0356 s\n",
            "loss: 0.2185 (epoch: 10, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 0.2209 (epoch: 10, step: 150) // Avg time/img: 0.0353 s\n",
            "loss: 0.2196 (epoch: 10, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2185 (epoch: 10, step: 250) // Avg time/img: 0.0352 s\n",
            "loss: 0.2172 (epoch: 10, step: 300) // Avg time/img: 0.0352 s\n",
            "loss: 0.216 (epoch: 10, step: 350) // Avg time/img: 0.0351 s\n",
            "loss: 0.2171 (epoch: 10, step: 400) // Avg time/img: 0.0351 s\n",
            "loss: 0.2163 (epoch: 10, step: 450) // Avg time/img: 0.0351 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.3056 (epoch: 10, step: 0) // Avg time/img: 0.0481 s\n",
            "VAL loss: 0.403 (epoch: 10, step: 50) // Avg time/img: 0.0313 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.39\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  2.679433656340733e-05\n",
            "loss: 0.184 (epoch: 11, step: 0) // Avg time/img: 0.0503 s\n",
            "loss: 0.218 (epoch: 11, step: 50) // Avg time/img: 0.0354 s\n",
            "loss: 0.2174 (epoch: 11, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2178 (epoch: 11, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 0.2158 (epoch: 11, step: 200) // Avg time/img: 0.0352 s\n",
            "loss: 0.2173 (epoch: 11, step: 250) // Avg time/img: 0.0351 s\n",
            "loss: 0.2164 (epoch: 11, step: 300) // Avg time/img: 0.0351 s\n",
            "loss: 0.2165 (epoch: 11, step: 350) // Avg time/img: 0.0351 s\n",
            "loss: 0.2156 (epoch: 11, step: 400) // Avg time/img: 0.0351 s\n",
            "loss: 0.2154 (epoch: 11, step: 450) // Avg time/img: 0.0351 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.316 (epoch: 11, step: 0) // Avg time/img: 0.0337 s\n",
            "VAL loss: 0.4 (epoch: 11, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.91\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/erfnet_training_void_ft/model-011.pth (epoch: 11)\n",
            "save: ../save/erfnet_training_void_ft/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  2.437032195894977e-05\n",
            "loss: 0.1662 (epoch: 12, step: 0) // Avg time/img: 0.0356 s\n",
            "loss: 0.211 (epoch: 12, step: 50) // Avg time/img: 0.0351 s\n",
            "loss: 0.2116 (epoch: 12, step: 100) // Avg time/img: 0.0351 s\n",
            "loss: 0.212 (epoch: 12, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 0.2107 (epoch: 12, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2111 (epoch: 12, step: 250) // Avg time/img: 0.0351 s\n",
            "loss: 0.2131 (epoch: 12, step: 300) // Avg time/img: 0.0351 s\n",
            "loss: 0.212 (epoch: 12, step: 350) // Avg time/img: 0.0351 s\n",
            "loss: 0.2115 (epoch: 12, step: 400) // Avg time/img: 0.0351 s\n",
            "loss: 0.2126 (epoch: 12, step: 450) // Avg time/img: 0.0351 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.2941 (epoch: 12, step: 0) // Avg time/img: 0.0330 s\n",
            "VAL loss: 0.3948 (epoch: 12, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.59\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  2.191916452770435e-05\n",
            "loss: 0.2098 (epoch: 13, step: 0) // Avg time/img: 0.0359 s\n",
            "loss: 0.2177 (epoch: 13, step: 50) // Avg time/img: 0.0354 s\n",
            "loss: 0.2145 (epoch: 13, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 0.2156 (epoch: 13, step: 150) // Avg time/img: 0.0351 s\n",
            "loss: 0.213 (epoch: 13, step: 200) // Avg time/img: 0.0351 s\n",
            "loss: 0.2136 (epoch: 13, step: 250) // Avg time/img: 0.0351 s\n",
            "loss: 0.2117 (epoch: 13, step: 300) // Avg time/img: 0.0351 s\n",
            "loss: 0.213 (epoch: 13, step: 350) // Avg time/img: 0.0351 s\n",
            "loss: 0.2128 (epoch: 13, step: 400) // Avg time/img: 0.0351 s\n",
            "loss: 0.2125 (epoch: 13, step: 450) // Avg time/img: 0.0351 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.3144 (epoch: 13, step: 0) // Avg time/img: 0.0330 s\n",
            "VAL loss: 0.3943 (epoch: 13, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.61\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  1.9437089939938174e-05\n",
            "loss: 0.2488 (epoch: 14, step: 0) // Avg time/img: 0.0421 s\n",
            "loss: 0.2067 (epoch: 14, step: 50) // Avg time/img: 0.0350 s\n",
            "loss: 0.2126 (epoch: 14, step: 100) // Avg time/img: 0.0350 s\n",
            "loss: 0.2125 (epoch: 14, step: 150) // Avg time/img: 0.0350 s\n",
            "loss: 0.2124 (epoch: 14, step: 200) // Avg time/img: 0.0350 s\n",
            "loss: 0.2126 (epoch: 14, step: 250) // Avg time/img: 0.0350 s\n",
            "loss: 0.2118 (epoch: 14, step: 300) // Avg time/img: 0.0350 s\n",
            "loss: 0.2116 (epoch: 14, step: 350) // Avg time/img: 0.0350 s\n",
            "loss: 0.2119 (epoch: 14, step: 400) // Avg time/img: 0.0352 s\n",
            "loss: 0.2111 (epoch: 14, step: 450) // Avg time/img: 0.0353 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.3224 (epoch: 14, step: 0) // Avg time/img: 0.0328 s\n",
            "VAL loss: 0.3942 (epoch: 14, step: 50) // Avg time/img: 0.0324 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.46\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  1.6919173095082493e-05\n",
            "loss: 0.2302 (epoch: 15, step: 0) // Avg time/img: 0.0425 s\n",
            "loss: 0.21 (epoch: 15, step: 50) // Avg time/img: 0.0353 s\n",
            "loss: 0.2094 (epoch: 15, step: 100) // Avg time/img: 0.0351 s\n",
            "loss: 0.2114 (epoch: 15, step: 150) // Avg time/img: 0.0351 s\n",
            "loss: 0.2108 (epoch: 15, step: 200) // Avg time/img: 0.0350 s\n",
            "loss: 0.2091 (epoch: 15, step: 250) // Avg time/img: 0.0350 s\n",
            "loss: 0.2095 (epoch: 15, step: 300) // Avg time/img: 0.0351 s\n",
            "loss: 0.2103 (epoch: 15, step: 350) // Avg time/img: 0.0350 s\n",
            "loss: 0.2087 (epoch: 15, step: 400) // Avg time/img: 0.0350 s\n",
            "loss: 0.2091 (epoch: 15, step: 450) // Avg time/img: 0.0350 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.3213 (epoch: 15, step: 0) // Avg time/img: 0.0292 s\n",
            "VAL loss: 0.3905 (epoch: 15, step: 50) // Avg time/img: 0.0310 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.73\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  1.4358729437462937e-05\n",
            "loss: 0.2116 (epoch: 16, step: 0) // Avg time/img: 0.0446 s\n",
            "loss: 0.2121 (epoch: 16, step: 50) // Avg time/img: 0.0360 s\n",
            "loss: 0.2115 (epoch: 16, step: 100) // Avg time/img: 0.0363 s\n",
            "loss: 0.2114 (epoch: 16, step: 150) // Avg time/img: 0.0361 s\n",
            "loss: 0.2088 (epoch: 16, step: 200) // Avg time/img: 0.0362 s\n",
            "loss: 0.207 (epoch: 16, step: 250) // Avg time/img: 0.0362 s\n",
            "loss: 0.2082 (epoch: 16, step: 300) // Avg time/img: 0.0361 s\n",
            "loss: 0.2078 (epoch: 16, step: 350) // Avg time/img: 0.0362 s\n",
            "loss: 0.209 (epoch: 16, step: 400) // Avg time/img: 0.0361 s\n",
            "loss: 0.2091 (epoch: 16, step: 450) // Avg time/img: 0.0362 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.3103 (epoch: 16, step: 0) // Avg time/img: 0.0342 s\n",
            "VAL loss: 0.3923 (epoch: 16, step: 50) // Avg time/img: 0.0324 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.82\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  1.1746189430880188e-05\n",
            "loss: 0.256 (epoch: 17, step: 0) // Avg time/img: 0.0379 s\n",
            "loss: 0.2083 (epoch: 17, step: 50) // Avg time/img: 0.0366 s\n",
            "loss: 0.2065 (epoch: 17, step: 100) // Avg time/img: 0.0365 s\n",
            "loss: 0.2091 (epoch: 17, step: 150) // Avg time/img: 0.0366 s\n",
            "loss: 0.2106 (epoch: 17, step: 200) // Avg time/img: 0.0364 s\n",
            "loss: 0.21 (epoch: 17, step: 250) // Avg time/img: 0.0364 s\n",
            "loss: 0.2109 (epoch: 17, step: 300) // Avg time/img: 0.0363 s\n",
            "loss: 0.21 (epoch: 17, step: 350) // Avg time/img: 0.0363 s\n",
            "loss: 0.2091 (epoch: 17, step: 400) // Avg time/img: 0.0363 s\n",
            "loss: 0.2094 (epoch: 17, step: 450) // Avg time/img: 0.0362 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.3276 (epoch: 17, step: 0) // Avg time/img: 0.0360 s\n",
            "VAL loss: 0.3902 (epoch: 17, step: 50) // Avg time/img: 0.0323 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.86\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  9.066760365683729e-06\n",
            "loss: 0.1541 (epoch: 18, step: 0) // Avg time/img: 0.0543 s\n",
            "loss: 0.2083 (epoch: 18, step: 50) // Avg time/img: 0.0363 s\n",
            "loss: 0.2081 (epoch: 18, step: 100) // Avg time/img: 0.0361 s\n",
            "loss: 0.2119 (epoch: 18, step: 150) // Avg time/img: 0.0361 s\n",
            "loss: 0.209 (epoch: 18, step: 200) // Avg time/img: 0.0361 s\n",
            "loss: 0.2094 (epoch: 18, step: 250) // Avg time/img: 0.0361 s\n",
            "loss: 0.2103 (epoch: 18, step: 300) // Avg time/img: 0.0361 s\n",
            "loss: 0.2095 (epoch: 18, step: 350) // Avg time/img: 0.0361 s\n",
            "loss: 0.2103 (epoch: 18, step: 400) // Avg time/img: 0.0361 s\n",
            "loss: 0.2096 (epoch: 18, step: 450) // Avg time/img: 0.0362 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.3042 (epoch: 18, step: 0) // Avg time/img: 0.0358 s\n",
            "VAL loss: 0.3866 (epoch: 18, step: 50) // Avg time/img: 0.0325 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.87\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  6.294627058970836e-06\n",
            "loss: 0.2169 (epoch: 19, step: 0) // Avg time/img: 0.0337 s\n",
            "loss: 0.206 (epoch: 19, step: 50) // Avg time/img: 0.0363 s\n",
            "loss: 0.205 (epoch: 19, step: 100) // Avg time/img: 0.0361 s\n",
            "loss: 0.2071 (epoch: 19, step: 150) // Avg time/img: 0.0361 s\n",
            "loss: 0.2067 (epoch: 19, step: 200) // Avg time/img: 0.0360 s\n",
            "loss: 0.2067 (epoch: 19, step: 250) // Avg time/img: 0.0361 s\n",
            "loss: 0.2081 (epoch: 19, step: 300) // Avg time/img: 0.0362 s\n",
            "loss: 0.2075 (epoch: 19, step: 350) // Avg time/img: 0.0361 s\n",
            "loss: 0.2076 (epoch: 19, step: 400) // Avg time/img: 0.0361 s\n",
            "loss: 0.2082 (epoch: 19, step: 450) // Avg time/img: 0.0362 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.3074 (epoch: 19, step: 0) // Avg time/img: 0.0335 s\n",
            "VAL loss: 0.3874 (epoch: 19, step: 50) // Avg time/img: 0.0323 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.65\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  3.373207119183911e-06\n",
            "loss: 0.2585 (epoch: 20, step: 0) // Avg time/img: 0.0488 s\n",
            "loss: 0.2171 (epoch: 20, step: 50) // Avg time/img: 0.0362 s\n",
            "loss: 0.2142 (epoch: 20, step: 100) // Avg time/img: 0.0363 s\n",
            "loss: 0.2106 (epoch: 20, step: 150) // Avg time/img: 0.0361 s\n",
            "loss: 0.2108 (epoch: 20, step: 200) // Avg time/img: 0.0362 s\n",
            "loss: 0.2092 (epoch: 20, step: 250) // Avg time/img: 0.0362 s\n",
            "loss: 0.209 (epoch: 20, step: 300) // Avg time/img: 0.0362 s\n",
            "loss: 0.2093 (epoch: 20, step: 350) // Avg time/img: 0.0362 s\n",
            "loss: 0.2091 (epoch: 20, step: 400) // Avg time/img: 0.0361 s\n",
            "loss: 0.2089 (epoch: 20, step: 450) // Avg time/img: 0.0361 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.3013 (epoch: 20, step: 0) // Avg time/img: 0.0331 s\n",
            "VAL loss: 0.3859 (epoch: 20, step: 50) // Avg time/img: 0.0327 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m71.68\u001b[0m %\n",
            "save: ../save/erfnet_training_void_ft/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BiSeNet Training"
      ],
      "metadata": {
        "id": "rTIaOpXWEYV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of directly fine-tuning a pretrained model, we opted to train BiSeNet from scratch for 40 epochs.\n",
        "\n",
        "In the first run, due to GPU time limitations on Google Colab, the training is intentionally interrupted after 20 epochs by setting the parameter `stop_epoch` equal to 20. Remember to set `num_epochs` to 40 from the beginning to ensure that the learning rate scheduler behaves correctly across the full training process. The process is then resumed in the following run from epoch 21 using the `--resume` flag."
      ],
      "metadata": {
        "id": "R-5bPbOzoCpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"bisenet\", num_epochs=40, batch_size=6, stop_epoch=20)\n",
        "# %cd ../save\n",
        "# !zip -r bisenet_training_void.zip bisenet_training_void/"
      ],
      "metadata": {
        "id": "sR7HHfo0lKpX",
        "outputId": "468e0bf3-882b-4026-ccb1-2d29a81b2103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 301MB/s]\n",
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.025\n",
            "loss: 5.835 (epoch: 1, step: 0) // Avg time/img: 0.4772 s\n",
            "loss: 3.602 (epoch: 1, step: 50) // Avg time/img: 0.0502 s\n",
            "loss: 3.355 (epoch: 1, step: 100) // Avg time/img: 0.0460 s\n",
            "loss: 3.206 (epoch: 1, step: 150) // Avg time/img: 0.0446 s\n",
            "loss: 3.098 (epoch: 1, step: 200) // Avg time/img: 0.0440 s\n",
            "loss: 3.043 (epoch: 1, step: 250) // Avg time/img: 0.0438 s\n",
            "loss: 2.982 (epoch: 1, step: 300) // Avg time/img: 0.0437 s\n",
            "loss: 2.926 (epoch: 1, step: 350) // Avg time/img: 0.0436 s\n",
            "loss: 2.897 (epoch: 1, step: 400) // Avg time/img: 0.0435 s\n",
            "loss: 2.871 (epoch: 1, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 2.136 (epoch: 1, step: 0) // Avg time/img: 0.0375 s\n",
            "VAL loss: 2.807 (epoch: 1, step: 50) // Avg time/img: 0.0257 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m23.95\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-001.pth (epoch: 1)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.02443679034375874\n",
            "loss: 2.322 (epoch: 2, step: 0) // Avg time/img: 0.0623 s\n",
            "loss: 2.606 (epoch: 2, step: 50) // Avg time/img: 0.0436 s\n",
            "loss: 2.599 (epoch: 2, step: 100) // Avg time/img: 0.0431 s\n",
            "loss: 2.579 (epoch: 2, step: 150) // Avg time/img: 0.0431 s\n",
            "loss: 2.584 (epoch: 2, step: 200) // Avg time/img: 0.0430 s\n",
            "loss: 2.571 (epoch: 2, step: 250) // Avg time/img: 0.0430 s\n",
            "loss: 2.566 (epoch: 2, step: 300) // Avg time/img: 0.0430 s\n",
            "loss: 2.555 (epoch: 2, step: 350) // Avg time/img: 0.0429 s\n",
            "loss: 2.553 (epoch: 2, step: 400) // Avg time/img: 0.0429 s\n",
            "loss: 2.556 (epoch: 2, step: 450) // Avg time/img: 0.0428 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 1.781 (epoch: 2, step: 0) // Avg time/img: 0.0251 s\n",
            "VAL loss: 2.231 (epoch: 2, step: 50) // Avg time/img: 0.0244 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.13\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-002.pth (epoch: 2)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.023872134540537493\n",
            "loss: 2.361 (epoch: 3, step: 0) // Avg time/img: 0.0592 s\n",
            "loss: 2.4 (epoch: 3, step: 50) // Avg time/img: 0.0431 s\n",
            "loss: 2.436 (epoch: 3, step: 100) // Avg time/img: 0.0429 s\n",
            "loss: 2.426 (epoch: 3, step: 150) // Avg time/img: 0.0429 s\n",
            "loss: 2.433 (epoch: 3, step: 200) // Avg time/img: 0.0428 s\n",
            "loss: 2.436 (epoch: 3, step: 250) // Avg time/img: 0.0428 s\n",
            "loss: 2.432 (epoch: 3, step: 300) // Avg time/img: 0.0427 s\n",
            "loss: 2.437 (epoch: 3, step: 350) // Avg time/img: 0.0427 s\n",
            "loss: 2.427 (epoch: 3, step: 400) // Avg time/img: 0.0428 s\n",
            "loss: 2.427 (epoch: 3, step: 450) // Avg time/img: 0.0428 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 1.796 (epoch: 3, step: 0) // Avg time/img: 0.0268 s\n",
            "VAL loss: 2.236 (epoch: 3, step: 50) // Avg time/img: 0.0259 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m35.79\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-003.pth (epoch: 3)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.02330599066348749\n",
            "loss: 2.744 (epoch: 4, step: 0) // Avg time/img: 0.0581 s\n",
            "loss: 2.391 (epoch: 4, step: 50) // Avg time/img: 0.0432 s\n",
            "loss: 2.376 (epoch: 4, step: 100) // Avg time/img: 0.0430 s\n",
            "loss: 2.385 (epoch: 4, step: 150) // Avg time/img: 0.0427 s\n",
            "loss: 2.383 (epoch: 4, step: 200) // Avg time/img: 0.0427 s\n",
            "loss: 2.38 (epoch: 4, step: 250) // Avg time/img: 0.0426 s\n",
            "loss: 2.371 (epoch: 4, step: 300) // Avg time/img: 0.0426 s\n",
            "loss: 2.37 (epoch: 4, step: 350) // Avg time/img: 0.0425 s\n",
            "loss: 2.372 (epoch: 4, step: 400) // Avg time/img: 0.0426 s\n",
            "loss: 2.371 (epoch: 4, step: 450) // Avg time/img: 0.0427 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 2.027 (epoch: 4, step: 0) // Avg time/img: 0.0265 s\n",
            "VAL loss: 2.477 (epoch: 4, step: 50) // Avg time/img: 0.0250 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m34.96\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.022738314402074057\n",
            "loss: 2.998 (epoch: 5, step: 0) // Avg time/img: 0.0629 s\n",
            "loss: 2.264 (epoch: 5, step: 50) // Avg time/img: 0.0430 s\n",
            "loss: 2.276 (epoch: 5, step: 100) // Avg time/img: 0.0426 s\n",
            "loss: 2.305 (epoch: 5, step: 150) // Avg time/img: 0.0426 s\n",
            "loss: 2.304 (epoch: 5, step: 200) // Avg time/img: 0.0426 s\n",
            "loss: 2.289 (epoch: 5, step: 250) // Avg time/img: 0.0426 s\n",
            "loss: 2.296 (epoch: 5, step: 300) // Avg time/img: 0.0425 s\n",
            "loss: 2.294 (epoch: 5, step: 350) // Avg time/img: 0.0426 s\n",
            "loss: 2.289 (epoch: 5, step: 400) // Avg time/img: 0.0426 s\n",
            "loss: 2.292 (epoch: 5, step: 450) // Avg time/img: 0.0426 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 2.032 (epoch: 5, step: 0) // Avg time/img: 0.0253 s\n",
            "VAL loss: 2.403 (epoch: 5, step: 50) // Avg time/img: 0.0255 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m35.72\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.022169058856430182\n",
            "loss: 2.115 (epoch: 6, step: 0) // Avg time/img: 0.0565 s\n",
            "loss: 2.286 (epoch: 6, step: 50) // Avg time/img: 0.0431 s\n",
            "loss: 2.273 (epoch: 6, step: 100) // Avg time/img: 0.0429 s\n",
            "loss: 2.276 (epoch: 6, step: 150) // Avg time/img: 0.0426 s\n",
            "loss: 2.269 (epoch: 6, step: 200) // Avg time/img: 0.0427 s\n",
            "loss: 2.278 (epoch: 6, step: 250) // Avg time/img: 0.0427 s\n",
            "loss: 2.281 (epoch: 6, step: 300) // Avg time/img: 0.0427 s\n",
            "loss: 2.281 (epoch: 6, step: 350) // Avg time/img: 0.0427 s\n",
            "loss: 2.29 (epoch: 6, step: 400) // Avg time/img: 0.0427 s\n",
            "loss: 2.285 (epoch: 6, step: 450) // Avg time/img: 0.0426 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 1.573 (epoch: 6, step: 0) // Avg time/img: 0.0266 s\n",
            "VAL loss: 2.102 (epoch: 6, step: 50) // Avg time/img: 0.0252 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m40.52\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-006.pth (epoch: 6)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.02159817430757048\n",
            "loss: 3.117 (epoch: 7, step: 0) // Avg time/img: 0.0582 s\n",
            "loss: 2.34 (epoch: 7, step: 50) // Avg time/img: 0.0434 s\n",
            "loss: 2.291 (epoch: 7, step: 100) // Avg time/img: 0.0429 s\n",
            "loss: 2.264 (epoch: 7, step: 150) // Avg time/img: 0.0428 s\n",
            "loss: 2.269 (epoch: 7, step: 200) // Avg time/img: 0.0426 s\n",
            "loss: 2.28 (epoch: 7, step: 250) // Avg time/img: 0.0426 s\n",
            "loss: 2.275 (epoch: 7, step: 300) // Avg time/img: 0.0426 s\n",
            "loss: 2.268 (epoch: 7, step: 350) // Avg time/img: 0.0426 s\n",
            "loss: 2.255 (epoch: 7, step: 400) // Avg time/img: 0.0426 s\n",
            "loss: 2.253 (epoch: 7, step: 450) // Avg time/img: 0.0426 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 2.159 (epoch: 7, step: 0) // Avg time/img: 0.0233 s\n",
            "VAL loss: 2.509 (epoch: 7, step: 50) // Avg time/img: 0.0255 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m38.64\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.021025607959836438\n",
            "loss: 2.765 (epoch: 8, step: 0) // Avg time/img: 0.0585 s\n",
            "loss: 2.22 (epoch: 8, step: 50) // Avg time/img: 0.0433 s\n",
            "loss: 2.209 (epoch: 8, step: 100) // Avg time/img: 0.0432 s\n",
            "loss: 2.222 (epoch: 8, step: 150) // Avg time/img: 0.0431 s\n",
            "loss: 2.224 (epoch: 8, step: 200) // Avg time/img: 0.0430 s\n",
            "loss: 2.204 (epoch: 8, step: 250) // Avg time/img: 0.0429 s\n",
            "loss: 2.217 (epoch: 8, step: 300) // Avg time/img: 0.0428 s\n",
            "loss: 2.223 (epoch: 8, step: 350) // Avg time/img: 0.0427 s\n",
            "loss: 2.22 (epoch: 8, step: 400) // Avg time/img: 0.0426 s\n",
            "loss: 2.209 (epoch: 8, step: 450) // Avg time/img: 0.0426 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 2.059 (epoch: 8, step: 0) // Avg time/img: 0.0310 s\n",
            "VAL loss: 2.406 (epoch: 8, step: 50) // Avg time/img: 0.0260 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m42.01\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-008.pth (epoch: 8)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.02045130365127146\n",
            "loss: 2.192 (epoch: 9, step: 0) // Avg time/img: 0.0572 s\n",
            "loss: 2.163 (epoch: 9, step: 50) // Avg time/img: 0.0429 s\n",
            "loss: 2.178 (epoch: 9, step: 100) // Avg time/img: 0.0424 s\n",
            "loss: 2.187 (epoch: 9, step: 150) // Avg time/img: 0.0424 s\n",
            "loss: 2.196 (epoch: 9, step: 200) // Avg time/img: 0.0424 s\n",
            "loss: 2.192 (epoch: 9, step: 250) // Avg time/img: 0.0424 s\n",
            "loss: 2.19 (epoch: 9, step: 300) // Avg time/img: 0.0425 s\n",
            "loss: 2.199 (epoch: 9, step: 350) // Avg time/img: 0.0424 s\n",
            "loss: 2.199 (epoch: 9, step: 400) // Avg time/img: 0.0424 s\n",
            "loss: 2.201 (epoch: 9, step: 450) // Avg time/img: 0.0424 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 2.095 (epoch: 9, step: 0) // Avg time/img: 0.0292 s\n",
            "VAL loss: 2.838 (epoch: 9, step: 50) // Avg time/img: 0.0253 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m40.86\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.01987520152680185\n",
            "loss: 2.024 (epoch: 10, step: 0) // Avg time/img: 0.0552 s\n",
            "loss: 2.234 (epoch: 10, step: 50) // Avg time/img: 0.0427 s\n",
            "loss: 2.245 (epoch: 10, step: 100) // Avg time/img: 0.0426 s\n",
            "loss: 2.231 (epoch: 10, step: 150) // Avg time/img: 0.0425 s\n",
            "loss: 2.228 (epoch: 10, step: 200) // Avg time/img: 0.0425 s\n",
            "loss: 2.215 (epoch: 10, step: 250) // Avg time/img: 0.0424 s\n",
            "loss: 2.201 (epoch: 10, step: 300) // Avg time/img: 0.0424 s\n",
            "loss: 2.194 (epoch: 10, step: 350) // Avg time/img: 0.0424 s\n",
            "loss: 2.192 (epoch: 10, step: 400) // Avg time/img: 0.0423 s\n",
            "loss: 2.189 (epoch: 10, step: 450) // Avg time/img: 0.0423 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.945 (epoch: 10, step: 0) // Avg time/img: 0.0241 s\n",
            "VAL loss: 2.417 (epoch: 10, step: 50) // Avg time/img: 0.0249 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m44.16\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-010.pth (epoch: 10)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.019297237668089263\n",
            "loss: 2.563 (epoch: 11, step: 0) // Avg time/img: 0.0603 s\n",
            "loss: 2.132 (epoch: 11, step: 50) // Avg time/img: 0.0427 s\n",
            "loss: 2.151 (epoch: 11, step: 100) // Avg time/img: 0.0427 s\n",
            "loss: 2.146 (epoch: 11, step: 150) // Avg time/img: 0.0427 s\n",
            "loss: 2.165 (epoch: 11, step: 200) // Avg time/img: 0.0426 s\n",
            "loss: 2.17 (epoch: 11, step: 250) // Avg time/img: 0.0426 s\n",
            "loss: 2.172 (epoch: 11, step: 300) // Avg time/img: 0.0425 s\n",
            "loss: 2.185 (epoch: 11, step: 350) // Avg time/img: 0.0424 s\n",
            "loss: 2.175 (epoch: 11, step: 400) // Avg time/img: 0.0423 s\n",
            "loss: 2.182 (epoch: 11, step: 450) // Avg time/img: 0.0423 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 2.008 (epoch: 11, step: 0) // Avg time/img: 0.0368 s\n",
            "VAL loss: 2.267 (epoch: 11, step: 50) // Avg time/img: 0.0258 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m47.57\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-011.pth (epoch: 11)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.018717343672668973\n",
            "loss: 2.519 (epoch: 12, step: 0) // Avg time/img: 0.0549 s\n",
            "loss: 2.183 (epoch: 12, step: 50) // Avg time/img: 0.0425 s\n",
            "loss: 2.134 (epoch: 12, step: 100) // Avg time/img: 0.0422 s\n",
            "loss: 2.143 (epoch: 12, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 2.147 (epoch: 12, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 2.138 (epoch: 12, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 2.143 (epoch: 12, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 2.145 (epoch: 12, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 2.153 (epoch: 12, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 2.159 (epoch: 12, step: 450) // Avg time/img: 0.0420 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 2.062 (epoch: 12, step: 0) // Avg time/img: 0.0245 s\n",
            "VAL loss: 2.446 (epoch: 12, step: 50) // Avg time/img: 0.0251 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m43.89\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0181354461734305\n",
            "loss: 1.988 (epoch: 13, step: 0) // Avg time/img: 0.0582 s\n",
            "loss: 2.131 (epoch: 13, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 2.111 (epoch: 13, step: 100) // Avg time/img: 0.0421 s\n",
            "loss: 2.151 (epoch: 13, step: 150) // Avg time/img: 0.0420 s\n",
            "loss: 2.141 (epoch: 13, step: 200) // Avg time/img: 0.0421 s\n",
            "loss: 2.145 (epoch: 13, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 2.142 (epoch: 13, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 2.144 (epoch: 13, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 2.14 (epoch: 13, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 2.147 (epoch: 13, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 1.911 (epoch: 13, step: 0) // Avg time/img: 0.0249 s\n",
            "VAL loss: 2.334 (epoch: 13, step: 50) // Avg time/img: 0.0249 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m47.70\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-013.pth (epoch: 13)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0175514662875424\n",
            "loss: 1.827 (epoch: 14, step: 0) // Avg time/img: 0.0530 s\n",
            "loss: 2.189 (epoch: 14, step: 50) // Avg time/img: 0.0429 s\n",
            "loss: 2.128 (epoch: 14, step: 100) // Avg time/img: 0.0427 s\n",
            "loss: 2.128 (epoch: 14, step: 150) // Avg time/img: 0.0424 s\n",
            "loss: 2.122 (epoch: 14, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 2.121 (epoch: 14, step: 250) // Avg time/img: 0.0422 s\n",
            "loss: 2.116 (epoch: 14, step: 300) // Avg time/img: 0.0423 s\n",
            "loss: 2.118 (epoch: 14, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 2.119 (epoch: 14, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 2.118 (epoch: 14, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 1.918 (epoch: 14, step: 0) // Avg time/img: 0.0401 s\n",
            "VAL loss: 2.355 (epoch: 14, step: 50) // Avg time/img: 0.0248 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m49.12\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-014.pth (epoch: 14)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.016965318981453127\n",
            "loss: 1.959 (epoch: 15, step: 0) // Avg time/img: 0.0597 s\n",
            "loss: 2.079 (epoch: 15, step: 50) // Avg time/img: 0.0431 s\n",
            "loss: 2.104 (epoch: 15, step: 100) // Avg time/img: 0.0426 s\n",
            "loss: 2.103 (epoch: 15, step: 150) // Avg time/img: 0.0425 s\n",
            "loss: 2.093 (epoch: 15, step: 200) // Avg time/img: 0.0425 s\n",
            "loss: 2.097 (epoch: 15, step: 250) // Avg time/img: 0.0424 s\n",
            "loss: 2.103 (epoch: 15, step: 300) // Avg time/img: 0.0423 s\n",
            "loss: 2.102 (epoch: 15, step: 350) // Avg time/img: 0.0423 s\n",
            "loss: 2.106 (epoch: 15, step: 400) // Avg time/img: 0.0423 s\n",
            "loss: 2.105 (epoch: 15, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 1.522 (epoch: 15, step: 0) // Avg time/img: 0.0255 s\n",
            "VAL loss: 1.95 (epoch: 15, step: 50) // Avg time/img: 0.0249 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m51.03\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-015.pth (epoch: 15)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.016376912335452465\n",
            "loss: 1.911 (epoch: 16, step: 0) // Avg time/img: 0.0570 s\n",
            "loss: 2.068 (epoch: 16, step: 50) // Avg time/img: 0.0431 s\n",
            "loss: 2.054 (epoch: 16, step: 100) // Avg time/img: 0.0425 s\n",
            "loss: 2.078 (epoch: 16, step: 150) // Avg time/img: 0.0425 s\n",
            "loss: 2.08 (epoch: 16, step: 200) // Avg time/img: 0.0424 s\n",
            "loss: 2.073 (epoch: 16, step: 250) // Avg time/img: 0.0424 s\n",
            "loss: 2.072 (epoch: 16, step: 300) // Avg time/img: 0.0423 s\n",
            "loss: 2.073 (epoch: 16, step: 350) // Avg time/img: 0.0423 s\n",
            "loss: 2.075 (epoch: 16, step: 400) // Avg time/img: 0.0423 s\n",
            "loss: 2.08 (epoch: 16, step: 450) // Avg time/img: 0.0423 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 2.15 (epoch: 16, step: 0) // Avg time/img: 0.0290 s\n",
            "VAL loss: 2.446 (epoch: 16, step: 50) // Avg time/img: 0.0249 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m47.02\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.015786146687233882\n",
            "loss: 2.237 (epoch: 17, step: 0) // Avg time/img: 0.0599 s\n",
            "loss: 2.059 (epoch: 17, step: 50) // Avg time/img: 0.0429 s\n",
            "loss: 2.066 (epoch: 17, step: 100) // Avg time/img: 0.0425 s\n",
            "loss: 2.077 (epoch: 17, step: 150) // Avg time/img: 0.0424 s\n",
            "loss: 2.063 (epoch: 17, step: 200) // Avg time/img: 0.0423 s\n",
            "loss: 2.073 (epoch: 17, step: 250) // Avg time/img: 0.0423 s\n",
            "loss: 2.08 (epoch: 17, step: 300) // Avg time/img: 0.0422 s\n",
            "loss: 2.077 (epoch: 17, step: 350) // Avg time/img: 0.0422 s\n",
            "loss: 2.076 (epoch: 17, step: 400) // Avg time/img: 0.0422 s\n",
            "loss: 2.074 (epoch: 17, step: 450) // Avg time/img: 0.0422 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 2.014 (epoch: 17, step: 0) // Avg time/img: 0.0348 s\n",
            "VAL loss: 2.457 (epoch: 17, step: 50) // Avg time/img: 0.0252 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m52.22\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-017.pth (epoch: 17)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.01519291362865223\n",
            "loss: 2.303 (epoch: 18, step: 0) // Avg time/img: 0.0585 s\n",
            "loss: 2.038 (epoch: 18, step: 50) // Avg time/img: 0.0426 s\n",
            "loss: 2.054 (epoch: 18, step: 100) // Avg time/img: 0.0424 s\n",
            "loss: 2.061 (epoch: 18, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 2.064 (epoch: 18, step: 200) // Avg time/img: 0.0422 s\n",
            "loss: 2.055 (epoch: 18, step: 250) // Avg time/img: 0.0421 s\n",
            "loss: 2.061 (epoch: 18, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 2.064 (epoch: 18, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 2.067 (epoch: 18, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 2.064 (epoch: 18, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 1.801 (epoch: 18, step: 0) // Avg time/img: 0.0227 s\n",
            "VAL loss: 2.144 (epoch: 18, step: 50) // Avg time/img: 0.0253 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m49.73\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.014597094822999507\n",
            "loss: 2.115 (epoch: 19, step: 0) // Avg time/img: 0.0570 s\n",
            "loss: 2.091 (epoch: 19, step: 50) // Avg time/img: 0.0421 s\n",
            "loss: 2.079 (epoch: 19, step: 100) // Avg time/img: 0.0421 s\n",
            "loss: 2.073 (epoch: 19, step: 150) // Avg time/img: 0.0423 s\n",
            "loss: 2.071 (epoch: 19, step: 200) // Avg time/img: 0.0423 s\n",
            "loss: 2.06 (epoch: 19, step: 250) // Avg time/img: 0.0422 s\n",
            "loss: 2.066 (epoch: 19, step: 300) // Avg time/img: 0.0421 s\n",
            "loss: 2.066 (epoch: 19, step: 350) // Avg time/img: 0.0421 s\n",
            "loss: 2.065 (epoch: 19, step: 400) // Avg time/img: 0.0421 s\n",
            "loss: 2.065 (epoch: 19, step: 450) // Avg time/img: 0.0421 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 1.647 (epoch: 19, step: 0) // Avg time/img: 0.0273 s\n",
            "VAL loss: 2.097 (epoch: 19, step: 50) // Avg time/img: 0.0257 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m52.89\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-019.pth (epoch: 19)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.013998560601021132\n",
            "loss: 1.679 (epoch: 20, step: 0) // Avg time/img: 0.0557 s\n",
            "loss: 2.075 (epoch: 20, step: 50) // Avg time/img: 0.0425 s\n",
            "loss: 2.063 (epoch: 20, step: 100) // Avg time/img: 0.0422 s\n",
            "loss: 2.051 (epoch: 20, step: 150) // Avg time/img: 0.0420 s\n",
            "loss: 2.042 (epoch: 20, step: 200) // Avg time/img: 0.0420 s\n",
            "loss: 2.036 (epoch: 20, step: 250) // Avg time/img: 0.0419 s\n",
            "loss: 2.047 (epoch: 20, step: 300) // Avg time/img: 0.0419 s\n",
            "loss: 2.05 (epoch: 20, step: 350) // Avg time/img: 0.0419 s\n",
            "loss: 2.048 (epoch: 20, step: 400) // Avg time/img: 0.0419 s\n",
            "loss: 2.046 (epoch: 20, step: 450) // Avg time/img: 0.0419 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 1.983 (epoch: 20, step: 0) // Avg time/img: 0.0256 s\n",
            "VAL loss: 2.442 (epoch: 20, step: 50) // Avg time/img: 0.0250 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m52.45\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"bisenet\", num_epochs=40, batch_size=6, stop_epoch=40, resume=True)\n",
        "# %cd ../save\n",
        "# !zip -r bisenet_training_void.zip bisenet_training_void/"
      ],
      "metadata": {
        "id": "jqwx8Yat-_eP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64409bba-24ac-475d-e0ef-8be5e63051da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 384MB/s]\n",
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "=> Loaded checkpoint at epoch 21)\n",
            "----- TRAINING - EPOCH 21 -----\n",
            "LEARNING RATE:  0.013397168281703665\n",
            "loss: 2.068 (epoch: 21, step: 0) // Avg time/img: 0.4451 s\n",
            "loss: 2.012 (epoch: 21, step: 50) // Avg time/img: 0.0482 s\n",
            "loss: 2.009 (epoch: 21, step: 100) // Avg time/img: 0.0447 s\n",
            "loss: 2.011 (epoch: 21, step: 150) // Avg time/img: 0.0438 s\n",
            "loss: 2.021 (epoch: 21, step: 200) // Avg time/img: 0.0436 s\n",
            "loss: 2.012 (epoch: 21, step: 250) // Avg time/img: 0.0436 s\n",
            "loss: 2.017 (epoch: 21, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 2.026 (epoch: 21, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 2.02 (epoch: 21, step: 400) // Avg time/img: 0.0436 s\n",
            "loss: 2.019 (epoch: 21, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 21 -----\n",
            "VAL loss: 1.843 (epoch: 21, step: 0) // Avg time/img: 0.0292 s\n",
            "VAL loss: 2.257 (epoch: 21, step: 50) // Avg time/img: 0.0261 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m52.88\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-021.pth (epoch: 21)\n",
            "----- TRAINING - EPOCH 22 -----\n",
            "LEARNING RATE:  0.012792760147322056\n",
            "loss: 1.753 (epoch: 22, step: 0) // Avg time/img: 0.0549 s\n",
            "loss: 2.042 (epoch: 22, step: 50) // Avg time/img: 0.0438 s\n",
            "loss: 2.031 (epoch: 22, step: 100) // Avg time/img: 0.0438 s\n",
            "loss: 2.044 (epoch: 22, step: 150) // Avg time/img: 0.0436 s\n",
            "loss: 2.029 (epoch: 22, step: 200) // Avg time/img: 0.0436 s\n",
            "loss: 2.029 (epoch: 22, step: 250) // Avg time/img: 0.0437 s\n",
            "loss: 2.044 (epoch: 22, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 2.035 (epoch: 22, step: 350) // Avg time/img: 0.0436 s\n",
            "loss: 2.031 (epoch: 22, step: 400) // Avg time/img: 0.0436 s\n",
            "loss: 2.028 (epoch: 22, step: 450) // Avg time/img: 0.0436 s\n",
            "----- VALIDATING - EPOCH 22 -----\n",
            "VAL loss: 1.804 (epoch: 22, step: 0) // Avg time/img: 0.0288 s\n",
            "VAL loss: 2.086 (epoch: 22, step: 50) // Avg time/img: 0.0258 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m43.35\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-022.pth (epoch: 22)\n",
            "----- TRAINING - EPOCH 23 -----\n",
            "LEARNING RATE:  0.012185160979474885\n",
            "loss: 1.874 (epoch: 23, step: 0) // Avg time/img: 0.0587 s\n",
            "loss: 1.98 (epoch: 23, step: 50) // Avg time/img: 0.0436 s\n",
            "loss: 1.972 (epoch: 23, step: 100) // Avg time/img: 0.0434 s\n",
            "loss: 1.989 (epoch: 23, step: 150) // Avg time/img: 0.0433 s\n",
            "loss: 1.986 (epoch: 23, step: 200) // Avg time/img: 0.0433 s\n",
            "loss: 1.998 (epoch: 23, step: 250) // Avg time/img: 0.0433 s\n",
            "loss: 1.994 (epoch: 23, step: 300) // Avg time/img: 0.0434 s\n",
            "loss: 2.004 (epoch: 23, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 2.01 (epoch: 23, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 2.014 (epoch: 23, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 23 -----\n",
            "VAL loss: 1.771 (epoch: 23, step: 0) // Avg time/img: 0.0280 s\n",
            "VAL loss: 2.194 (epoch: 23, step: 50) // Avg time/img: 0.0252 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m54.75\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-023.pth (epoch: 23)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 23)\n",
            "----- TRAINING - EPOCH 24 -----\n",
            "LEARNING RATE:  0.011574175031043611\n",
            "loss: 1.952 (epoch: 24, step: 0) // Avg time/img: 0.0576 s\n",
            "loss: 1.988 (epoch: 24, step: 50) // Avg time/img: 0.0441 s\n",
            "loss: 1.988 (epoch: 24, step: 100) // Avg time/img: 0.0434 s\n",
            "loss: 2.007 (epoch: 24, step: 150) // Avg time/img: 0.0435 s\n",
            "loss: 2.0 (epoch: 24, step: 200) // Avg time/img: 0.0435 s\n",
            "loss: 2.007 (epoch: 24, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 2.008 (epoch: 24, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 2.004 (epoch: 24, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 2.009 (epoch: 24, step: 400) // Avg time/img: 0.0435 s\n",
            "loss: 2.01 (epoch: 24, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 24 -----\n",
            "VAL loss: 1.631 (epoch: 24, step: 0) // Avg time/img: 0.0260 s\n",
            "VAL loss: 2.182 (epoch: 24, step: 50) // Avg time/img: 0.0251 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m53.21\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-024.pth (epoch: 24)\n",
            "----- TRAINING - EPOCH 25 -----\n",
            "LEARNING RATE:  0.010959582263852173\n",
            "loss: 2.025 (epoch: 25, step: 0) // Avg time/img: 0.0565 s\n",
            "loss: 2.026 (epoch: 25, step: 50) // Avg time/img: 0.0437 s\n",
            "loss: 1.996 (epoch: 25, step: 100) // Avg time/img: 0.0434 s\n",
            "loss: 1.998 (epoch: 25, step: 150) // Avg time/img: 0.0434 s\n",
            "loss: 2.0 (epoch: 25, step: 200) // Avg time/img: 0.0435 s\n",
            "loss: 1.995 (epoch: 25, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.991 (epoch: 25, step: 300) // Avg time/img: 0.0435 s\n",
            "loss: 1.997 (epoch: 25, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 2.001 (epoch: 25, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 2.013 (epoch: 25, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 25 -----\n",
            "VAL loss: 1.708 (epoch: 25, step: 0) // Avg time/img: 0.0263 s\n",
            "VAL loss: 2.151 (epoch: 25, step: 50) // Avg time/img: 0.0253 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m55.05\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-025.pth (epoch: 25)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 25)\n",
            "----- TRAINING - EPOCH 26 -----\n",
            "LEARNING RATE:  0.010341133616456905\n",
            "loss: 2.053 (epoch: 26, step: 0) // Avg time/img: 0.0506 s\n",
            "loss: 1.985 (epoch: 26, step: 50) // Avg time/img: 0.0442 s\n",
            "loss: 1.969 (epoch: 26, step: 100) // Avg time/img: 0.0437 s\n",
            "loss: 1.965 (epoch: 26, step: 150) // Avg time/img: 0.0437 s\n",
            "loss: 1.989 (epoch: 26, step: 200) // Avg time/img: 0.0436 s\n",
            "loss: 1.995 (epoch: 26, step: 250) // Avg time/img: 0.0436 s\n",
            "loss: 1.999 (epoch: 26, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 1.998 (epoch: 26, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 1.994 (epoch: 26, step: 400) // Avg time/img: 0.0435 s\n",
            "loss: 1.998 (epoch: 26, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 26 -----\n",
            "VAL loss: 1.741 (epoch: 26, step: 0) // Avg time/img: 0.0342 s\n",
            "VAL loss: 2.084 (epoch: 26, step: 50) // Avg time/img: 0.0259 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m54.39\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-026.pth (epoch: 26)\n",
            "----- TRAINING - EPOCH 27 -----\n",
            "LEARNING RATE:  0.009718544969969087\n",
            "loss: 1.836 (epoch: 27, step: 0) // Avg time/img: 0.0611 s\n",
            "loss: 2.016 (epoch: 27, step: 50) // Avg time/img: 0.0441 s\n",
            "loss: 2.031 (epoch: 27, step: 100) // Avg time/img: 0.0438 s\n",
            "loss: 2.02 (epoch: 27, step: 150) // Avg time/img: 0.0436 s\n",
            "loss: 2.009 (epoch: 27, step: 200) // Avg time/img: 0.0435 s\n",
            "loss: 2.015 (epoch: 27, step: 250) // Avg time/img: 0.0434 s\n",
            "loss: 2.009 (epoch: 27, step: 300) // Avg time/img: 0.0434 s\n",
            "loss: 2.01 (epoch: 27, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 1.998 (epoch: 27, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 1.998 (epoch: 27, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 27 -----\n",
            "VAL loss: 1.963 (epoch: 27, step: 0) // Avg time/img: 0.0333 s\n",
            "VAL loss: 2.264 (epoch: 27, step: 50) // Avg time/img: 0.0251 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m54.81\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-027.pth (epoch: 27)\n",
            "----- TRAINING - EPOCH 28 -----\n",
            "LEARNING RATE:  0.009091489333892356\n",
            "loss: 1.94 (epoch: 28, step: 0) // Avg time/img: 0.0539 s\n",
            "loss: 1.986 (epoch: 28, step: 50) // Avg time/img: 0.0444 s\n",
            "loss: 1.982 (epoch: 28, step: 100) // Avg time/img: 0.0438 s\n",
            "loss: 1.969 (epoch: 28, step: 150) // Avg time/img: 0.0436 s\n",
            "loss: 1.962 (epoch: 28, step: 200) // Avg time/img: 0.0436 s\n",
            "loss: 1.958 (epoch: 28, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.966 (epoch: 28, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 1.964 (epoch: 28, step: 350) // Avg time/img: 0.0436 s\n",
            "loss: 1.963 (epoch: 28, step: 400) // Avg time/img: 0.0436 s\n",
            "loss: 1.959 (epoch: 28, step: 450) // Avg time/img: 0.0436 s\n",
            "----- VALIDATING - EPOCH 28 -----\n",
            "VAL loss: 1.825 (epoch: 28, step: 0) // Avg time/img: 0.0355 s\n",
            "VAL loss: 2.292 (epoch: 28, step: 50) // Avg time/img: 0.0258 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m57.36\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-028.pth (epoch: 28)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 28)\n",
            "----- TRAINING - EPOCH 29 -----\n",
            "LEARNING RATE:  0.008459586547541247\n",
            "loss: 1.939 (epoch: 29, step: 0) // Avg time/img: 0.0554 s\n",
            "loss: 1.972 (epoch: 29, step: 50) // Avg time/img: 0.0440 s\n",
            "loss: 1.956 (epoch: 29, step: 100) // Avg time/img: 0.0436 s\n",
            "loss: 1.966 (epoch: 29, step: 150) // Avg time/img: 0.0435 s\n",
            "loss: 1.962 (epoch: 29, step: 200) // Avg time/img: 0.0434 s\n",
            "loss: 1.971 (epoch: 29, step: 250) // Avg time/img: 0.0434 s\n",
            "loss: 1.963 (epoch: 29, step: 300) // Avg time/img: 0.0433 s\n",
            "loss: 1.966 (epoch: 29, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 1.964 (epoch: 29, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 1.961 (epoch: 29, step: 450) // Avg time/img: 0.0433 s\n",
            "----- VALIDATING - EPOCH 29 -----\n",
            "VAL loss: 1.774 (epoch: 29, step: 0) // Avg time/img: 0.0234 s\n",
            "VAL loss: 2.144 (epoch: 29, step: 50) // Avg time/img: 0.0254 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m57.57\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-029.pth (epoch: 29)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 29)\n",
            "----- TRAINING - EPOCH 30 -----\n",
            "LEARNING RATE:  0.007822389430708392\n",
            "loss: 2.179 (epoch: 30, step: 0) // Avg time/img: 0.0573 s\n",
            "loss: 1.98 (epoch: 30, step: 50) // Avg time/img: 0.0444 s\n",
            "loss: 1.957 (epoch: 30, step: 100) // Avg time/img: 0.0438 s\n",
            "loss: 1.955 (epoch: 30, step: 150) // Avg time/img: 0.0436 s\n",
            "loss: 1.949 (epoch: 30, step: 200) // Avg time/img: 0.0437 s\n",
            "loss: 1.956 (epoch: 30, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.954 (epoch: 30, step: 300) // Avg time/img: 0.0434 s\n",
            "loss: 1.954 (epoch: 30, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 1.95 (epoch: 30, step: 400) // Avg time/img: 0.0435 s\n",
            "loss: 1.951 (epoch: 30, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 30 -----\n",
            "VAL loss: 1.781 (epoch: 30, step: 0) // Avg time/img: 0.0300 s\n",
            "VAL loss: 2.186 (epoch: 30, step: 50) // Avg time/img: 0.0259 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m56.64\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-030.pth (epoch: 30)\n",
            "----- TRAINING - EPOCH 31 -----\n",
            "LEARNING RATE:  0.007179364718731468\n",
            "loss: 2.038 (epoch: 31, step: 0) // Avg time/img: 0.0610 s\n",
            "loss: 1.972 (epoch: 31, step: 50) // Avg time/img: 0.0443 s\n",
            "loss: 1.948 (epoch: 31, step: 100) // Avg time/img: 0.0440 s\n",
            "loss: 1.953 (epoch: 31, step: 150) // Avg time/img: 0.0438 s\n",
            "loss: 1.935 (epoch: 31, step: 200) // Avg time/img: 0.0438 s\n",
            "loss: 1.938 (epoch: 31, step: 250) // Avg time/img: 0.0437 s\n",
            "loss: 1.944 (epoch: 31, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 1.944 (epoch: 31, step: 350) // Avg time/img: 0.0436 s\n",
            "loss: 1.947 (epoch: 31, step: 400) // Avg time/img: 0.0436 s\n",
            "loss: 1.949 (epoch: 31, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 31 -----\n",
            "VAL loss: 1.77 (epoch: 31, step: 0) // Avg time/img: 0.0250 s\n",
            "VAL loss: 2.204 (epoch: 31, step: 50) // Avg time/img: 0.0254 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m58.92\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-031.pth (epoch: 31)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 31)\n",
            "----- TRAINING - EPOCH 32 -----\n",
            "LEARNING RATE:  0.006529866087266964\n",
            "loss: 1.639 (epoch: 32, step: 0) // Avg time/img: 0.0608 s\n",
            "loss: 1.896 (epoch: 32, step: 50) // Avg time/img: 0.0440 s\n",
            "loss: 1.907 (epoch: 32, step: 100) // Avg time/img: 0.0439 s\n",
            "loss: 1.918 (epoch: 32, step: 150) // Avg time/img: 0.0438 s\n",
            "loss: 1.937 (epoch: 32, step: 200) // Avg time/img: 0.0437 s\n",
            "loss: 1.935 (epoch: 32, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.942 (epoch: 32, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 1.937 (epoch: 32, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 1.932 (epoch: 32, step: 400) // Avg time/img: 0.0435 s\n",
            "loss: 1.931 (epoch: 32, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 32 -----\n",
            "VAL loss: 1.941 (epoch: 32, step: 0) // Avg time/img: 0.0261 s\n",
            "VAL loss: 2.397 (epoch: 32, step: 50) // Avg time/img: 0.0256 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m58.49\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-032.pth (epoch: 32)\n",
            "----- TRAINING - EPOCH 33 -----\n",
            "LEARNING RATE:  0.005873094715440094\n",
            "loss: 1.972 (epoch: 33, step: 0) // Avg time/img: 0.0622 s\n",
            "loss: 1.954 (epoch: 33, step: 50) // Avg time/img: 0.0443 s\n",
            "loss: 1.913 (epoch: 33, step: 100) // Avg time/img: 0.0438 s\n",
            "loss: 1.938 (epoch: 33, step: 150) // Avg time/img: 0.0436 s\n",
            "loss: 1.927 (epoch: 33, step: 200) // Avg time/img: 0.0435 s\n",
            "loss: 1.92 (epoch: 33, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.918 (epoch: 33, step: 300) // Avg time/img: 0.0435 s\n",
            "loss: 1.927 (epoch: 33, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 1.925 (epoch: 33, step: 400) // Avg time/img: 0.0435 s\n",
            "loss: 1.923 (epoch: 33, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 33 -----\n",
            "VAL loss: 1.717 (epoch: 33, step: 0) // Avg time/img: 0.0268 s\n",
            "VAL loss: 2.127 (epoch: 33, step: 50) // Avg time/img: 0.0251 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m58.81\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-033.pth (epoch: 33)\n",
            "----- TRAINING - EPOCH 34 -----\n",
            "LEARNING RATE:  0.005208039296639223\n",
            "loss: 2.014 (epoch: 34, step: 0) // Avg time/img: 0.0571 s\n",
            "loss: 1.933 (epoch: 34, step: 50) // Avg time/img: 0.0442 s\n",
            "loss: 1.935 (epoch: 34, step: 100) // Avg time/img: 0.0439 s\n",
            "loss: 1.925 (epoch: 34, step: 150) // Avg time/img: 0.0437 s\n",
            "loss: 1.926 (epoch: 34, step: 200) // Avg time/img: 0.0437 s\n",
            "loss: 1.936 (epoch: 34, step: 250) // Avg time/img: 0.0437 s\n",
            "loss: 1.933 (epoch: 34, step: 300) // Avg time/img: 0.0436 s\n",
            "loss: 1.937 (epoch: 34, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 1.929 (epoch: 34, step: 400) // Avg time/img: 0.0435 s\n",
            "loss: 1.934 (epoch: 34, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 34 -----\n",
            "VAL loss: 1.771 (epoch: 34, step: 0) // Avg time/img: 0.0263 s\n",
            "VAL loss: 2.188 (epoch: 34, step: 50) // Avg time/img: 0.0260 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m59.92\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-034.pth (epoch: 34)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 34)\n",
            "----- TRAINING - EPOCH 35 -----\n",
            "LEARNING RATE:  0.004533380182841864\n",
            "loss: 2.172 (epoch: 35, step: 0) // Avg time/img: 0.0547 s\n",
            "loss: 1.904 (epoch: 35, step: 50) // Avg time/img: 0.0441 s\n",
            "loss: 1.9 (epoch: 35, step: 100) // Avg time/img: 0.0438 s\n",
            "loss: 1.904 (epoch: 35, step: 150) // Avg time/img: 0.0438 s\n",
            "loss: 1.906 (epoch: 35, step: 200) // Avg time/img: 0.0437 s\n",
            "loss: 1.891 (epoch: 35, step: 250) // Avg time/img: 0.0436 s\n",
            "loss: 1.898 (epoch: 35, step: 300) // Avg time/img: 0.0435 s\n",
            "loss: 1.901 (epoch: 35, step: 350) // Avg time/img: 0.0435 s\n",
            "loss: 1.898 (epoch: 35, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 1.903 (epoch: 35, step: 450) // Avg time/img: 0.0435 s\n",
            "----- VALIDATING - EPOCH 35 -----\n",
            "VAL loss: 1.547 (epoch: 35, step: 0) // Avg time/img: 0.0261 s\n",
            "VAL loss: 2.003 (epoch: 35, step: 50) // Avg time/img: 0.0253 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m61.00\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-035.pth (epoch: 35)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 35)\n",
            "----- TRAINING - EPOCH 36 -----\n",
            "LEARNING RATE:  0.0038473262917028636\n",
            "loss: 1.951 (epoch: 36, step: 0) // Avg time/img: 0.0640 s\n",
            "loss: 1.952 (epoch: 36, step: 50) // Avg time/img: 0.0437 s\n",
            "loss: 1.916 (epoch: 36, step: 100) // Avg time/img: 0.0435 s\n",
            "loss: 1.919 (epoch: 36, step: 150) // Avg time/img: 0.0435 s\n",
            "loss: 1.927 (epoch: 36, step: 200) // Avg time/img: 0.0434 s\n",
            "loss: 1.925 (epoch: 36, step: 250) // Avg time/img: 0.0433 s\n",
            "loss: 1.916 (epoch: 36, step: 300) // Avg time/img: 0.0433 s\n",
            "loss: 1.918 (epoch: 36, step: 350) // Avg time/img: 0.0433 s\n",
            "loss: 1.92 (epoch: 36, step: 400) // Avg time/img: 0.0433 s\n",
            "loss: 1.912 (epoch: 36, step: 450) // Avg time/img: 0.0433 s\n",
            "----- VALIDATING - EPOCH 36 -----\n",
            "VAL loss: 1.881 (epoch: 36, step: 0) // Avg time/img: 0.0284 s\n",
            "VAL loss: 2.303 (epoch: 36, step: 50) // Avg time/img: 0.0257 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m61.23\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-036.pth (epoch: 36)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 36)\n",
            "----- TRAINING - EPOCH 37 -----\n",
            "LEARNING RATE:  0.003147313529485418\n",
            "loss: 1.822 (epoch: 37, step: 0) // Avg time/img: 0.0534 s\n",
            "loss: 1.926 (epoch: 37, step: 50) // Avg time/img: 0.0437 s\n",
            "loss: 1.904 (epoch: 37, step: 100) // Avg time/img: 0.0436 s\n",
            "loss: 1.893 (epoch: 37, step: 150) // Avg time/img: 0.0436 s\n",
            "loss: 1.895 (epoch: 37, step: 200) // Avg time/img: 0.0436 s\n",
            "loss: 1.894 (epoch: 37, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.892 (epoch: 37, step: 300) // Avg time/img: 0.0434 s\n",
            "loss: 1.889 (epoch: 37, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 1.887 (epoch: 37, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 1.892 (epoch: 37, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 37 -----\n",
            "VAL loss: 1.704 (epoch: 37, step: 0) // Avg time/img: 0.0272 s\n",
            "VAL loss: 2.154 (epoch: 37, step: 50) // Avg time/img: 0.0250 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m61.06\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-037.pth (epoch: 37)\n",
            "----- TRAINING - EPOCH 38 -----\n",
            "LEARNING RATE:  0.0024293782877789177\n",
            "loss: 1.723 (epoch: 38, step: 0) // Avg time/img: 0.0536 s\n",
            "loss: 1.878 (epoch: 38, step: 50) // Avg time/img: 0.0436 s\n",
            "loss: 1.892 (epoch: 38, step: 100) // Avg time/img: 0.0436 s\n",
            "loss: 1.899 (epoch: 38, step: 150) // Avg time/img: 0.0435 s\n",
            "loss: 1.903 (epoch: 38, step: 200) // Avg time/img: 0.0435 s\n",
            "loss: 1.905 (epoch: 38, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.901 (epoch: 38, step: 300) // Avg time/img: 0.0434 s\n",
            "loss: 1.9 (epoch: 38, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 1.899 (epoch: 38, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 1.894 (epoch: 38, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 38 -----\n",
            "VAL loss: 1.696 (epoch: 38, step: 0) // Avg time/img: 0.0262 s\n",
            "VAL loss: 2.135 (epoch: 38, step: 50) // Avg time/img: 0.0257 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m60.85\u001b[0m %\n",
            "save: ../save/bisenet_training_void_2/model-038.pth (epoch: 38)\n",
            "----- TRAINING - EPOCH 39 -----\n",
            "LEARNING RATE:  0.0016866035595919555\n",
            "loss: 1.657 (epoch: 39, step: 0) // Avg time/img: 0.0520 s\n",
            "loss: 1.877 (epoch: 39, step: 50) // Avg time/img: 0.0436 s\n",
            "loss: 1.876 (epoch: 39, step: 100) // Avg time/img: 0.0436 s\n",
            "loss: 1.889 (epoch: 39, step: 150) // Avg time/img: 0.0437 s\n",
            "loss: 1.886 (epoch: 39, step: 200) // Avg time/img: 0.0436 s\n",
            "loss: 1.885 (epoch: 39, step: 250) // Avg time/img: 0.0435 s\n",
            "loss: 1.884 (epoch: 39, step: 300) // Avg time/img: 0.0435 s\n",
            "loss: 1.885 (epoch: 39, step: 350) // Avg time/img: 0.0434 s\n",
            "loss: 1.887 (epoch: 39, step: 400) // Avg time/img: 0.0434 s\n",
            "loss: 1.889 (epoch: 39, step: 450) // Avg time/img: 0.0434 s\n",
            "----- VALIDATING - EPOCH 39 -----\n",
            "VAL loss: 1.827 (epoch: 39, step: 0) // Avg time/img: 0.0353 s\n",
            "VAL loss: 2.205 (epoch: 39, step: 50) // Avg time/img: 0.0258 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m61.62\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-039.pth (epoch: 39)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 39)\n",
            "----- TRAINING - EPOCH 40 -----\n",
            "LEARNING RATE:  0.0009038284684949537\n",
            "loss: 2.231 (epoch: 40, step: 0) // Avg time/img: 0.0588 s\n",
            "loss: 1.891 (epoch: 40, step: 50) // Avg time/img: 0.0440 s\n",
            "loss: 1.894 (epoch: 40, step: 100) // Avg time/img: 0.0435 s\n",
            "loss: 1.886 (epoch: 40, step: 150) // Avg time/img: 0.0434 s\n",
            "loss: 1.881 (epoch: 40, step: 200) // Avg time/img: 0.0432 s\n",
            "loss: 1.879 (epoch: 40, step: 250) // Avg time/img: 0.0432 s\n",
            "loss: 1.881 (epoch: 40, step: 300) // Avg time/img: 0.0432 s\n",
            "loss: 1.878 (epoch: 40, step: 350) // Avg time/img: 0.0433 s\n",
            "loss: 1.88 (epoch: 40, step: 400) // Avg time/img: 0.0433 s\n",
            "loss: 1.883 (epoch: 40, step: 450) // Avg time/img: 0.0433 s\n",
            "----- VALIDATING - EPOCH 40 -----\n",
            "VAL loss: 1.81 (epoch: 40, step: 0) // Avg time/img: 0.0242 s\n",
            "VAL loss: 2.19 (epoch: 40, step: 50) // Avg time/img: 0.0252 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m62.20\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_2/model-040.pth (epoch: 40)\n",
            "save: ../save/bisenet_training_void_2/model_best.pth (epoch: 40)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BiSeNet Fine-Tuning"
      ],
      "metadata": {
        "id": "dNIDZtsXNX4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tune a pretrained BiSeNet model on the Cityscapes dataset for 20 epochs by setting `fineTune=True`."
      ],
      "metadata": {
        "id": "pxTEvdb0fFWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"bisenet\", num_epochs=20, batch_size=6, fineTune=True)\n",
        "# %cd ../save\n",
        "# !zip -r bisenet_training_void_ft.zip bisenet_training_void_ft/"
      ],
      "metadata": {
        "id": "mm8otlVa_ewP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31604b85-6e13-40aa-9fc5-907ea3bcf5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model bisenet with weights ../trained_models/bisenet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "Criterion: <class 'utils.losses.ohem_ce_loss.OhemCELoss'>\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0025\n",
            "loss: 6.09 (epoch: 1, step: 0) // Avg time/img: 0.4535 s\n",
            "loss: 5.907 (epoch: 1, step: 50) // Avg time/img: 0.0267 s\n",
            "loss: 5.636 (epoch: 1, step: 100) // Avg time/img: 0.0225 s\n",
            "loss: 5.524 (epoch: 1, step: 150) // Avg time/img: 0.0203 s\n",
            "loss: 5.437 (epoch: 1, step: 200) // Avg time/img: 0.0195 s\n",
            "loss: 5.399 (epoch: 1, step: 250) // Avg time/img: 0.0191 s\n",
            "loss: 5.411 (epoch: 1, step: 300) // Avg time/img: 0.0187 s\n",
            "loss: 5.409 (epoch: 1, step: 350) // Avg time/img: 0.0183 s\n",
            "loss: 5.387 (epoch: 1, step: 400) // Avg time/img: 0.0182 s\n",
            "loss: 5.391 (epoch: 1, step: 450) // Avg time/img: 0.0180 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 7.204 (epoch: 1, step: 0) // Avg time/img: 0.0365 s\n",
            "VAL loss: 6.475 (epoch: 1, step: 50) // Avg time/img: 0.0245 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.64\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_ft/model-001.pth (epoch: 1)\n",
            "save: ../save/bisenet_training_void_ft/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.0023872134540537495\n",
            "loss: 4.805 (epoch: 2, step: 0) // Avg time/img: 0.0204 s\n",
            "loss: 5.322 (epoch: 2, step: 50) // Avg time/img: 0.0170 s\n",
            "loss: 5.329 (epoch: 2, step: 100) // Avg time/img: 0.0173 s\n",
            "loss: 5.27 (epoch: 2, step: 150) // Avg time/img: 0.0173 s\n",
            "loss: 5.266 (epoch: 2, step: 200) // Avg time/img: 0.0172 s\n",
            "loss: 5.261 (epoch: 2, step: 250) // Avg time/img: 0.0171 s\n",
            "loss: 5.301 (epoch: 2, step: 300) // Avg time/img: 0.0170 s\n",
            "loss: 5.336 (epoch: 2, step: 350) // Avg time/img: 0.0170 s\n",
            "loss: 5.343 (epoch: 2, step: 400) // Avg time/img: 0.0170 s\n",
            "loss: 5.347 (epoch: 2, step: 450) // Avg time/img: 0.0170 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 7.017 (epoch: 2, step: 0) // Avg time/img: 0.0295 s\n",
            "VAL loss: 6.362 (epoch: 2, step: 50) // Avg time/img: 0.0252 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.32\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-002.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.0022738314402074057\n",
            "loss: 4.892 (epoch: 3, step: 0) // Avg time/img: 0.0232 s\n",
            "loss: 5.337 (epoch: 3, step: 50) // Avg time/img: 0.0176 s\n",
            "loss: 5.281 (epoch: 3, step: 100) // Avg time/img: 0.0175 s\n",
            "loss: 5.302 (epoch: 3, step: 150) // Avg time/img: 0.0176 s\n",
            "loss: 5.334 (epoch: 3, step: 200) // Avg time/img: 0.0174 s\n",
            "loss: 5.315 (epoch: 3, step: 250) // Avg time/img: 0.0171 s\n",
            "loss: 5.316 (epoch: 3, step: 300) // Avg time/img: 0.0172 s\n",
            "loss: 5.349 (epoch: 3, step: 350) // Avg time/img: 0.0171 s\n",
            "loss: 5.332 (epoch: 3, step: 400) // Avg time/img: 0.0172 s\n",
            "loss: 5.332 (epoch: 3, step: 450) // Avg time/img: 0.0172 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 6.727 (epoch: 3, step: 0) // Avg time/img: 0.0257 s\n",
            "VAL loss: 6.1 (epoch: 3, step: 50) // Avg time/img: 0.0254 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.57\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-003.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0021598174307570477\n",
            "loss: 6.168 (epoch: 4, step: 0) // Avg time/img: 0.0264 s\n",
            "loss: 5.358 (epoch: 4, step: 50) // Avg time/img: 0.0180 s\n",
            "loss: 5.57 (epoch: 4, step: 100) // Avg time/img: 0.0178 s\n",
            "loss: 5.57 (epoch: 4, step: 150) // Avg time/img: 0.0179 s\n",
            "loss: 5.478 (epoch: 4, step: 200) // Avg time/img: 0.0178 s\n",
            "loss: 5.435 (epoch: 4, step: 250) // Avg time/img: 0.0179 s\n",
            "loss: 5.45 (epoch: 4, step: 300) // Avg time/img: 0.0178 s\n",
            "loss: 5.407 (epoch: 4, step: 350) // Avg time/img: 0.0177 s\n",
            "loss: 5.409 (epoch: 4, step: 400) // Avg time/img: 0.0177 s\n",
            "loss: 5.369 (epoch: 4, step: 450) // Avg time/img: 0.0175 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 6.829 (epoch: 4, step: 0) // Avg time/img: 0.0208 s\n",
            "VAL loss: 6.182 (epoch: 4, step: 50) // Avg time/img: 0.0250 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.28\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-004.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.002045130365127146\n",
            "loss: 4.963 (epoch: 5, step: 0) // Avg time/img: 0.0204 s\n",
            "loss: 5.631 (epoch: 5, step: 50) // Avg time/img: 0.0179 s\n",
            "loss: 5.415 (epoch: 5, step: 100) // Avg time/img: 0.0173 s\n",
            "loss: 5.373 (epoch: 5, step: 150) // Avg time/img: 0.0174 s\n",
            "loss: 5.409 (epoch: 5, step: 200) // Avg time/img: 0.0174 s\n",
            "loss: 5.414 (epoch: 5, step: 250) // Avg time/img: 0.0173 s\n",
            "loss: 5.375 (epoch: 5, step: 300) // Avg time/img: 0.0172 s\n",
            "loss: 5.384 (epoch: 5, step: 350) // Avg time/img: 0.0172 s\n",
            "loss: 5.385 (epoch: 5, step: 400) // Avg time/img: 0.0173 s\n",
            "loss: 5.364 (epoch: 5, step: 450) // Avg time/img: 0.0172 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 6.716 (epoch: 5, step: 0) // Avg time/img: 0.0255 s\n",
            "VAL loss: 6.113 (epoch: 5, step: 50) // Avg time/img: 0.0246 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.42\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-005.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0019297237668089262\n",
            "loss: 4.053 (epoch: 6, step: 0) // Avg time/img: 0.0239 s\n",
            "loss: 5.309 (epoch: 6, step: 50) // Avg time/img: 0.0164 s\n",
            "loss: 5.298 (epoch: 6, step: 100) // Avg time/img: 0.0168 s\n",
            "loss: 5.296 (epoch: 6, step: 150) // Avg time/img: 0.0170 s\n",
            "loss: 5.299 (epoch: 6, step: 200) // Avg time/img: 0.0170 s\n",
            "loss: 5.314 (epoch: 6, step: 250) // Avg time/img: 0.0171 s\n",
            "loss: 5.312 (epoch: 6, step: 300) // Avg time/img: 0.0171 s\n",
            "loss: 5.315 (epoch: 6, step: 350) // Avg time/img: 0.0170 s\n",
            "loss: 5.318 (epoch: 6, step: 400) // Avg time/img: 0.0170 s\n",
            "loss: 5.351 (epoch: 6, step: 450) // Avg time/img: 0.0170 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 6.68 (epoch: 6, step: 0) // Avg time/img: 0.0273 s\n",
            "VAL loss: 6.117 (epoch: 6, step: 50) // Avg time/img: 0.0249 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.50\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-006.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.0018135446173430498\n",
            "loss: 3.535 (epoch: 7, step: 0) // Avg time/img: 0.0216 s\n",
            "loss: 5.167 (epoch: 7, step: 50) // Avg time/img: 0.0188 s\n",
            "loss: 5.189 (epoch: 7, step: 100) // Avg time/img: 0.0180 s\n",
            "loss: 5.232 (epoch: 7, step: 150) // Avg time/img: 0.0178 s\n",
            "loss: 5.308 (epoch: 7, step: 200) // Avg time/img: 0.0176 s\n",
            "loss: 5.305 (epoch: 7, step: 250) // Avg time/img: 0.0174 s\n",
            "loss: 5.311 (epoch: 7, step: 300) // Avg time/img: 0.0174 s\n",
            "loss: 5.317 (epoch: 7, step: 350) // Avg time/img: 0.0173 s\n",
            "loss: 5.306 (epoch: 7, step: 400) // Avg time/img: 0.0172 s\n",
            "loss: 5.316 (epoch: 7, step: 450) // Avg time/img: 0.0172 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 6.622 (epoch: 7, step: 0) // Avg time/img: 0.0237 s\n",
            "VAL loss: 6.069 (epoch: 7, step: 50) // Avg time/img: 0.0252 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.62\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-007.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0016965318981453127\n",
            "loss: 4.342 (epoch: 8, step: 0) // Avg time/img: 0.0250 s\n",
            "loss: 5.667 (epoch: 8, step: 50) // Avg time/img: 0.0183 s\n",
            "loss: 5.614 (epoch: 8, step: 100) // Avg time/img: 0.0179 s\n",
            "loss: 5.477 (epoch: 8, step: 150) // Avg time/img: 0.0177 s\n",
            "loss: 5.372 (epoch: 8, step: 200) // Avg time/img: 0.0177 s\n",
            "loss: 5.372 (epoch: 8, step: 250) // Avg time/img: 0.0175 s\n",
            "loss: 5.388 (epoch: 8, step: 300) // Avg time/img: 0.0174 s\n",
            "loss: 5.363 (epoch: 8, step: 350) // Avg time/img: 0.0174 s\n",
            "loss: 5.356 (epoch: 8, step: 400) // Avg time/img: 0.0174 s\n",
            "loss: 5.342 (epoch: 8, step: 450) // Avg time/img: 0.0174 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 6.636 (epoch: 8, step: 0) // Avg time/img: 0.0263 s\n",
            "VAL loss: 6.066 (epoch: 8, step: 50) // Avg time/img: 0.0254 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.69\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_ft/model-008.pth (epoch: 8)\n",
            "save: ../save/bisenet_training_void_ft/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.0015786146687233882\n",
            "loss: 5.098 (epoch: 9, step: 0) // Avg time/img: 0.0221 s\n",
            "loss: 5.384 (epoch: 9, step: 50) // Avg time/img: 0.0189 s\n",
            "loss: 5.305 (epoch: 9, step: 100) // Avg time/img: 0.0184 s\n",
            "loss: 5.32 (epoch: 9, step: 150) // Avg time/img: 0.0177 s\n",
            "loss: 5.318 (epoch: 9, step: 200) // Avg time/img: 0.0175 s\n",
            "loss: 5.328 (epoch: 9, step: 250) // Avg time/img: 0.0174 s\n",
            "loss: 5.314 (epoch: 9, step: 300) // Avg time/img: 0.0174 s\n",
            "loss: 5.314 (epoch: 9, step: 350) // Avg time/img: 0.0173 s\n",
            "loss: 5.321 (epoch: 9, step: 400) // Avg time/img: 0.0173 s\n",
            "loss: 5.33 (epoch: 9, step: 450) // Avg time/img: 0.0172 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 6.659 (epoch: 9, step: 0) // Avg time/img: 0.0309 s\n",
            "VAL loss: 6.114 (epoch: 9, step: 50) // Avg time/img: 0.0251 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.76\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_ft/model-009.pth (epoch: 9)\n",
            "save: ../save/bisenet_training_void_ft/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.0014597094822999506\n",
            "loss: 5.394 (epoch: 10, step: 0) // Avg time/img: 0.0227 s\n",
            "loss: 5.571 (epoch: 10, step: 50) // Avg time/img: 0.0191 s\n",
            "loss: 5.405 (epoch: 10, step: 100) // Avg time/img: 0.0190 s\n",
            "loss: 5.374 (epoch: 10, step: 150) // Avg time/img: 0.0184 s\n",
            "loss: 5.341 (epoch: 10, step: 200) // Avg time/img: 0.0183 s\n",
            "loss: 5.354 (epoch: 10, step: 250) // Avg time/img: 0.0180 s\n",
            "loss: 5.325 (epoch: 10, step: 300) // Avg time/img: 0.0179 s\n",
            "loss: 5.333 (epoch: 10, step: 350) // Avg time/img: 0.0178 s\n",
            "loss: 5.338 (epoch: 10, step: 400) // Avg time/img: 0.0176 s\n",
            "loss: 5.343 (epoch: 10, step: 450) // Avg time/img: 0.0176 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 6.909 (epoch: 10, step: 0) // Avg time/img: 0.0231 s\n",
            "VAL loss: 6.203 (epoch: 10, step: 50) // Avg time/img: 0.0248 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.33\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0013397168281703664\n",
            "loss: 5.472 (epoch: 11, step: 0) // Avg time/img: 0.0206 s\n",
            "loss: 5.327 (epoch: 11, step: 50) // Avg time/img: 0.0178 s\n",
            "loss: 5.417 (epoch: 11, step: 100) // Avg time/img: 0.0181 s\n",
            "loss: 5.376 (epoch: 11, step: 150) // Avg time/img: 0.0182 s\n",
            "loss: 5.385 (epoch: 11, step: 200) // Avg time/img: 0.0182 s\n",
            "loss: 5.42 (epoch: 11, step: 250) // Avg time/img: 0.0181 s\n",
            "loss: 5.416 (epoch: 11, step: 300) // Avg time/img: 0.0180 s\n",
            "loss: 5.386 (epoch: 11, step: 350) // Avg time/img: 0.0180 s\n",
            "loss: 5.371 (epoch: 11, step: 400) // Avg time/img: 0.0182 s\n",
            "loss: 5.375 (epoch: 11, step: 450) // Avg time/img: 0.0182 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 6.725 (epoch: 11, step: 0) // Avg time/img: 0.0335 s\n",
            "VAL loss: 6.095 (epoch: 11, step: 50) // Avg time/img: 0.0249 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.71\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-011.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.0012185160979474884\n",
            "loss: 5.59 (epoch: 12, step: 0) // Avg time/img: 0.0231 s\n",
            "loss: 5.488 (epoch: 12, step: 50) // Avg time/img: 0.0174 s\n",
            "loss: 5.455 (epoch: 12, step: 100) // Avg time/img: 0.0183 s\n",
            "loss: 5.43 (epoch: 12, step: 150) // Avg time/img: 0.0186 s\n",
            "loss: 5.442 (epoch: 12, step: 200) // Avg time/img: 0.0186 s\n",
            "loss: 5.407 (epoch: 12, step: 250) // Avg time/img: 0.0184 s\n",
            "loss: 5.41 (epoch: 12, step: 300) // Avg time/img: 0.0181 s\n",
            "loss: 5.382 (epoch: 12, step: 350) // Avg time/img: 0.0179 s\n",
            "loss: 5.356 (epoch: 12, step: 400) // Avg time/img: 0.0179 s\n",
            "loss: 5.355 (epoch: 12, step: 450) // Avg time/img: 0.0177 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 6.596 (epoch: 12, step: 0) // Avg time/img: 0.0360 s\n",
            "VAL loss: 6.072 (epoch: 12, step: 50) // Avg time/img: 0.0253 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.87\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/bisenet_training_void_ft/model-012.pth (epoch: 12)\n",
            "save: ../save/bisenet_training_void_ft/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0010959582263852174\n",
            "loss: 4.754 (epoch: 13, step: 0) // Avg time/img: 0.0324 s\n",
            "loss: 5.324 (epoch: 13, step: 50) // Avg time/img: 0.0182 s\n",
            "loss: 5.377 (epoch: 13, step: 100) // Avg time/img: 0.0178 s\n",
            "loss: 5.363 (epoch: 13, step: 150) // Avg time/img: 0.0174 s\n",
            "loss: 5.35 (epoch: 13, step: 200) // Avg time/img: 0.0174 s\n",
            "loss: 5.339 (epoch: 13, step: 250) // Avg time/img: 0.0173 s\n",
            "loss: 5.33 (epoch: 13, step: 300) // Avg time/img: 0.0173 s\n",
            "loss: 5.322 (epoch: 13, step: 350) // Avg time/img: 0.0173 s\n",
            "loss: 5.342 (epoch: 13, step: 400) // Avg time/img: 0.0173 s\n",
            "loss: 5.401 (epoch: 13, step: 450) // Avg time/img: 0.0173 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 6.746 (epoch: 13, step: 0) // Avg time/img: 0.0407 s\n",
            "VAL loss: 6.15 (epoch: 13, step: 50) // Avg time/img: 0.0252 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.66\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-013.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0009718544969969087\n",
            "loss: 4.492 (epoch: 14, step: 0) // Avg time/img: 0.0227 s\n",
            "loss: 5.062 (epoch: 14, step: 50) // Avg time/img: 0.0179 s\n",
            "loss: 5.162 (epoch: 14, step: 100) // Avg time/img: 0.0181 s\n",
            "loss: 5.222 (epoch: 14, step: 150) // Avg time/img: 0.0181 s\n",
            "loss: 5.262 (epoch: 14, step: 200) // Avg time/img: 0.0179 s\n",
            "loss: 5.294 (epoch: 14, step: 250) // Avg time/img: 0.0178 s\n",
            "loss: 5.313 (epoch: 14, step: 300) // Avg time/img: 0.0180 s\n",
            "loss: 5.312 (epoch: 14, step: 350) // Avg time/img: 0.0178 s\n",
            "loss: 5.305 (epoch: 14, step: 400) // Avg time/img: 0.0177 s\n",
            "loss: 5.301 (epoch: 14, step: 450) // Avg time/img: 0.0176 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 6.914 (epoch: 14, step: 0) // Avg time/img: 0.0262 s\n",
            "VAL loss: 6.267 (epoch: 14, step: 50) // Avg time/img: 0.0244 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.26\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-014.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0008459586547541247\n",
            "loss: 6.61 (epoch: 15, step: 0) // Avg time/img: 0.0240 s\n",
            "loss: 5.345 (epoch: 15, step: 50) // Avg time/img: 0.0171 s\n",
            "loss: 5.423 (epoch: 15, step: 100) // Avg time/img: 0.0171 s\n",
            "loss: 5.377 (epoch: 15, step: 150) // Avg time/img: 0.0171 s\n",
            "loss: 5.352 (epoch: 15, step: 200) // Avg time/img: 0.0172 s\n",
            "loss: 5.325 (epoch: 15, step: 250) // Avg time/img: 0.0172 s\n",
            "loss: 5.31 (epoch: 15, step: 300) // Avg time/img: 0.0173 s\n",
            "loss: 5.306 (epoch: 15, step: 350) // Avg time/img: 0.0172 s\n",
            "loss: 5.271 (epoch: 15, step: 400) // Avg time/img: 0.0171 s\n",
            "loss: 5.3 (epoch: 15, step: 450) // Avg time/img: 0.0172 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 6.764 (epoch: 15, step: 0) // Avg time/img: 0.0251 s\n",
            "VAL loss: 6.131 (epoch: 15, step: 50) // Avg time/img: 0.0255 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.71\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0007179364718731469\n",
            "loss: 6.383 (epoch: 16, step: 0) // Avg time/img: 0.0232 s\n",
            "loss: 5.477 (epoch: 16, step: 50) // Avg time/img: 0.0183 s\n",
            "loss: 5.318 (epoch: 16, step: 100) // Avg time/img: 0.0185 s\n",
            "loss: 5.27 (epoch: 16, step: 150) // Avg time/img: 0.0184 s\n",
            "loss: 5.258 (epoch: 16, step: 200) // Avg time/img: 0.0182 s\n",
            "loss: 5.254 (epoch: 16, step: 250) // Avg time/img: 0.0180 s\n",
            "loss: 5.286 (epoch: 16, step: 300) // Avg time/img: 0.0178 s\n",
            "loss: 5.28 (epoch: 16, step: 350) // Avg time/img: 0.0177 s\n",
            "loss: 5.292 (epoch: 16, step: 400) // Avg time/img: 0.0177 s\n",
            "loss: 5.329 (epoch: 16, step: 450) // Avg time/img: 0.0176 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 6.623 (epoch: 16, step: 0) // Avg time/img: 0.0322 s\n",
            "VAL loss: 6.044 (epoch: 16, step: 50) // Avg time/img: 0.0255 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.87\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-016.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.0005873094715440094\n",
            "loss: 3.336 (epoch: 17, step: 0) // Avg time/img: 0.0239 s\n",
            "loss: 5.597 (epoch: 17, step: 50) // Avg time/img: 0.0174 s\n",
            "loss: 5.568 (epoch: 17, step: 100) // Avg time/img: 0.0170 s\n",
            "loss: 5.515 (epoch: 17, step: 150) // Avg time/img: 0.0173 s\n",
            "loss: 5.525 (epoch: 17, step: 200) // Avg time/img: 0.0171 s\n",
            "loss: 5.497 (epoch: 17, step: 250) // Avg time/img: 0.0171 s\n",
            "loss: 5.476 (epoch: 17, step: 300) // Avg time/img: 0.0171 s\n",
            "loss: 5.432 (epoch: 17, step: 350) // Avg time/img: 0.0173 s\n",
            "loss: 5.373 (epoch: 17, step: 400) // Avg time/img: 0.0175 s\n",
            "loss: 5.351 (epoch: 17, step: 450) // Avg time/img: 0.0175 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 6.753 (epoch: 17, step: 0) // Avg time/img: 0.0434 s\n",
            "VAL loss: 6.161 (epoch: 17, step: 50) // Avg time/img: 0.0251 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.75\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0004533380182841864\n",
            "loss: 5.659 (epoch: 18, step: 0) // Avg time/img: 0.0216 s\n",
            "loss: 5.259 (epoch: 18, step: 50) // Avg time/img: 0.0187 s\n",
            "loss: 5.314 (epoch: 18, step: 100) // Avg time/img: 0.0181 s\n",
            "loss: 5.36 (epoch: 18, step: 150) // Avg time/img: 0.0179 s\n",
            "loss: 5.353 (epoch: 18, step: 200) // Avg time/img: 0.0176 s\n",
            "loss: 5.355 (epoch: 18, step: 250) // Avg time/img: 0.0176 s\n",
            "loss: 5.367 (epoch: 18, step: 300) // Avg time/img: 0.0175 s\n",
            "loss: 5.35 (epoch: 18, step: 350) // Avg time/img: 0.0174 s\n",
            "loss: 5.348 (epoch: 18, step: 400) // Avg time/img: 0.0174 s\n",
            "loss: 5.344 (epoch: 18, step: 450) // Avg time/img: 0.0174 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 6.68 (epoch: 18, step: 0) // Avg time/img: 0.0331 s\n",
            "VAL loss: 6.115 (epoch: 18, step: 50) // Avg time/img: 0.0259 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.69\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.00031473135294854176\n",
            "loss: 6.242 (epoch: 19, step: 0) // Avg time/img: 0.0240 s\n",
            "loss: 5.481 (epoch: 19, step: 50) // Avg time/img: 0.0183 s\n",
            "loss: 5.362 (epoch: 19, step: 100) // Avg time/img: 0.0177 s\n",
            "loss: 5.288 (epoch: 19, step: 150) // Avg time/img: 0.0176 s\n",
            "loss: 5.291 (epoch: 19, step: 200) // Avg time/img: 0.0176 s\n",
            "loss: 5.343 (epoch: 19, step: 250) // Avg time/img: 0.0175 s\n",
            "loss: 5.302 (epoch: 19, step: 300) // Avg time/img: 0.0174 s\n",
            "loss: 5.337 (epoch: 19, step: 350) // Avg time/img: 0.0173 s\n",
            "loss: 5.326 (epoch: 19, step: 400) // Avg time/img: 0.0173 s\n",
            "loss: 5.334 (epoch: 19, step: 450) // Avg time/img: 0.0173 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 6.913 (epoch: 19, step: 0) // Avg time/img: 0.0301 s\n",
            "VAL loss: 6.228 (epoch: 19, step: 50) // Avg time/img: 0.0261 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.42\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-019.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.00016866035595919555\n",
            "loss: 5.35 (epoch: 20, step: 0) // Avg time/img: 0.0237 s\n",
            "loss: 5.321 (epoch: 20, step: 50) // Avg time/img: 0.0178 s\n",
            "loss: 5.303 (epoch: 20, step: 100) // Avg time/img: 0.0181 s\n",
            "loss: 5.352 (epoch: 20, step: 150) // Avg time/img: 0.0183 s\n",
            "loss: 5.36 (epoch: 20, step: 200) // Avg time/img: 0.0183 s\n",
            "loss: 5.343 (epoch: 20, step: 250) // Avg time/img: 0.0183 s\n",
            "loss: 5.346 (epoch: 20, step: 300) // Avg time/img: 0.0182 s\n",
            "loss: 5.359 (epoch: 20, step: 350) // Avg time/img: 0.0180 s\n",
            "loss: 5.371 (epoch: 20, step: 400) // Avg time/img: 0.0180 s\n",
            "loss: 5.381 (epoch: 20, step: 450) // Avg time/img: 0.0179 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 6.675 (epoch: 20, step: 0) // Avg time/img: 0.0238 s\n",
            "VAL loss: 6.085 (epoch: 20, step: 50) // Avg time/img: 0.0246 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m66.84\u001b[0m %\n",
            "save: ../save/bisenet_training_void_ft/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ENet Training"
      ],
      "metadata": {
        "id": "MXZtMjC4gdDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same procedure adopted for BiSeNet is applied for ENet here."
      ],
      "metadata": {
        "id": "O6VNWGTVAx-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"enet\", num_epochs=40, batch_size=6, stop_epoch=20)\n",
        "# %cd ../save\n",
        "# !zip -r enet_training_void.zip enet_training_void/"
      ],
      "metadata": {
        "id": "ZhVD_Bszgu8y",
        "outputId": "d8a8f849-f2b5-4e93-d0de-3c47e73d5572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "Criterion: CrossEntropyLoss2d\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 3.071 (epoch: 1, step: 0) // Avg time/img: 0.4825 s\n",
            "loss: 2.726 (epoch: 1, step: 50) // Avg time/img: 0.0765 s\n",
            "loss: 2.334 (epoch: 1, step: 100) // Avg time/img: 0.0721 s\n",
            "loss: 2.05 (epoch: 1, step: 150) // Avg time/img: 0.0711 s\n",
            "loss: 1.858 (epoch: 1, step: 200) // Avg time/img: 0.0707 s\n",
            "loss: 1.716 (epoch: 1, step: 250) // Avg time/img: 0.0707 s\n",
            "loss: 1.613 (epoch: 1, step: 300) // Avg time/img: 0.0705 s\n",
            "loss: 1.531 (epoch: 1, step: 350) // Avg time/img: 0.0707 s\n",
            "loss: 1.462 (epoch: 1, step: 400) // Avg time/img: 0.0707 s\n",
            "loss: 1.406 (epoch: 1, step: 450) // Avg time/img: 0.0707 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 0.7688 (epoch: 1, step: 0) // Avg time/img: 0.0339 s\n",
            "VAL loss: 0.9467 (epoch: 1, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m17.06\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-001.pth (epoch: 1)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.8654 (epoch: 2, step: 0) // Avg time/img: 0.0818 s\n",
            "loss: 0.8898 (epoch: 2, step: 50) // Avg time/img: 0.0720 s\n",
            "loss: 0.8933 (epoch: 2, step: 100) // Avg time/img: 0.0711 s\n",
            "loss: 0.8946 (epoch: 2, step: 150) // Avg time/img: 0.0710 s\n",
            "loss: 0.8772 (epoch: 2, step: 200) // Avg time/img: 0.0708 s\n",
            "loss: 0.8709 (epoch: 2, step: 250) // Avg time/img: 0.0708 s\n",
            "loss: 0.8633 (epoch: 2, step: 300) // Avg time/img: 0.0706 s\n",
            "loss: 0.8531 (epoch: 2, step: 350) // Avg time/img: 0.0705 s\n",
            "loss: 0.8472 (epoch: 2, step: 400) // Avg time/img: 0.0706 s\n",
            "loss: 0.8387 (epoch: 2, step: 450) // Avg time/img: 0.0705 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 0.6003 (epoch: 2, step: 0) // Avg time/img: 0.0339 s\n",
            "VAL loss: 0.8097 (epoch: 2, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m21.62\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-002.pth (epoch: 2)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.6776 (epoch: 3, step: 0) // Avg time/img: 0.0805 s\n",
            "loss: 0.7644 (epoch: 3, step: 50) // Avg time/img: 0.0704 s\n",
            "loss: 0.7647 (epoch: 3, step: 100) // Avg time/img: 0.0704 s\n",
            "loss: 0.7667 (epoch: 3, step: 150) // Avg time/img: 0.0701 s\n",
            "loss: 0.761 (epoch: 3, step: 200) // Avg time/img: 0.0705 s\n",
            "loss: 0.7554 (epoch: 3, step: 250) // Avg time/img: 0.0702 s\n",
            "loss: 0.7559 (epoch: 3, step: 300) // Avg time/img: 0.0702 s\n",
            "loss: 0.7492 (epoch: 3, step: 350) // Avg time/img: 0.0703 s\n",
            "loss: 0.7413 (epoch: 3, step: 400) // Avg time/img: 0.0702 s\n",
            "loss: 0.7368 (epoch: 3, step: 450) // Avg time/img: 0.0702 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 0.5135 (epoch: 3, step: 0) // Avg time/img: 0.0340 s\n",
            "VAL loss: 0.718 (epoch: 3, step: 50) // Avg time/img: 0.0312 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.33\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-003.pth (epoch: 3)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.6359 (epoch: 4, step: 0) // Avg time/img: 0.0911 s\n",
            "loss: 0.6842 (epoch: 4, step: 50) // Avg time/img: 0.0716 s\n",
            "loss: 0.6708 (epoch: 4, step: 100) // Avg time/img: 0.0714 s\n",
            "loss: 0.6689 (epoch: 4, step: 150) // Avg time/img: 0.0715 s\n",
            "loss: 0.6664 (epoch: 4, step: 200) // Avg time/img: 0.0710 s\n",
            "loss: 0.6765 (epoch: 4, step: 250) // Avg time/img: 0.0709 s\n",
            "loss: 0.6727 (epoch: 4, step: 300) // Avg time/img: 0.0712 s\n",
            "loss: 0.6751 (epoch: 4, step: 350) // Avg time/img: 0.0709 s\n",
            "loss: 0.6718 (epoch: 4, step: 400) // Avg time/img: 0.0710 s\n",
            "loss: 0.6708 (epoch: 4, step: 450) // Avg time/img: 0.0709 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 0.6086 (epoch: 4, step: 0) // Avg time/img: 0.0376 s\n",
            "VAL loss: 0.7198 (epoch: 4, step: 50) // Avg time/img: 0.0321 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m24.40\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-004.pth (epoch: 4)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.5145 (epoch: 5, step: 0) // Avg time/img: 0.0739 s\n",
            "loss: 0.6258 (epoch: 5, step: 50) // Avg time/img: 0.0720 s\n",
            "loss: 0.6396 (epoch: 5, step: 100) // Avg time/img: 0.0716 s\n",
            "loss: 0.6345 (epoch: 5, step: 150) // Avg time/img: 0.0711 s\n",
            "loss: 0.6271 (epoch: 5, step: 200) // Avg time/img: 0.0711 s\n",
            "loss: 0.6315 (epoch: 5, step: 250) // Avg time/img: 0.0710 s\n",
            "loss: 0.6335 (epoch: 5, step: 300) // Avg time/img: 0.0709 s\n",
            "loss: 0.6313 (epoch: 5, step: 350) // Avg time/img: 0.0711 s\n",
            "loss: 0.6264 (epoch: 5, step: 400) // Avg time/img: 0.0709 s\n",
            "loss: 0.6287 (epoch: 5, step: 450) // Avg time/img: 0.0710 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 0.4608 (epoch: 5, step: 0) // Avg time/img: 0.0543 s\n",
            "VAL loss: 0.6572 (epoch: 5, step: 50) // Avg time/img: 0.0316 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m26.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-005.pth (epoch: 5)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.5744 (epoch: 6, step: 0) // Avg time/img: 0.1021 s\n",
            "loss: 0.6159 (epoch: 6, step: 50) // Avg time/img: 0.0712 s\n",
            "loss: 0.6047 (epoch: 6, step: 100) // Avg time/img: 0.0710 s\n",
            "loss: 0.6015 (epoch: 6, step: 150) // Avg time/img: 0.0708 s\n",
            "loss: 0.601 (epoch: 6, step: 200) // Avg time/img: 0.0712 s\n",
            "loss: 0.5999 (epoch: 6, step: 250) // Avg time/img: 0.0712 s\n",
            "loss: 0.5965 (epoch: 6, step: 300) // Avg time/img: 0.0713 s\n",
            "loss: 0.5983 (epoch: 6, step: 350) // Avg time/img: 0.0711 s\n",
            "loss: 0.5996 (epoch: 6, step: 400) // Avg time/img: 0.0711 s\n",
            "loss: 0.5977 (epoch: 6, step: 450) // Avg time/img: 0.0712 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 0.4647 (epoch: 6, step: 0) // Avg time/img: 0.0398 s\n",
            "VAL loss: 0.6201 (epoch: 6, step: 50) // Avg time/img: 0.0290 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.64\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-006.pth (epoch: 6)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.5365 (epoch: 7, step: 0) // Avg time/img: 0.0859 s\n",
            "loss: 0.5655 (epoch: 7, step: 50) // Avg time/img: 0.0708 s\n",
            "loss: 0.5651 (epoch: 7, step: 100) // Avg time/img: 0.0709 s\n",
            "loss: 0.5665 (epoch: 7, step: 150) // Avg time/img: 0.0709 s\n",
            "loss: 0.5674 (epoch: 7, step: 200) // Avg time/img: 0.0710 s\n",
            "loss: 0.5673 (epoch: 7, step: 250) // Avg time/img: 0.0708 s\n",
            "loss: 0.5674 (epoch: 7, step: 300) // Avg time/img: 0.0711 s\n",
            "loss: 0.5663 (epoch: 7, step: 350) // Avg time/img: 0.0711 s\n",
            "loss: 0.569 (epoch: 7, step: 400) // Avg time/img: 0.0712 s\n",
            "loss: 0.5686 (epoch: 7, step: 450) // Avg time/img: 0.0713 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 0.4213 (epoch: 7, step: 0) // Avg time/img: 0.0380 s\n",
            "VAL loss: 0.5944 (epoch: 7, step: 50) // Avg time/img: 0.0308 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m27.95\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-007.pth (epoch: 7)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.5909 (epoch: 8, step: 0) // Avg time/img: 0.0906 s\n",
            "loss: 0.5517 (epoch: 8, step: 50) // Avg time/img: 0.0717 s\n",
            "loss: 0.547 (epoch: 8, step: 100) // Avg time/img: 0.0719 s\n",
            "loss: 0.5463 (epoch: 8, step: 150) // Avg time/img: 0.0715 s\n",
            "loss: 0.5417 (epoch: 8, step: 200) // Avg time/img: 0.0713 s\n",
            "loss: 0.548 (epoch: 8, step: 250) // Avg time/img: 0.0713 s\n",
            "loss: 0.5443 (epoch: 8, step: 300) // Avg time/img: 0.0715 s\n",
            "loss: 0.5451 (epoch: 8, step: 350) // Avg time/img: 0.0714 s\n",
            "loss: 0.5469 (epoch: 8, step: 400) // Avg time/img: 0.0713 s\n",
            "loss: 0.5522 (epoch: 8, step: 450) // Avg time/img: 0.0713 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 0.3893 (epoch: 8, step: 0) // Avg time/img: 0.0283 s\n",
            "VAL loss: 0.5826 (epoch: 8, step: 50) // Avg time/img: 0.0306 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.00\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-008.pth (epoch: 8)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.4075 (epoch: 9, step: 0) // Avg time/img: 0.0851 s\n",
            "loss: 0.5278 (epoch: 9, step: 50) // Avg time/img: 0.0726 s\n",
            "loss: 0.5363 (epoch: 9, step: 100) // Avg time/img: 0.0718 s\n",
            "loss: 0.5315 (epoch: 9, step: 150) // Avg time/img: 0.0717 s\n",
            "loss: 0.5306 (epoch: 9, step: 200) // Avg time/img: 0.0716 s\n",
            "loss: 0.5377 (epoch: 9, step: 250) // Avg time/img: 0.0712 s\n",
            "loss: 0.5388 (epoch: 9, step: 300) // Avg time/img: 0.0713 s\n",
            "loss: 0.539 (epoch: 9, step: 350) // Avg time/img: 0.0713 s\n",
            "loss: 0.5382 (epoch: 9, step: 400) // Avg time/img: 0.0711 s\n",
            "loss: 0.5349 (epoch: 9, step: 450) // Avg time/img: 0.0712 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 0.3828 (epoch: 9, step: 0) // Avg time/img: 0.0290 s\n",
            "VAL loss: 0.6014 (epoch: 9, step: 50) // Avg time/img: 0.0305 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.47\u001b[0m %\n",
            "save: ../save/enet_training_void/model-009.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3678 (epoch: 10, step: 0) // Avg time/img: 0.0844 s\n",
            "loss: 0.5234 (epoch: 10, step: 50) // Avg time/img: 0.0731 s\n",
            "loss: 0.5248 (epoch: 10, step: 100) // Avg time/img: 0.0721 s\n",
            "loss: 0.5224 (epoch: 10, step: 150) // Avg time/img: 0.0716 s\n",
            "loss: 0.5234 (epoch: 10, step: 200) // Avg time/img: 0.0713 s\n",
            "loss: 0.5261 (epoch: 10, step: 250) // Avg time/img: 0.0712 s\n",
            "loss: 0.5248 (epoch: 10, step: 300) // Avg time/img: 0.0711 s\n",
            "loss: 0.5248 (epoch: 10, step: 350) // Avg time/img: 0.0712 s\n",
            "loss: 0.5251 (epoch: 10, step: 400) // Avg time/img: 0.0714 s\n",
            "loss: 0.5245 (epoch: 10, step: 450) // Avg time/img: 0.0714 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 0.3757 (epoch: 10, step: 0) // Avg time/img: 0.0417 s\n",
            "VAL loss: 0.6113 (epoch: 10, step: 50) // Avg time/img: 0.0302 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m28.47\u001b[0m %\n",
            "save: ../save/enet_training_void/model-010.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.5153 (epoch: 11, step: 0) // Avg time/img: 0.0912 s\n",
            "loss: 0.544 (epoch: 11, step: 50) // Avg time/img: 0.0722 s\n",
            "loss: 0.533 (epoch: 11, step: 100) // Avg time/img: 0.0720 s\n",
            "loss: 0.528 (epoch: 11, step: 150) // Avg time/img: 0.0717 s\n",
            "loss: 0.5212 (epoch: 11, step: 200) // Avg time/img: 0.0711 s\n",
            "loss: 0.5154 (epoch: 11, step: 250) // Avg time/img: 0.0710 s\n",
            "loss: 0.5148 (epoch: 11, step: 300) // Avg time/img: 0.0710 s\n",
            "loss: 0.5148 (epoch: 11, step: 350) // Avg time/img: 0.0709 s\n",
            "loss: 0.5124 (epoch: 11, step: 400) // Avg time/img: 0.0709 s\n",
            "loss: 0.5101 (epoch: 11, step: 450) // Avg time/img: 0.0709 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 0.3537 (epoch: 11, step: 0) // Avg time/img: 0.0239 s\n",
            "VAL loss: 0.5286 (epoch: 11, step: 50) // Avg time/img: 0.0300 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.08\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-011.pth (epoch: 11)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.4812 (epoch: 12, step: 0) // Avg time/img: 0.0895 s\n",
            "loss: 0.4578 (epoch: 12, step: 50) // Avg time/img: 0.0718 s\n",
            "loss: 0.4668 (epoch: 12, step: 100) // Avg time/img: 0.0723 s\n",
            "loss: 0.4806 (epoch: 12, step: 150) // Avg time/img: 0.0723 s\n",
            "loss: 0.4861 (epoch: 12, step: 200) // Avg time/img: 0.0719 s\n",
            "loss: 0.4953 (epoch: 12, step: 250) // Avg time/img: 0.0719 s\n",
            "loss: 0.497 (epoch: 12, step: 300) // Avg time/img: 0.0717 s\n",
            "loss: 0.4941 (epoch: 12, step: 350) // Avg time/img: 0.0714 s\n",
            "loss: 0.4943 (epoch: 12, step: 400) // Avg time/img: 0.0715 s\n",
            "loss: 0.493 (epoch: 12, step: 450) // Avg time/img: 0.0714 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 0.4151 (epoch: 12, step: 0) // Avg time/img: 0.0292 s\n",
            "VAL loss: 0.573 (epoch: 12, step: 50) // Avg time/img: 0.0296 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m29.64\u001b[0m %\n",
            "save: ../save/enet_training_void/model-012.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.4527 (epoch: 13, step: 0) // Avg time/img: 0.0849 s\n",
            "loss: 0.4852 (epoch: 13, step: 50) // Avg time/img: 0.0722 s\n",
            "loss: 0.4741 (epoch: 13, step: 100) // Avg time/img: 0.0712 s\n",
            "loss: 0.4772 (epoch: 13, step: 150) // Avg time/img: 0.0713 s\n",
            "loss: 0.4791 (epoch: 13, step: 200) // Avg time/img: 0.0713 s\n",
            "loss: 0.4819 (epoch: 13, step: 250) // Avg time/img: 0.0712 s\n",
            "loss: 0.4832 (epoch: 13, step: 300) // Avg time/img: 0.0712 s\n",
            "loss: 0.4846 (epoch: 13, step: 350) // Avg time/img: 0.0713 s\n",
            "loss: 0.4859 (epoch: 13, step: 400) // Avg time/img: 0.0712 s\n",
            "loss: 0.4884 (epoch: 13, step: 450) // Avg time/img: 0.0712 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 0.3441 (epoch: 13, step: 0) // Avg time/img: 0.0264 s\n",
            "VAL loss: 0.5391 (epoch: 13, step: 50) // Avg time/img: 0.0298 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m30.31\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-013.pth (epoch: 13)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.5451 (epoch: 14, step: 0) // Avg time/img: 0.0861 s\n",
            "loss: 0.4601 (epoch: 14, step: 50) // Avg time/img: 0.0725 s\n",
            "loss: 0.4665 (epoch: 14, step: 100) // Avg time/img: 0.0716 s\n",
            "loss: 0.4613 (epoch: 14, step: 150) // Avg time/img: 0.0714 s\n",
            "loss: 0.4699 (epoch: 14, step: 200) // Avg time/img: 0.0717 s\n",
            "loss: 0.4697 (epoch: 14, step: 250) // Avg time/img: 0.0716 s\n",
            "loss: 0.4728 (epoch: 14, step: 300) // Avg time/img: 0.0716 s\n",
            "loss: 0.4756 (epoch: 14, step: 350) // Avg time/img: 0.0717 s\n",
            "loss: 0.4781 (epoch: 14, step: 400) // Avg time/img: 0.0716 s\n",
            "loss: 0.4766 (epoch: 14, step: 450) // Avg time/img: 0.0714 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 0.3911 (epoch: 14, step: 0) // Avg time/img: 0.0394 s\n",
            "VAL loss: 0.5276 (epoch: 14, step: 50) // Avg time/img: 0.0293 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.64\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-014.pth (epoch: 14)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3574 (epoch: 15, step: 0) // Avg time/img: 0.0759 s\n",
            "loss: 0.4487 (epoch: 15, step: 50) // Avg time/img: 0.0723 s\n",
            "loss: 0.4575 (epoch: 15, step: 100) // Avg time/img: 0.0720 s\n",
            "loss: 0.4566 (epoch: 15, step: 150) // Avg time/img: 0.0717 s\n",
            "loss: 0.4621 (epoch: 15, step: 200) // Avg time/img: 0.0713 s\n",
            "loss: 0.4628 (epoch: 15, step: 250) // Avg time/img: 0.0714 s\n",
            "loss: 0.4617 (epoch: 15, step: 300) // Avg time/img: 0.0718 s\n",
            "loss: 0.463 (epoch: 15, step: 350) // Avg time/img: 0.0717 s\n",
            "loss: 0.4639 (epoch: 15, step: 400) // Avg time/img: 0.0716 s\n",
            "loss: 0.4643 (epoch: 15, step: 450) // Avg time/img: 0.0714 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 0.3394 (epoch: 15, step: 0) // Avg time/img: 0.0382 s\n",
            "VAL loss: 0.5261 (epoch: 15, step: 50) // Avg time/img: 0.0304 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m31.56\u001b[0m %\n",
            "save: ../save/enet_training_void/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3961 (epoch: 16, step: 0) // Avg time/img: 0.0815 s\n",
            "loss: 0.4433 (epoch: 16, step: 50) // Avg time/img: 0.0710 s\n",
            "loss: 0.4452 (epoch: 16, step: 100) // Avg time/img: 0.0712 s\n",
            "loss: 0.4511 (epoch: 16, step: 150) // Avg time/img: 0.0710 s\n",
            "loss: 0.4551 (epoch: 16, step: 200) // Avg time/img: 0.0709 s\n",
            "loss: 0.4608 (epoch: 16, step: 250) // Avg time/img: 0.0709 s\n",
            "loss: 0.4586 (epoch: 16, step: 300) // Avg time/img: 0.0709 s\n",
            "loss: 0.4595 (epoch: 16, step: 350) // Avg time/img: 0.0710 s\n",
            "loss: 0.4594 (epoch: 16, step: 400) // Avg time/img: 0.0711 s\n",
            "loss: 0.459 (epoch: 16, step: 450) // Avg time/img: 0.0710 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 0.3849 (epoch: 16, step: 0) // Avg time/img: 0.0368 s\n",
            "VAL loss: 0.5152 (epoch: 16, step: 50) // Avg time/img: 0.0294 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.47\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-016.pth (epoch: 16)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3412 (epoch: 17, step: 0) // Avg time/img: 0.0837 s\n",
            "loss: 0.4565 (epoch: 17, step: 50) // Avg time/img: 0.0707 s\n",
            "loss: 0.46 (epoch: 17, step: 100) // Avg time/img: 0.0708 s\n",
            "loss: 0.4531 (epoch: 17, step: 150) // Avg time/img: 0.0708 s\n",
            "loss: 0.4513 (epoch: 17, step: 200) // Avg time/img: 0.0705 s\n",
            "loss: 0.4451 (epoch: 17, step: 250) // Avg time/img: 0.0705 s\n",
            "loss: 0.4461 (epoch: 17, step: 300) // Avg time/img: 0.0703 s\n",
            "loss: 0.4479 (epoch: 17, step: 350) // Avg time/img: 0.0706 s\n",
            "loss: 0.4496 (epoch: 17, step: 400) // Avg time/img: 0.0706 s\n",
            "loss: 0.451 (epoch: 17, step: 450) // Avg time/img: 0.0706 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 0.3668 (epoch: 17, step: 0) // Avg time/img: 0.0298 s\n",
            "VAL loss: 0.5024 (epoch: 17, step: 50) // Avg time/img: 0.0293 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.05\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-017.pth (epoch: 17)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.4001 (epoch: 18, step: 0) // Avg time/img: 0.0872 s\n",
            "loss: 0.4312 (epoch: 18, step: 50) // Avg time/img: 0.0712 s\n",
            "loss: 0.4392 (epoch: 18, step: 100) // Avg time/img: 0.0713 s\n",
            "loss: 0.4352 (epoch: 18, step: 150) // Avg time/img: 0.0709 s\n",
            "loss: 0.4346 (epoch: 18, step: 200) // Avg time/img: 0.0705 s\n",
            "loss: 0.4356 (epoch: 18, step: 250) // Avg time/img: 0.0705 s\n",
            "loss: 0.4395 (epoch: 18, step: 300) // Avg time/img: 0.0703 s\n",
            "loss: 0.443 (epoch: 18, step: 350) // Avg time/img: 0.0704 s\n",
            "loss: 0.4454 (epoch: 18, step: 400) // Avg time/img: 0.0702 s\n",
            "loss: 0.4453 (epoch: 18, step: 450) // Avg time/img: 0.0701 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 0.3821 (epoch: 18, step: 0) // Avg time/img: 0.0283 s\n",
            "VAL loss: 0.4929 (epoch: 18, step: 50) // Avg time/img: 0.0289 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.46\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-018.pth (epoch: 18)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.462 (epoch: 19, step: 0) // Avg time/img: 0.0849 s\n",
            "loss: 0.4254 (epoch: 19, step: 50) // Avg time/img: 0.0704 s\n",
            "loss: 0.4313 (epoch: 19, step: 100) // Avg time/img: 0.0712 s\n",
            "loss: 0.4317 (epoch: 19, step: 150) // Avg time/img: 0.0710 s\n",
            "loss: 0.4333 (epoch: 19, step: 200) // Avg time/img: 0.0707 s\n",
            "loss: 0.4345 (epoch: 19, step: 250) // Avg time/img: 0.0705 s\n",
            "loss: 0.4343 (epoch: 19, step: 300) // Avg time/img: 0.0703 s\n",
            "loss: 0.4343 (epoch: 19, step: 350) // Avg time/img: 0.0704 s\n",
            "loss: 0.4411 (epoch: 19, step: 400) // Avg time/img: 0.0704 s\n",
            "loss: 0.4386 (epoch: 19, step: 450) // Avg time/img: 0.0703 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 0.3093 (epoch: 19, step: 0) // Avg time/img: 0.0253 s\n",
            "VAL loss: 0.4757 (epoch: 19, step: 50) // Avg time/img: 0.0295 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m33.48\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-019.pth (epoch: 19)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3925 (epoch: 20, step: 0) // Avg time/img: 0.0848 s\n",
            "loss: 0.4327 (epoch: 20, step: 50) // Avg time/img: 0.0720 s\n",
            "loss: 0.4197 (epoch: 20, step: 100) // Avg time/img: 0.0710 s\n",
            "loss: 0.4241 (epoch: 20, step: 150) // Avg time/img: 0.0701 s\n",
            "loss: 0.4289 (epoch: 20, step: 200) // Avg time/img: 0.0698 s\n",
            "loss: 0.4302 (epoch: 20, step: 250) // Avg time/img: 0.0699 s\n",
            "loss: 0.4295 (epoch: 20, step: 300) // Avg time/img: 0.0699 s\n",
            "loss: 0.4272 (epoch: 20, step: 350) // Avg time/img: 0.0701 s\n",
            "loss: 0.4291 (epoch: 20, step: 400) // Avg time/img: 0.0701 s\n",
            "loss: 0.4287 (epoch: 20, step: 450) // Avg time/img: 0.0700 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 0.3178 (epoch: 20, step: 0) // Avg time/img: 0.0353 s\n",
            "VAL loss: 0.5272 (epoch: 20, step: 50) // Avg time/img: 0.0292 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.09\u001b[0m %\n",
            "save: ../save/enet_training_void/model-020.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"enet\", num_epochs=40, batch_size=6, stop_epoch=40, resume=True)\n",
        "# %cd ../save\n",
        "# !zip -r enet_training_void.zip enet_training_void/"
      ],
      "metadata": {
        "id": "ljzbCISEgyPw",
        "outputId": "02c9cb55-2e19-4ae2-a38c-fc75416bbbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "Criterion: CrossEntropyLoss2d\n",
            "=> Loaded checkpoint at epoch 21)\n",
            "----- TRAINING - EPOCH 21 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3133 (epoch: 21, step: 0) // Avg time/img: 0.4467 s\n",
            "loss: 0.423 (epoch: 21, step: 50) // Avg time/img: 0.0767 s\n",
            "loss: 0.4169 (epoch: 21, step: 100) // Avg time/img: 0.0734 s\n",
            "loss: 0.4144 (epoch: 21, step: 150) // Avg time/img: 0.0724 s\n",
            "loss: 0.42 (epoch: 21, step: 200) // Avg time/img: 0.0723 s\n",
            "loss: 0.4207 (epoch: 21, step: 250) // Avg time/img: 0.0722 s\n",
            "loss: 0.4208 (epoch: 21, step: 300) // Avg time/img: 0.0719 s\n",
            "loss: 0.4202 (epoch: 21, step: 350) // Avg time/img: 0.0720 s\n",
            "loss: 0.4216 (epoch: 21, step: 400) // Avg time/img: 0.0720 s\n",
            "loss: 0.4243 (epoch: 21, step: 450) // Avg time/img: 0.0721 s\n",
            "----- VALIDATING - EPOCH 21 -----\n",
            "VAL loss: 0.3167 (epoch: 21, step: 0) // Avg time/img: 0.0396 s\n",
            "VAL loss: 0.4666 (epoch: 21, step: 50) // Avg time/img: 0.0312 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m35.06\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-021.pth (epoch: 21)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 21)\n",
            "----- TRAINING - EPOCH 22 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.5094 (epoch: 22, step: 0) // Avg time/img: 0.0788 s\n",
            "loss: 0.3926 (epoch: 22, step: 50) // Avg time/img: 0.0722 s\n",
            "loss: 0.4048 (epoch: 22, step: 100) // Avg time/img: 0.0718 s\n",
            "loss: 0.4038 (epoch: 22, step: 150) // Avg time/img: 0.0720 s\n",
            "loss: 0.4096 (epoch: 22, step: 200) // Avg time/img: 0.0719 s\n",
            "loss: 0.4105 (epoch: 22, step: 250) // Avg time/img: 0.0716 s\n",
            "loss: 0.4116 (epoch: 22, step: 300) // Avg time/img: 0.0715 s\n",
            "loss: 0.4122 (epoch: 22, step: 350) // Avg time/img: 0.0717 s\n",
            "loss: 0.4155 (epoch: 22, step: 400) // Avg time/img: 0.0715 s\n",
            "loss: 0.4176 (epoch: 22, step: 450) // Avg time/img: 0.0715 s\n",
            "----- VALIDATING - EPOCH 22 -----\n",
            "VAL loss: 0.4187 (epoch: 22, step: 0) // Avg time/img: 0.0248 s\n",
            "VAL loss: 0.5914 (epoch: 22, step: 50) // Avg time/img: 0.0305 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m32.46\u001b[0m %\n",
            "save: ../save/enet_training_void/model-022.pth (epoch: 22)\n",
            "----- TRAINING - EPOCH 23 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.523 (epoch: 23, step: 0) // Avg time/img: 0.0801 s\n",
            "loss: 0.4062 (epoch: 23, step: 50) // Avg time/img: 0.0710 s\n",
            "loss: 0.4116 (epoch: 23, step: 100) // Avg time/img: 0.0704 s\n",
            "loss: 0.4145 (epoch: 23, step: 150) // Avg time/img: 0.0712 s\n",
            "loss: 0.4141 (epoch: 23, step: 200) // Avg time/img: 0.0713 s\n",
            "loss: 0.4136 (epoch: 23, step: 250) // Avg time/img: 0.0717 s\n",
            "loss: 0.4104 (epoch: 23, step: 300) // Avg time/img: 0.0717 s\n",
            "loss: 0.4113 (epoch: 23, step: 350) // Avg time/img: 0.0720 s\n",
            "loss: 0.4137 (epoch: 23, step: 400) // Avg time/img: 0.0720 s\n",
            "loss: 0.4123 (epoch: 23, step: 450) // Avg time/img: 0.0721 s\n",
            "----- VALIDATING - EPOCH 23 -----\n",
            "VAL loss: 0.3123 (epoch: 23, step: 0) // Avg time/img: 0.0255 s\n",
            "VAL loss: 0.4653 (epoch: 23, step: 50) // Avg time/img: 0.0309 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m36.57\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-023.pth (epoch: 23)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 23)\n",
            "----- TRAINING - EPOCH 24 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.4529 (epoch: 24, step: 0) // Avg time/img: 0.0845 s\n",
            "loss: 0.4149 (epoch: 24, step: 50) // Avg time/img: 0.0727 s\n",
            "loss: 0.4177 (epoch: 24, step: 100) // Avg time/img: 0.0727 s\n",
            "loss: 0.4142 (epoch: 24, step: 150) // Avg time/img: 0.0724 s\n",
            "loss: 0.4108 (epoch: 24, step: 200) // Avg time/img: 0.0723 s\n",
            "loss: 0.4098 (epoch: 24, step: 250) // Avg time/img: 0.0721 s\n",
            "loss: 0.4074 (epoch: 24, step: 300) // Avg time/img: 0.0720 s\n",
            "loss: 0.4046 (epoch: 24, step: 350) // Avg time/img: 0.0719 s\n",
            "loss: 0.4022 (epoch: 24, step: 400) // Avg time/img: 0.0719 s\n",
            "loss: 0.4051 (epoch: 24, step: 450) // Avg time/img: 0.0720 s\n",
            "----- VALIDATING - EPOCH 24 -----\n",
            "VAL loss: 0.3289 (epoch: 24, step: 0) // Avg time/img: 0.0370 s\n",
            "VAL loss: 0.4809 (epoch: 24, step: 50) // Avg time/img: 0.0312 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m36.71\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-024.pth (epoch: 24)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 24)\n",
            "----- TRAINING - EPOCH 25 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.4251 (epoch: 25, step: 0) // Avg time/img: 0.0870 s\n",
            "loss: 0.3943 (epoch: 25, step: 50) // Avg time/img: 0.0709 s\n",
            "loss: 0.397 (epoch: 25, step: 100) // Avg time/img: 0.0712 s\n",
            "loss: 0.3998 (epoch: 25, step: 150) // Avg time/img: 0.0718 s\n",
            "loss: 0.3944 (epoch: 25, step: 200) // Avg time/img: 0.0719 s\n",
            "loss: 0.392 (epoch: 25, step: 250) // Avg time/img: 0.0719 s\n",
            "loss: 0.3963 (epoch: 25, step: 300) // Avg time/img: 0.0719 s\n",
            "loss: 0.401 (epoch: 25, step: 350) // Avg time/img: 0.0719 s\n",
            "loss: 0.4019 (epoch: 25, step: 400) // Avg time/img: 0.0718 s\n",
            "loss: 0.4016 (epoch: 25, step: 450) // Avg time/img: 0.0716 s\n",
            "----- VALIDATING - EPOCH 25 -----\n",
            "VAL loss: 0.3322 (epoch: 25, step: 0) // Avg time/img: 0.0369 s\n",
            "VAL loss: 0.4573 (epoch: 25, step: 50) // Avg time/img: 0.0303 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m37.40\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-025.pth (epoch: 25)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 25)\n",
            "----- TRAINING - EPOCH 26 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.4046 (epoch: 26, step: 0) // Avg time/img: 0.0714 s\n",
            "loss: 0.377 (epoch: 26, step: 50) // Avg time/img: 0.0716 s\n",
            "loss: 0.3927 (epoch: 26, step: 100) // Avg time/img: 0.0720 s\n",
            "loss: 0.3921 (epoch: 26, step: 150) // Avg time/img: 0.0722 s\n",
            "loss: 0.3993 (epoch: 26, step: 200) // Avg time/img: 0.0723 s\n",
            "loss: 0.394 (epoch: 26, step: 250) // Avg time/img: 0.0722 s\n",
            "loss: 0.3962 (epoch: 26, step: 300) // Avg time/img: 0.0721 s\n",
            "loss: 0.3983 (epoch: 26, step: 350) // Avg time/img: 0.0723 s\n",
            "loss: 0.3982 (epoch: 26, step: 400) // Avg time/img: 0.0722 s\n",
            "loss: 0.3985 (epoch: 26, step: 450) // Avg time/img: 0.0723 s\n",
            "----- VALIDATING - EPOCH 26 -----\n",
            "VAL loss: 0.3161 (epoch: 26, step: 0) // Avg time/img: 0.0604 s\n",
            "VAL loss: 0.4701 (epoch: 26, step: 50) // Avg time/img: 0.0315 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m37.55\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-026.pth (epoch: 26)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 26)\n",
            "----- TRAINING - EPOCH 27 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3855 (epoch: 27, step: 0) // Avg time/img: 0.0828 s\n",
            "loss: 0.392 (epoch: 27, step: 50) // Avg time/img: 0.0721 s\n",
            "loss: 0.3956 (epoch: 27, step: 100) // Avg time/img: 0.0720 s\n",
            "loss: 0.3841 (epoch: 27, step: 150) // Avg time/img: 0.0727 s\n",
            "loss: 0.389 (epoch: 27, step: 200) // Avg time/img: 0.0727 s\n",
            "loss: 0.3938 (epoch: 27, step: 250) // Avg time/img: 0.0725 s\n",
            "loss: 0.395 (epoch: 27, step: 300) // Avg time/img: 0.0722 s\n",
            "loss: 0.3969 (epoch: 27, step: 350) // Avg time/img: 0.0720 s\n",
            "loss: 0.3967 (epoch: 27, step: 400) // Avg time/img: 0.0719 s\n",
            "loss: 0.3955 (epoch: 27, step: 450) // Avg time/img: 0.0718 s\n",
            "----- VALIDATING - EPOCH 27 -----\n",
            "VAL loss: 0.3295 (epoch: 27, step: 0) // Avg time/img: 0.0303 s\n",
            "VAL loss: 0.4956 (epoch: 27, step: 50) // Avg time/img: 0.0298 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m36.78\u001b[0m %\n",
            "save: ../save/enet_training_void/model-027.pth (epoch: 27)\n",
            "----- TRAINING - EPOCH 28 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3124 (epoch: 28, step: 0) // Avg time/img: 0.0826 s\n",
            "loss: 0.3827 (epoch: 28, step: 50) // Avg time/img: 0.0727 s\n",
            "loss: 0.3969 (epoch: 28, step: 100) // Avg time/img: 0.0719 s\n",
            "loss: 0.4 (epoch: 28, step: 150) // Avg time/img: 0.0722 s\n",
            "loss: 0.397 (epoch: 28, step: 200) // Avg time/img: 0.0721 s\n",
            "loss: 0.3952 (epoch: 28, step: 250) // Avg time/img: 0.0720 s\n",
            "loss: 0.3959 (epoch: 28, step: 300) // Avg time/img: 0.0718 s\n",
            "loss: 0.3937 (epoch: 28, step: 350) // Avg time/img: 0.0718 s\n",
            "loss: 0.3913 (epoch: 28, step: 400) // Avg time/img: 0.0718 s\n",
            "loss: 0.391 (epoch: 28, step: 450) // Avg time/img: 0.0718 s\n",
            "----- VALIDATING - EPOCH 28 -----\n",
            "VAL loss: 0.3361 (epoch: 28, step: 0) // Avg time/img: 0.0312 s\n",
            "VAL loss: 0.4705 (epoch: 28, step: 50) // Avg time/img: 0.0298 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m38.07\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-028.pth (epoch: 28)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 28)\n",
            "----- TRAINING - EPOCH 29 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.355 (epoch: 29, step: 0) // Avg time/img: 0.0863 s\n",
            "loss: 0.3863 (epoch: 29, step: 50) // Avg time/img: 0.0728 s\n",
            "loss: 0.3733 (epoch: 29, step: 100) // Avg time/img: 0.0727 s\n",
            "loss: 0.3718 (epoch: 29, step: 150) // Avg time/img: 0.0723 s\n",
            "loss: 0.3721 (epoch: 29, step: 200) // Avg time/img: 0.0723 s\n",
            "loss: 0.3725 (epoch: 29, step: 250) // Avg time/img: 0.0724 s\n",
            "loss: 0.3733 (epoch: 29, step: 300) // Avg time/img: 0.0722 s\n",
            "loss: 0.375 (epoch: 29, step: 350) // Avg time/img: 0.0722 s\n",
            "loss: 0.3765 (epoch: 29, step: 400) // Avg time/img: 0.0723 s\n",
            "loss: 0.3822 (epoch: 29, step: 450) // Avg time/img: 0.0721 s\n",
            "----- VALIDATING - EPOCH 29 -----\n",
            "VAL loss: 0.3301 (epoch: 29, step: 0) // Avg time/img: 0.0345 s\n",
            "VAL loss: 0.4737 (epoch: 29, step: 50) // Avg time/img: 0.0306 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m38.85\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-029.pth (epoch: 29)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 29)\n",
            "----- TRAINING - EPOCH 30 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.6984 (epoch: 30, step: 0) // Avg time/img: 0.0848 s\n",
            "loss: 0.3656 (epoch: 30, step: 50) // Avg time/img: 0.0715 s\n",
            "loss: 0.375 (epoch: 30, step: 100) // Avg time/img: 0.0720 s\n",
            "loss: 0.3798 (epoch: 30, step: 150) // Avg time/img: 0.0728 s\n",
            "loss: 0.3811 (epoch: 30, step: 200) // Avg time/img: 0.0726 s\n",
            "loss: 0.3784 (epoch: 30, step: 250) // Avg time/img: 0.0725 s\n",
            "loss: 0.3813 (epoch: 30, step: 300) // Avg time/img: 0.0725 s\n",
            "loss: 0.3823 (epoch: 30, step: 350) // Avg time/img: 0.0725 s\n",
            "loss: 0.3799 (epoch: 30, step: 400) // Avg time/img: 0.0725 s\n",
            "loss: 0.38 (epoch: 30, step: 450) // Avg time/img: 0.0723 s\n",
            "----- VALIDATING - EPOCH 30 -----\n",
            "VAL loss: 0.3111 (epoch: 30, step: 0) // Avg time/img: 0.0250 s\n",
            "VAL loss: 0.4485 (epoch: 30, step: 50) // Avg time/img: 0.0315 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m39.83\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-030.pth (epoch: 30)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 30)\n",
            "----- TRAINING - EPOCH 31 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3481 (epoch: 31, step: 0) // Avg time/img: 0.0918 s\n",
            "loss: 0.3703 (epoch: 31, step: 50) // Avg time/img: 0.0722 s\n",
            "loss: 0.3793 (epoch: 31, step: 100) // Avg time/img: 0.0728 s\n",
            "loss: 0.3763 (epoch: 31, step: 150) // Avg time/img: 0.0722 s\n",
            "loss: 0.3757 (epoch: 31, step: 200) // Avg time/img: 0.0722 s\n",
            "loss: 0.3762 (epoch: 31, step: 250) // Avg time/img: 0.0721 s\n",
            "loss: 0.3766 (epoch: 31, step: 300) // Avg time/img: 0.0721 s\n",
            "loss: 0.3754 (epoch: 31, step: 350) // Avg time/img: 0.0722 s\n",
            "loss: 0.3778 (epoch: 31, step: 400) // Avg time/img: 0.0722 s\n",
            "loss: 0.3784 (epoch: 31, step: 450) // Avg time/img: 0.0723 s\n",
            "----- VALIDATING - EPOCH 31 -----\n",
            "VAL loss: 0.3519 (epoch: 31, step: 0) // Avg time/img: 0.0375 s\n",
            "VAL loss: 0.5001 (epoch: 31, step: 50) // Avg time/img: 0.0257 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m38.05\u001b[0m %\n",
            "save: ../save/enet_training_void/model-031.pth (epoch: 31)\n",
            "----- TRAINING - EPOCH 32 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.2625 (epoch: 32, step: 0) // Avg time/img: 0.0806 s\n",
            "loss: 0.4288 (epoch: 32, step: 50) // Avg time/img: 0.0683 s\n",
            "loss: 0.4017 (epoch: 32, step: 100) // Avg time/img: 0.0680 s\n",
            "loss: 0.4034 (epoch: 32, step: 150) // Avg time/img: 0.0672 s\n",
            "loss: 0.4024 (epoch: 32, step: 200) // Avg time/img: 0.0672 s\n",
            "loss: 0.3949 (epoch: 32, step: 250) // Avg time/img: 0.0671 s\n",
            "loss: 0.3914 (epoch: 32, step: 300) // Avg time/img: 0.0672 s\n",
            "loss: 0.39 (epoch: 32, step: 350) // Avg time/img: 0.0671 s\n",
            "loss: 0.3893 (epoch: 32, step: 400) // Avg time/img: 0.0671 s\n",
            "loss: 0.3868 (epoch: 32, step: 450) // Avg time/img: 0.0670 s\n",
            "----- VALIDATING - EPOCH 32 -----\n",
            "VAL loss: 0.4067 (epoch: 32, step: 0) // Avg time/img: 0.0266 s\n",
            "VAL loss: 0.4844 (epoch: 32, step: 50) // Avg time/img: 0.0244 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m38.83\u001b[0m %\n",
            "save: ../save/enet_training_void/model-032.pth (epoch: 32)\n",
            "----- TRAINING - EPOCH 33 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.2753 (epoch: 33, step: 0) // Avg time/img: 0.0748 s\n",
            "loss: 0.3614 (epoch: 33, step: 50) // Avg time/img: 0.0674 s\n",
            "loss: 0.3722 (epoch: 33, step: 100) // Avg time/img: 0.0674 s\n",
            "loss: 0.3726 (epoch: 33, step: 150) // Avg time/img: 0.0674 s\n",
            "loss: 0.375 (epoch: 33, step: 200) // Avg time/img: 0.0672 s\n",
            "loss: 0.3724 (epoch: 33, step: 250) // Avg time/img: 0.0675 s\n",
            "loss: 0.3698 (epoch: 33, step: 300) // Avg time/img: 0.0675 s\n",
            "loss: 0.3695 (epoch: 33, step: 350) // Avg time/img: 0.0675 s\n",
            "loss: 0.3687 (epoch: 33, step: 400) // Avg time/img: 0.0675 s\n",
            "loss: 0.3702 (epoch: 33, step: 450) // Avg time/img: 0.0674 s\n",
            "----- VALIDATING - EPOCH 33 -----\n",
            "VAL loss: 0.3544 (epoch: 33, step: 0) // Avg time/img: 0.0301 s\n",
            "VAL loss: 0.4622 (epoch: 33, step: 50) // Avg time/img: 0.0248 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m40.88\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-033.pth (epoch: 33)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 33)\n",
            "----- TRAINING - EPOCH 34 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.318 (epoch: 34, step: 0) // Avg time/img: 0.0838 s\n",
            "loss: 0.3584 (epoch: 34, step: 50) // Avg time/img: 0.0679 s\n",
            "loss: 0.3519 (epoch: 34, step: 100) // Avg time/img: 0.0679 s\n",
            "loss: 0.3644 (epoch: 34, step: 150) // Avg time/img: 0.0679 s\n",
            "loss: 0.3664 (epoch: 34, step: 200) // Avg time/img: 0.0679 s\n",
            "loss: 0.3647 (epoch: 34, step: 250) // Avg time/img: 0.0680 s\n",
            "loss: 0.3647 (epoch: 34, step: 300) // Avg time/img: 0.0679 s\n",
            "loss: 0.3647 (epoch: 34, step: 350) // Avg time/img: 0.0677 s\n",
            "loss: 0.3667 (epoch: 34, step: 400) // Avg time/img: 0.0676 s\n",
            "loss: 0.3706 (epoch: 34, step: 450) // Avg time/img: 0.0675 s\n",
            "----- VALIDATING - EPOCH 34 -----\n",
            "VAL loss: 0.32 (epoch: 34, step: 0) // Avg time/img: 0.0333 s\n",
            "VAL loss: 0.4406 (epoch: 34, step: 50) // Avg time/img: 0.0244 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m40.08\u001b[0m %\n",
            "save: ../save/enet_training_void/model-034.pth (epoch: 34)\n",
            "----- TRAINING - EPOCH 35 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3159 (epoch: 35, step: 0) // Avg time/img: 0.0730 s\n",
            "loss: 0.3647 (epoch: 35, step: 50) // Avg time/img: 0.0678 s\n",
            "loss: 0.3619 (epoch: 35, step: 100) // Avg time/img: 0.0677 s\n",
            "loss: 0.3641 (epoch: 35, step: 150) // Avg time/img: 0.0674 s\n",
            "loss: 0.3689 (epoch: 35, step: 200) // Avg time/img: 0.0673 s\n",
            "loss: 0.3661 (epoch: 35, step: 250) // Avg time/img: 0.0671 s\n",
            "loss: 0.3641 (epoch: 35, step: 300) // Avg time/img: 0.0672 s\n",
            "loss: 0.3656 (epoch: 35, step: 350) // Avg time/img: 0.0670 s\n",
            "loss: 0.3676 (epoch: 35, step: 400) // Avg time/img: 0.0670 s\n",
            "loss: 0.3667 (epoch: 35, step: 450) // Avg time/img: 0.0671 s\n",
            "----- VALIDATING - EPOCH 35 -----\n",
            "VAL loss: 0.3293 (epoch: 35, step: 0) // Avg time/img: 0.0352 s\n",
            "VAL loss: 0.4526 (epoch: 35, step: 50) // Avg time/img: 0.0254 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m39.59\u001b[0m %\n",
            "save: ../save/enet_training_void/model-035.pth (epoch: 35)\n",
            "----- TRAINING - EPOCH 36 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.2971 (epoch: 36, step: 0) // Avg time/img: 0.0792 s\n",
            "loss: 0.3688 (epoch: 36, step: 50) // Avg time/img: 0.0685 s\n",
            "loss: 0.3687 (epoch: 36, step: 100) // Avg time/img: 0.0674 s\n",
            "loss: 0.3689 (epoch: 36, step: 150) // Avg time/img: 0.0675 s\n",
            "loss: 0.3628 (epoch: 36, step: 200) // Avg time/img: 0.0673 s\n",
            "loss: 0.3673 (epoch: 36, step: 250) // Avg time/img: 0.0673 s\n",
            "loss: 0.3663 (epoch: 36, step: 300) // Avg time/img: 0.0674 s\n",
            "loss: 0.3643 (epoch: 36, step: 350) // Avg time/img: 0.0675 s\n",
            "loss: 0.3654 (epoch: 36, step: 400) // Avg time/img: 0.0675 s\n",
            "loss: 0.3662 (epoch: 36, step: 450) // Avg time/img: 0.0674 s\n",
            "----- VALIDATING - EPOCH 36 -----\n",
            "VAL loss: 0.3397 (epoch: 36, step: 0) // Avg time/img: 0.0229 s\n",
            "VAL loss: 0.4602 (epoch: 36, step: 50) // Avg time/img: 0.0244 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m39.53\u001b[0m %\n",
            "save: ../save/enet_training_void/model-036.pth (epoch: 36)\n",
            "----- TRAINING - EPOCH 37 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3345 (epoch: 37, step: 0) // Avg time/img: 0.0713 s\n",
            "loss: 0.3859 (epoch: 37, step: 50) // Avg time/img: 0.0677 s\n",
            "loss: 0.3715 (epoch: 37, step: 100) // Avg time/img: 0.0678 s\n",
            "loss: 0.3713 (epoch: 37, step: 150) // Avg time/img: 0.0674 s\n",
            "loss: 0.3708 (epoch: 37, step: 200) // Avg time/img: 0.0676 s\n",
            "loss: 0.3724 (epoch: 37, step: 250) // Avg time/img: 0.0674 s\n",
            "loss: 0.3724 (epoch: 37, step: 300) // Avg time/img: 0.0675 s\n",
            "loss: 0.372 (epoch: 37, step: 350) // Avg time/img: 0.0673 s\n",
            "loss: 0.372 (epoch: 37, step: 400) // Avg time/img: 0.0672 s\n",
            "loss: 0.3702 (epoch: 37, step: 450) // Avg time/img: 0.0672 s\n",
            "----- VALIDATING - EPOCH 37 -----\n",
            "VAL loss: 0.2992 (epoch: 37, step: 0) // Avg time/img: 0.0458 s\n",
            "VAL loss: 0.4488 (epoch: 37, step: 50) // Avg time/img: 0.0244 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m40.14\u001b[0m %\n",
            "save: ../save/enet_training_void/model-037.pth (epoch: 37)\n",
            "----- TRAINING - EPOCH 38 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.2458 (epoch: 38, step: 0) // Avg time/img: 0.0948 s\n",
            "loss: 0.3612 (epoch: 38, step: 50) // Avg time/img: 0.0684 s\n",
            "loss: 0.3655 (epoch: 38, step: 100) // Avg time/img: 0.0675 s\n",
            "loss: 0.3636 (epoch: 38, step: 150) // Avg time/img: 0.0672 s\n",
            "loss: 0.3604 (epoch: 38, step: 200) // Avg time/img: 0.0671 s\n",
            "loss: 0.3611 (epoch: 38, step: 250) // Avg time/img: 0.0670 s\n",
            "loss: 0.364 (epoch: 38, step: 300) // Avg time/img: 0.0671 s\n",
            "loss: 0.3652 (epoch: 38, step: 350) // Avg time/img: 0.0671 s\n",
            "loss: 0.365 (epoch: 38, step: 400) // Avg time/img: 0.0671 s\n",
            "loss: 0.3657 (epoch: 38, step: 450) // Avg time/img: 0.0671 s\n",
            "----- VALIDATING - EPOCH 38 -----\n",
            "VAL loss: 0.3224 (epoch: 38, step: 0) // Avg time/img: 0.0329 s\n",
            "VAL loss: 0.4393 (epoch: 38, step: 50) // Avg time/img: 0.0299 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m41.34\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void/model-038.pth (epoch: 38)\n",
            "save: ../save/enet_training_void/model_best.pth (epoch: 38)\n",
            "----- TRAINING - EPOCH 39 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.494 (epoch: 39, step: 0) // Avg time/img: 0.0884 s\n",
            "loss: 0.3591 (epoch: 39, step: 50) // Avg time/img: 0.0717 s\n",
            "loss: 0.3517 (epoch: 39, step: 100) // Avg time/img: 0.0718 s\n",
            "loss: 0.3594 (epoch: 39, step: 150) // Avg time/img: 0.0720 s\n",
            "loss: 0.3607 (epoch: 39, step: 200) // Avg time/img: 0.0718 s\n",
            "loss: 0.3627 (epoch: 39, step: 250) // Avg time/img: 0.0717 s\n",
            "loss: 0.3603 (epoch: 39, step: 300) // Avg time/img: 0.0719 s\n",
            "loss: 0.3583 (epoch: 39, step: 350) // Avg time/img: 0.0718 s\n",
            "loss: 0.3575 (epoch: 39, step: 400) // Avg time/img: 0.0717 s\n",
            "loss: 0.3571 (epoch: 39, step: 450) // Avg time/img: 0.0717 s\n",
            "----- VALIDATING - EPOCH 39 -----\n",
            "VAL loss: 0.3167 (epoch: 39, step: 0) // Avg time/img: 0.0385 s\n",
            "VAL loss: 0.4806 (epoch: 39, step: 50) // Avg time/img: 0.0285 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m41.00\u001b[0m %\n",
            "save: ../save/enet_training_void/model-039.pth (epoch: 39)\n",
            "----- TRAINING - EPOCH 40 -----\n",
            "LEARNING RATE:  0.0005\n",
            "loss: 0.3439 (epoch: 40, step: 0) // Avg time/img: 0.0883 s\n",
            "loss: 0.3487 (epoch: 40, step: 50) // Avg time/img: 0.0738 s\n",
            "loss: 0.3554 (epoch: 40, step: 100) // Avg time/img: 0.0728 s\n",
            "loss: 0.3506 (epoch: 40, step: 150) // Avg time/img: 0.0723 s\n",
            "loss: 0.3498 (epoch: 40, step: 200) // Avg time/img: 0.0722 s\n",
            "loss: 0.3551 (epoch: 40, step: 250) // Avg time/img: 0.0720 s\n",
            "loss: 0.3588 (epoch: 40, step: 300) // Avg time/img: 0.0721 s\n",
            "loss: 0.3587 (epoch: 40, step: 350) // Avg time/img: 0.0720 s\n",
            "loss: 0.3578 (epoch: 40, step: 400) // Avg time/img: 0.0719 s\n",
            "loss: 0.3597 (epoch: 40, step: 450) // Avg time/img: 0.0721 s\n",
            "----- VALIDATING - EPOCH 40 -----\n",
            "VAL loss: 0.3289 (epoch: 40, step: 0) // Avg time/img: 0.0240 s\n",
            "VAL loss: 0.4907 (epoch: 40, step: 50) // Avg time/img: 0.0294 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m39.79\u001b[0m %\n",
            "save: ../save/enet_training_void/model-040.pth (epoch: 40)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Enet Fine-Tuning"
      ],
      "metadata": {
        "id": "V74DjxFWfAAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(\"enet\", num_epochs=20, batch_size=6, fineTune=True)\n",
        "# %cd ../save\n",
        "# !zip -r enet_training_void_ft.zip enet_training_void_ft/"
      ],
      "metadata": {
        "id": "CAiuCPvUg0Pt",
        "outputId": "912baaf3-bec9-4d9d-e3a4-0865f8fef6b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Import Model enet with weights ../trained_models/enet_pretrained.pth to FineTune\n",
            "========== TRAINING ===========\n",
            "../cityscapes/leftImg8bit/train\n",
            "../cityscapes/leftImg8bit/val\n",
            "Criterion: CrossEntropyLoss2d\n",
            "----- TRAINING - EPOCH 1 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 10.75 (epoch: 1, step: 0) // Avg time/img: 0.3722 s\n",
            "loss: 10.32 (epoch: 1, step: 50) // Avg time/img: 0.0422 s\n",
            "loss: 10.23 (epoch: 1, step: 100) // Avg time/img: 0.0380 s\n",
            "loss: 10.13 (epoch: 1, step: 150) // Avg time/img: 0.0369 s\n",
            "loss: 10.05 (epoch: 1, step: 200) // Avg time/img: 0.0360 s\n",
            "loss: 9.928 (epoch: 1, step: 250) // Avg time/img: 0.0355 s\n",
            "loss: 9.824 (epoch: 1, step: 300) // Avg time/img: 0.0357 s\n",
            "loss: 9.703 (epoch: 1, step: 350) // Avg time/img: 0.0355 s\n",
            "loss: 9.593 (epoch: 1, step: 400) // Avg time/img: 0.0355 s\n",
            "loss: 9.481 (epoch: 1, step: 450) // Avg time/img: 0.0353 s\n",
            "----- VALIDATING - EPOCH 1 -----\n",
            "VAL loss: 8.499 (epoch: 1, step: 0) // Avg time/img: 0.0389 s\n",
            "VAL loss: 8.385 (epoch: 1, step: 50) // Avg time/img: 0.0313 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m1.21\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-001.pth (epoch: 1)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 1)\n",
            "----- TRAINING - EPOCH 2 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 7.633 (epoch: 2, step: 0) // Avg time/img: 0.0331 s\n",
            "loss: 8.211 (epoch: 2, step: 50) // Avg time/img: 0.0348 s\n",
            "loss: 8.09 (epoch: 2, step: 100) // Avg time/img: 0.0356 s\n",
            "loss: 7.999 (epoch: 2, step: 150) // Avg time/img: 0.0339 s\n",
            "loss: 7.904 (epoch: 2, step: 200) // Avg time/img: 0.0340 s\n",
            "loss: 7.787 (epoch: 2, step: 250) // Avg time/img: 0.0334 s\n",
            "loss: 7.686 (epoch: 2, step: 300) // Avg time/img: 0.0332 s\n",
            "loss: 7.596 (epoch: 2, step: 350) // Avg time/img: 0.0329 s\n",
            "loss: 7.496 (epoch: 2, step: 400) // Avg time/img: 0.0332 s\n",
            "loss: 7.396 (epoch: 2, step: 450) // Avg time/img: 0.0332 s\n",
            "----- VALIDATING - EPOCH 2 -----\n",
            "VAL loss: 6.324 (epoch: 2, step: 0) // Avg time/img: 0.0383 s\n",
            "VAL loss: 6.406 (epoch: 2, step: 50) // Avg time/img: 0.0297 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m2.41\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-002.pth (epoch: 2)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 2)\n",
            "----- TRAINING - EPOCH 3 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 6.596 (epoch: 3, step: 0) // Avg time/img: 0.0374 s\n",
            "loss: 6.303 (epoch: 3, step: 50) // Avg time/img: 0.0312 s\n",
            "loss: 6.183 (epoch: 3, step: 100) // Avg time/img: 0.0322 s\n",
            "loss: 6.114 (epoch: 3, step: 150) // Avg time/img: 0.0330 s\n",
            "loss: 6.035 (epoch: 3, step: 200) // Avg time/img: 0.0325 s\n",
            "loss: 5.959 (epoch: 3, step: 250) // Avg time/img: 0.0328 s\n",
            "loss: 5.869 (epoch: 3, step: 300) // Avg time/img: 0.0327 s\n",
            "loss: 5.796 (epoch: 3, step: 350) // Avg time/img: 0.0323 s\n",
            "loss: 5.726 (epoch: 3, step: 400) // Avg time/img: 0.0324 s\n",
            "loss: 5.653 (epoch: 3, step: 450) // Avg time/img: 0.0327 s\n",
            "----- VALIDATING - EPOCH 3 -----\n",
            "VAL loss: 4.686 (epoch: 3, step: 0) // Avg time/img: 0.0267 s\n",
            "VAL loss: 4.878 (epoch: 3, step: 50) // Avg time/img: 0.0304 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m4.54\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-003.pth (epoch: 3)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 3)\n",
            "----- TRAINING - EPOCH 4 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 4.609 (epoch: 4, step: 0) // Avg time/img: 0.0417 s\n",
            "loss: 4.846 (epoch: 4, step: 50) // Avg time/img: 0.0360 s\n",
            "loss: 4.77 (epoch: 4, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 4.709 (epoch: 4, step: 150) // Avg time/img: 0.0346 s\n",
            "loss: 4.64 (epoch: 4, step: 200) // Avg time/img: 0.0336 s\n",
            "loss: 4.594 (epoch: 4, step: 250) // Avg time/img: 0.0332 s\n",
            "loss: 4.538 (epoch: 4, step: 300) // Avg time/img: 0.0326 s\n",
            "loss: 4.492 (epoch: 4, step: 350) // Avg time/img: 0.0329 s\n",
            "loss: 4.44 (epoch: 4, step: 400) // Avg time/img: 0.0329 s\n",
            "loss: 4.382 (epoch: 4, step: 450) // Avg time/img: 0.0327 s\n",
            "----- VALIDATING - EPOCH 4 -----\n",
            "VAL loss: 3.601 (epoch: 4, step: 0) // Avg time/img: 0.0357 s\n",
            "VAL loss: 3.85 (epoch: 4, step: 50) // Avg time/img: 0.0284 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m8.58\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-004.pth (epoch: 4)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 4)\n",
            "----- TRAINING - EPOCH 5 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 3.98 (epoch: 5, step: 0) // Avg time/img: 0.0402 s\n",
            "loss: 3.773 (epoch: 5, step: 50) // Avg time/img: 0.0367 s\n",
            "loss: 3.754 (epoch: 5, step: 100) // Avg time/img: 0.0353 s\n",
            "loss: 3.73 (epoch: 5, step: 150) // Avg time/img: 0.0348 s\n",
            "loss: 3.682 (epoch: 5, step: 200) // Avg time/img: 0.0348 s\n",
            "loss: 3.65 (epoch: 5, step: 250) // Avg time/img: 0.0345 s\n",
            "loss: 3.613 (epoch: 5, step: 300) // Avg time/img: 0.0341 s\n",
            "loss: 3.565 (epoch: 5, step: 350) // Avg time/img: 0.0335 s\n",
            "loss: 3.522 (epoch: 5, step: 400) // Avg time/img: 0.0338 s\n",
            "loss: 3.477 (epoch: 5, step: 450) // Avg time/img: 0.0338 s\n",
            "----- VALIDATING - EPOCH 5 -----\n",
            "VAL loss: 2.736 (epoch: 5, step: 0) // Avg time/img: 0.0304 s\n",
            "VAL loss: 3.04 (epoch: 5, step: 50) // Avg time/img: 0.0315 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m10.81\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-005.pth (epoch: 5)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 5)\n",
            "----- TRAINING - EPOCH 6 -----\n",
            "LEARNING RATE:  5e-05\n",
            "loss: 2.778 (epoch: 6, step: 0) // Avg time/img: 0.0446 s\n",
            "loss: 3.031 (epoch: 6, step: 50) // Avg time/img: 0.0369 s\n",
            "loss: 2.999 (epoch: 6, step: 100) // Avg time/img: 0.0360 s\n",
            "loss: 2.973 (epoch: 6, step: 150) // Avg time/img: 0.0357 s\n",
            "loss: 2.94 (epoch: 6, step: 200) // Avg time/img: 0.0355 s\n",
            "loss: 2.898 (epoch: 6, step: 250) // Avg time/img: 0.0353 s\n",
            "loss: 2.867 (epoch: 6, step: 300) // Avg time/img: 0.0353 s\n",
            "loss: 2.837 (epoch: 6, step: 350) // Avg time/img: 0.0350 s\n",
            "loss: 2.802 (epoch: 6, step: 400) // Avg time/img: 0.0343 s\n",
            "loss: 2.774 (epoch: 6, step: 450) // Avg time/img: 0.0345 s\n",
            "----- VALIDATING - EPOCH 6 -----\n",
            "VAL loss: 2.086 (epoch: 6, step: 0) // Avg time/img: 0.0340 s\n",
            "VAL loss: 2.413 (epoch: 6, step: 50) // Avg time/img: 0.0265 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m14.18\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-006.pth (epoch: 6)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 6)\n",
            "----- TRAINING - EPOCH 7 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 2.637 (epoch: 7, step: 0) // Avg time/img: 0.0405 s\n",
            "loss: 2.507 (epoch: 7, step: 50) // Avg time/img: 0.0345 s\n",
            "loss: 2.474 (epoch: 7, step: 100) // Avg time/img: 0.0329 s\n",
            "loss: 2.477 (epoch: 7, step: 150) // Avg time/img: 0.0318 s\n",
            "loss: 2.471 (epoch: 7, step: 200) // Avg time/img: 0.0315 s\n",
            "loss: 2.47 (epoch: 7, step: 250) // Avg time/img: 0.0311 s\n",
            "loss: 2.47 (epoch: 7, step: 300) // Avg time/img: 0.0309 s\n",
            "loss: 2.461 (epoch: 7, step: 350) // Avg time/img: 0.0311 s\n",
            "loss: 2.456 (epoch: 7, step: 400) // Avg time/img: 0.0309 s\n",
            "loss: 2.458 (epoch: 7, step: 450) // Avg time/img: 0.0309 s\n",
            "----- VALIDATING - EPOCH 7 -----\n",
            "VAL loss: 2.057 (epoch: 7, step: 0) // Avg time/img: 0.0353 s\n",
            "VAL loss: 2.378 (epoch: 7, step: 50) // Avg time/img: 0.0271 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m14.45\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-007.pth (epoch: 7)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 7)\n",
            "----- TRAINING - EPOCH 8 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 2.809 (epoch: 8, step: 0) // Avg time/img: 0.0427 s\n",
            "loss: 2.444 (epoch: 8, step: 50) // Avg time/img: 0.0350 s\n",
            "loss: 2.422 (epoch: 8, step: 100) // Avg time/img: 0.0334 s\n",
            "loss: 2.405 (epoch: 8, step: 150) // Avg time/img: 0.0324 s\n",
            "loss: 2.407 (epoch: 8, step: 200) // Avg time/img: 0.0323 s\n",
            "loss: 2.406 (epoch: 8, step: 250) // Avg time/img: 0.0319 s\n",
            "loss: 2.404 (epoch: 8, step: 300) // Avg time/img: 0.0315 s\n",
            "loss: 2.4 (epoch: 8, step: 350) // Avg time/img: 0.0320 s\n",
            "loss: 2.395 (epoch: 8, step: 400) // Avg time/img: 0.0319 s\n",
            "loss: 2.4 (epoch: 8, step: 450) // Avg time/img: 0.0317 s\n",
            "----- VALIDATING - EPOCH 8 -----\n",
            "VAL loss: 2.053 (epoch: 8, step: 0) // Avg time/img: 0.0311 s\n",
            "VAL loss: 2.355 (epoch: 8, step: 50) // Avg time/img: 0.0271 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m14.78\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-008.pth (epoch: 8)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 8)\n",
            "----- TRAINING - EPOCH 9 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 2.733 (epoch: 9, step: 0) // Avg time/img: 0.0452 s\n",
            "loss: 2.416 (epoch: 9, step: 50) // Avg time/img: 0.0354 s\n",
            "loss: 2.378 (epoch: 9, step: 100) // Avg time/img: 0.0351 s\n",
            "loss: 2.365 (epoch: 9, step: 150) // Avg time/img: 0.0335 s\n",
            "loss: 2.363 (epoch: 9, step: 200) // Avg time/img: 0.0338 s\n",
            "loss: 2.363 (epoch: 9, step: 250) // Avg time/img: 0.0332 s\n",
            "loss: 2.36 (epoch: 9, step: 300) // Avg time/img: 0.0332 s\n",
            "loss: 2.362 (epoch: 9, step: 350) // Avg time/img: 0.0329 s\n",
            "loss: 2.356 (epoch: 9, step: 400) // Avg time/img: 0.0332 s\n",
            "loss: 2.357 (epoch: 9, step: 450) // Avg time/img: 0.0328 s\n",
            "----- VALIDATING - EPOCH 9 -----\n",
            "VAL loss: 1.935 (epoch: 9, step: 0) // Avg time/img: 0.0518 s\n",
            "VAL loss: 2.263 (epoch: 9, step: 50) // Avg time/img: 0.0291 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m15.37\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-009.pth (epoch: 9)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 9)\n",
            "----- TRAINING - EPOCH 10 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 2.392 (epoch: 10, step: 0) // Avg time/img: 0.0384 s\n",
            "loss: 2.286 (epoch: 10, step: 50) // Avg time/img: 0.0310 s\n",
            "loss: 2.308 (epoch: 10, step: 100) // Avg time/img: 0.0305 s\n",
            "loss: 2.314 (epoch: 10, step: 150) // Avg time/img: 0.0317 s\n",
            "loss: 2.308 (epoch: 10, step: 200) // Avg time/img: 0.0323 s\n",
            "loss: 2.315 (epoch: 10, step: 250) // Avg time/img: 0.0319 s\n",
            "loss: 2.311 (epoch: 10, step: 300) // Avg time/img: 0.0319 s\n",
            "loss: 2.314 (epoch: 10, step: 350) // Avg time/img: 0.0320 s\n",
            "loss: 2.307 (epoch: 10, step: 400) // Avg time/img: 0.0323 s\n",
            "loss: 2.3 (epoch: 10, step: 450) // Avg time/img: 0.0322 s\n",
            "----- VALIDATING - EPOCH 10 -----\n",
            "VAL loss: 1.915 (epoch: 10, step: 0) // Avg time/img: 0.0325 s\n",
            "VAL loss: 2.241 (epoch: 10, step: 50) // Avg time/img: 0.0264 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m16.23\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-010.pth (epoch: 10)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 10)\n",
            "----- TRAINING - EPOCH 11 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 1.832 (epoch: 11, step: 0) // Avg time/img: 0.0310 s\n",
            "loss: 2.209 (epoch: 11, step: 50) // Avg time/img: 0.0349 s\n",
            "loss: 2.239 (epoch: 11, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 2.251 (epoch: 11, step: 150) // Avg time/img: 0.0354 s\n",
            "loss: 2.247 (epoch: 11, step: 200) // Avg time/img: 0.0352 s\n",
            "loss: 2.262 (epoch: 11, step: 250) // Avg time/img: 0.0354 s\n",
            "loss: 2.259 (epoch: 11, step: 300) // Avg time/img: 0.0347 s\n",
            "loss: 2.259 (epoch: 11, step: 350) // Avg time/img: 0.0347 s\n",
            "loss: 2.258 (epoch: 11, step: 400) // Avg time/img: 0.0348 s\n",
            "loss: 2.255 (epoch: 11, step: 450) // Avg time/img: 0.0346 s\n",
            "----- VALIDATING - EPOCH 11 -----\n",
            "VAL loss: 1.879 (epoch: 11, step: 0) // Avg time/img: 0.0258 s\n",
            "VAL loss: 2.19 (epoch: 11, step: 50) // Avg time/img: 0.0298 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m16.67\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-011.pth (epoch: 11)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 11)\n",
            "----- TRAINING - EPOCH 12 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 2.071 (epoch: 12, step: 0) // Avg time/img: 0.0349 s\n",
            "loss: 2.203 (epoch: 12, step: 50) // Avg time/img: 0.0363 s\n",
            "loss: 2.205 (epoch: 12, step: 100) // Avg time/img: 0.0362 s\n",
            "loss: 2.208 (epoch: 12, step: 150) // Avg time/img: 0.0345 s\n",
            "loss: 2.209 (epoch: 12, step: 200) // Avg time/img: 0.0343 s\n",
            "loss: 2.211 (epoch: 12, step: 250) // Avg time/img: 0.0344 s\n",
            "loss: 2.205 (epoch: 12, step: 300) // Avg time/img: 0.0339 s\n",
            "loss: 2.201 (epoch: 12, step: 350) // Avg time/img: 0.0338 s\n",
            "loss: 2.197 (epoch: 12, step: 400) // Avg time/img: 0.0332 s\n",
            "loss: 2.199 (epoch: 12, step: 450) // Avg time/img: 0.0331 s\n",
            "----- VALIDATING - EPOCH 12 -----\n",
            "VAL loss: 1.833 (epoch: 12, step: 0) // Avg time/img: 0.0324 s\n",
            "VAL loss: 2.154 (epoch: 12, step: 50) // Avg time/img: 0.0288 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m18.19\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-012.pth (epoch: 12)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 12)\n",
            "----- TRAINING - EPOCH 13 -----\n",
            "LEARNING RATE:  5e-06\n",
            "loss: 2.076 (epoch: 13, step: 0) // Avg time/img: 0.0601 s\n",
            "loss: 2.252 (epoch: 13, step: 50) // Avg time/img: 0.0351 s\n",
            "loss: 2.211 (epoch: 13, step: 100) // Avg time/img: 0.0352 s\n",
            "loss: 2.2 (epoch: 13, step: 150) // Avg time/img: 0.0351 s\n",
            "loss: 2.205 (epoch: 13, step: 200) // Avg time/img: 0.0353 s\n",
            "loss: 2.19 (epoch: 13, step: 250) // Avg time/img: 0.0350 s\n",
            "loss: 2.185 (epoch: 13, step: 300) // Avg time/img: 0.0343 s\n",
            "loss: 2.177 (epoch: 13, step: 350) // Avg time/img: 0.0339 s\n",
            "loss: 2.171 (epoch: 13, step: 400) // Avg time/img: 0.0336 s\n",
            "loss: 2.165 (epoch: 13, step: 450) // Avg time/img: 0.0331 s\n",
            "----- VALIDATING - EPOCH 13 -----\n",
            "VAL loss: 1.786 (epoch: 13, step: 0) // Avg time/img: 0.0228 s\n",
            "VAL loss: 2.109 (epoch: 13, step: 50) // Avg time/img: 0.0258 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.13\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-013.pth (epoch: 13)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 13)\n",
            "----- TRAINING - EPOCH 14 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 2.103 (epoch: 14, step: 0) // Avg time/img: 0.0449 s\n",
            "loss: 2.133 (epoch: 14, step: 50) // Avg time/img: 0.0354 s\n",
            "loss: 2.145 (epoch: 14, step: 100) // Avg time/img: 0.0347 s\n",
            "loss: 2.142 (epoch: 14, step: 150) // Avg time/img: 0.0347 s\n",
            "loss: 2.149 (epoch: 14, step: 200) // Avg time/img: 0.0346 s\n",
            "loss: 2.148 (epoch: 14, step: 250) // Avg time/img: 0.0346 s\n",
            "loss: 2.151 (epoch: 14, step: 300) // Avg time/img: 0.0346 s\n",
            "loss: 2.153 (epoch: 14, step: 350) // Avg time/img: 0.0346 s\n",
            "loss: 2.146 (epoch: 14, step: 400) // Avg time/img: 0.0346 s\n",
            "loss: 2.145 (epoch: 14, step: 450) // Avg time/img: 0.0346 s\n",
            "----- VALIDATING - EPOCH 14 -----\n",
            "VAL loss: 1.765 (epoch: 14, step: 0) // Avg time/img: 0.0355 s\n",
            "VAL loss: 2.081 (epoch: 14, step: 50) // Avg time/img: 0.0302 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.42\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-014.pth (epoch: 14)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 14)\n",
            "----- TRAINING - EPOCH 15 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 2.336 (epoch: 15, step: 0) // Avg time/img: 0.0387 s\n",
            "loss: 2.113 (epoch: 15, step: 50) // Avg time/img: 0.0337 s\n",
            "loss: 2.124 (epoch: 15, step: 100) // Avg time/img: 0.0337 s\n",
            "loss: 2.119 (epoch: 15, step: 150) // Avg time/img: 0.0327 s\n",
            "loss: 2.118 (epoch: 15, step: 200) // Avg time/img: 0.0321 s\n",
            "loss: 2.111 (epoch: 15, step: 250) // Avg time/img: 0.0320 s\n",
            "loss: 2.124 (epoch: 15, step: 300) // Avg time/img: 0.0324 s\n",
            "loss: 2.125 (epoch: 15, step: 350) // Avg time/img: 0.0328 s\n",
            "loss: 2.131 (epoch: 15, step: 400) // Avg time/img: 0.0324 s\n",
            "loss: 2.13 (epoch: 15, step: 450) // Avg time/img: 0.0325 s\n",
            "----- VALIDATING - EPOCH 15 -----\n",
            "VAL loss: 1.769 (epoch: 15, step: 0) // Avg time/img: 0.0351 s\n",
            "VAL loss: 2.108 (epoch: 15, step: 50) // Avg time/img: 0.0288 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.40\u001b[0m %\n",
            "save: ../save/enet_training_void_ft/model-015.pth (epoch: 15)\n",
            "----- TRAINING - EPOCH 16 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 2.119 (epoch: 16, step: 0) // Avg time/img: 0.0460 s\n",
            "loss: 2.131 (epoch: 16, step: 50) // Avg time/img: 0.0325 s\n",
            "loss: 2.122 (epoch: 16, step: 100) // Avg time/img: 0.0316 s\n",
            "loss: 2.107 (epoch: 16, step: 150) // Avg time/img: 0.0313 s\n",
            "loss: 2.113 (epoch: 16, step: 200) // Avg time/img: 0.0308 s\n",
            "loss: 2.123 (epoch: 16, step: 250) // Avg time/img: 0.0307 s\n",
            "loss: 2.12 (epoch: 16, step: 300) // Avg time/img: 0.0307 s\n",
            "loss: 2.12 (epoch: 16, step: 350) // Avg time/img: 0.0309 s\n",
            "loss: 2.125 (epoch: 16, step: 400) // Avg time/img: 0.0312 s\n",
            "loss: 2.122 (epoch: 16, step: 450) // Avg time/img: 0.0311 s\n",
            "----- VALIDATING - EPOCH 16 -----\n",
            "VAL loss: 1.766 (epoch: 16, step: 0) // Avg time/img: 0.0284 s\n",
            "VAL loss: 2.081 (epoch: 16, step: 50) // Avg time/img: 0.0257 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.42\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-016.pth (epoch: 16)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 16)\n",
            "----- TRAINING - EPOCH 17 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 2.096 (epoch: 17, step: 0) // Avg time/img: 0.0446 s\n",
            "loss: 2.126 (epoch: 17, step: 50) // Avg time/img: 0.0350 s\n",
            "loss: 2.128 (epoch: 17, step: 100) // Avg time/img: 0.0354 s\n",
            "loss: 2.138 (epoch: 17, step: 150) // Avg time/img: 0.0352 s\n",
            "loss: 2.129 (epoch: 17, step: 200) // Avg time/img: 0.0352 s\n",
            "loss: 2.126 (epoch: 17, step: 250) // Avg time/img: 0.0350 s\n",
            "loss: 2.123 (epoch: 17, step: 300) // Avg time/img: 0.0352 s\n",
            "loss: 2.124 (epoch: 17, step: 350) // Avg time/img: 0.0350 s\n",
            "loss: 2.13 (epoch: 17, step: 400) // Avg time/img: 0.0351 s\n",
            "loss: 2.126 (epoch: 17, step: 450) // Avg time/img: 0.0351 s\n",
            "----- VALIDATING - EPOCH 17 -----\n",
            "VAL loss: 1.728 (epoch: 17, step: 0) // Avg time/img: 0.0370 s\n",
            "VAL loss: 2.063 (epoch: 17, step: 50) // Avg time/img: 0.0287 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.42\u001b[0m %\n",
            "save: ../save/enet_training_void_ft/model-017.pth (epoch: 17)\n",
            "----- TRAINING - EPOCH 18 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 2.333 (epoch: 18, step: 0) // Avg time/img: 0.0514 s\n",
            "loss: 2.108 (epoch: 18, step: 50) // Avg time/img: 0.0347 s\n",
            "loss: 2.104 (epoch: 18, step: 100) // Avg time/img: 0.0323 s\n",
            "loss: 2.136 (epoch: 18, step: 150) // Avg time/img: 0.0317 s\n",
            "loss: 2.126 (epoch: 18, step: 200) // Avg time/img: 0.0321 s\n",
            "loss: 2.13 (epoch: 18, step: 250) // Avg time/img: 0.0323 s\n",
            "loss: 2.123 (epoch: 18, step: 300) // Avg time/img: 0.0319 s\n",
            "loss: 2.121 (epoch: 18, step: 350) // Avg time/img: 0.0317 s\n",
            "loss: 2.118 (epoch: 18, step: 400) // Avg time/img: 0.0321 s\n",
            "loss: 2.116 (epoch: 18, step: 450) // Avg time/img: 0.0323 s\n",
            "----- VALIDATING - EPOCH 18 -----\n",
            "VAL loss: 1.771 (epoch: 18, step: 0) // Avg time/img: 0.0430 s\n",
            "VAL loss: 2.092 (epoch: 18, step: 50) // Avg time/img: 0.0280 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.33\u001b[0m %\n",
            "save: ../save/enet_training_void_ft/model-018.pth (epoch: 18)\n",
            "----- TRAINING - EPOCH 19 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 1.96 (epoch: 19, step: 0) // Avg time/img: 0.0320 s\n",
            "loss: 2.124 (epoch: 19, step: 50) // Avg time/img: 0.0340 s\n",
            "loss: 2.124 (epoch: 19, step: 100) // Avg time/img: 0.0345 s\n",
            "loss: 2.119 (epoch: 19, step: 150) // Avg time/img: 0.0346 s\n",
            "loss: 2.11 (epoch: 19, step: 200) // Avg time/img: 0.0349 s\n",
            "loss: 2.116 (epoch: 19, step: 250) // Avg time/img: 0.0343 s\n",
            "loss: 2.116 (epoch: 19, step: 300) // Avg time/img: 0.0336 s\n",
            "loss: 2.121 (epoch: 19, step: 350) // Avg time/img: 0.0334 s\n",
            "loss: 2.121 (epoch: 19, step: 400) // Avg time/img: 0.0331 s\n",
            "loss: 2.118 (epoch: 19, step: 450) // Avg time/img: 0.0331 s\n",
            "----- VALIDATING - EPOCH 19 -----\n",
            "VAL loss: 1.764 (epoch: 19, step: 0) // Avg time/img: 0.0221 s\n",
            "VAL loss: 2.085 (epoch: 19, step: 50) // Avg time/img: 0.0283 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.43\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-019.pth (epoch: 19)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 19)\n",
            "----- TRAINING - EPOCH 20 -----\n",
            "LEARNING RATE:  5.000000000000001e-07\n",
            "loss: 2.062 (epoch: 20, step: 0) // Avg time/img: 0.0330 s\n",
            "loss: 2.089 (epoch: 20, step: 50) // Avg time/img: 0.0353 s\n",
            "loss: 2.114 (epoch: 20, step: 100) // Avg time/img: 0.0334 s\n",
            "loss: 2.123 (epoch: 20, step: 150) // Avg time/img: 0.0336 s\n",
            "loss: 2.117 (epoch: 20, step: 200) // Avg time/img: 0.0341 s\n",
            "loss: 2.115 (epoch: 20, step: 250) // Avg time/img: 0.0332 s\n",
            "loss: 2.115 (epoch: 20, step: 300) // Avg time/img: 0.0327 s\n",
            "loss: 2.114 (epoch: 20, step: 350) // Avg time/img: 0.0322 s\n",
            "loss: 2.115 (epoch: 20, step: 400) // Avg time/img: 0.0322 s\n",
            "loss: 2.111 (epoch: 20, step: 450) // Avg time/img: 0.0320 s\n",
            "----- VALIDATING - EPOCH 20 -----\n",
            "VAL loss: 1.749 (epoch: 20, step: 0) // Avg time/img: 0.0293 s\n",
            "VAL loss: 2.067 (epoch: 20, step: 50) // Avg time/img: 0.0290 s\n",
            "EPOCH IoU on VAL set:  \u001b[0m19.52\u001b[0m %\n",
            "Saving model as best\n",
            "save: ../save/enet_training_void_ft/model-020.pth (epoch: 20)\n",
            "save: ../save/enet_training_void_ft/model_best.pth (epoch: 20)\n",
            "========== TRAINING FINISHED ===========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UozW3CHhS7U9"
      },
      "source": [
        "## Void Classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../eval"
      ],
      "metadata": {
        "id": "1zLSoG43dtyf",
        "outputId": "3f5143dd-d1ef-4687-d7e0-9e58222859da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"erfnet\", \"bisenet\", \"enet\"]"
      ],
      "metadata": {
        "id": "O16C6JbpmkSi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: `eval_anomaly`"
      ],
      "metadata": {
        "id": "pNJm9vHlh-UY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the `eval_anomaly` script on the models trained from scratch. All trained on the Cityscapes dataset including the 20th *void* class."
      ],
      "metadata": {
        "id": "681h44IV0tma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "  print(f\"{'=' * 64}\")\n",
        "  print(f\"MODEL: {model.upper()}\")\n",
        "  print(f\"{'=' * 64}\")\n",
        "\n",
        "  for dataset, folder in datasets.items():\n",
        "    print(f\"Dataset {dataset:<15}\", end = \"\")\n",
        "\n",
        "    input_path = f\"../../validation_dataset/{folder}/images/*.*\"\n",
        "    plot_dir_path = f\"../plots/void/{model}/{folder}\"\n",
        "    load_dir = f\"../save/{model}_training_void/\"\n",
        "    load_weights = f\"model_best.pth\"\n",
        "\n",
        "    add_cmd = \"--cpu\" if not torch.cuda.is_available() else \"\"\n",
        "    !python evalAnomaly.py --input={input_path} --loadModel={model} --loadDir={load_dir} --loadWeights={load_weights} --method=\"void\" --plotdir={plot_dir_path} {add_cmd}\n",
        "\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpXVxW2YiOiJ",
        "outputId": "8751855e-480d-4824-e2e1-45d8f0d00dee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================\n",
            "MODEL: ERFNET\n",
            "================================================================\n",
            "Dataset SMIYC RA-21    | AUPRC score: 10.950 | FPR@TPR95: 91.396\n",
            "Dataset SMIYC RO-21    | AUPRC score:  0.917 | FPR@TPR95: 87.629\n",
            "Dataset FS L&F         | AUPRC score:  5.658 | FPR@TPR95: 51.060\n",
            "Dataset  FS Static     | AUPRC score:  7.318 | FPR@TPR95: 47.542\n",
            "Dataset Road Anomaly   | AUPRC score:  6.999 | FPR@TPR95: 94.774\n",
            "\n",
            "================================================================\n",
            "MODEL: BISENET\n",
            "================================================================\n",
            "Dataset SMIYC RA-21    | AUPRC score: 25.768 | FPR@TPR95: 85.145\n",
            "Dataset SMIYC RO-21    | AUPRC score: 10.697 | FPR@TPR95: 89.987\n",
            "Dataset FS L&F         | AUPRC score: 15.784 | FPR@TPR95: 60.907\n",
            "Dataset  FS Static     | AUPRC score:  6.232 | FPR@TPR95: 86.029\n",
            "Dataset Road Anomaly   | AUPRC score: 12.276 | FPR@TPR95: 94.143\n",
            "\n",
            "================================================================\n",
            "MODEL: ENET\n",
            "================================================================\n",
            "Dataset SMIYC RA-21    | AUPRC score: 12.348 | FPR@TPR95: 84.774\n",
            "Dataset SMIYC RO-21    | AUPRC score:  1.139 | FPR@TPR95: 63.600\n",
            "Dataset FS L&F         | AUPRC score:  2.102 | FPR@TPR95: 65.472\n",
            "Dataset  FS Static     | AUPRC score:  7.246 | FPR@TPR95: 39.319\n",
            "Dataset Road Anomaly   | AUPRC score: 10.377 | FPR@TPR95: 76.357\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the `eval_anomaly` script on the fine-tuned versions of ERFNet, BiSeNet, and ENet. Compare the performance with the previous ones."
      ],
      "metadata": {
        "id": "Hmy9TCdiimV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "  print(f\"{'=' * 64}\")\n",
        "  print(f\"MODEL: {model.upper()}\")\n",
        "  print(f\"{'=' * 64}\")\n",
        "\n",
        "  for dataset, folder in datasets.items():\n",
        "    print(f\"Dataset {dataset:<15}\", end = \"\")\n",
        "\n",
        "    input_path = f\"../../validation_dataset/{folder}/images/*.*\"\n",
        "    plot_dir_path = f\"../plots/void/{model}/{folder}\"\n",
        "    load_dir = f\"../save/{model}_training_void_ft/\"\n",
        "    load_weights = f\"model_best.pth\"\n",
        "\n",
        "    add_cmd = \"--cpu\" if not torch.cuda.is_available() else \"\"\n",
        "    !python evalAnomaly.py --input={input_path} --loadModel={model} --loadDir={load_dir} --loadWeights={load_weights} --method=\"void\" --plotdir={plot_dir_path} {add_cmd}\n",
        "\n",
        "  print()"
      ],
      "metadata": {
        "id": "56LpzlPjATsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53094cf2-817d-426e-dbf3-639dd3514b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================\n",
            "MODEL: ERFNET\n",
            "================================================================\n",
            "Dataset SMIYC RA-21    | AUPRC score: 24.239 | FPR@TPR95: 68.735\n",
            "Dataset SMIYC RO-21    | AUPRC score:  1.360 | FPR@TPR95: 99.809\n",
            "Dataset FS L&F         | AUPRC score: 11.535 | FPR@TPR95: 15.749\n",
            "Dataset  FS Static     | AUPRC score: 15.785 | FPR@TPR95: 49.445\n",
            "Dataset Road Anomaly   | AUPRC score: 10.978 | FPR@TPR95: 86.823\n",
            "\n",
            "================================================================\n",
            "MODEL: BISENET\n",
            "================================================================\n",
            "Dataset SMIYC RA-21    | AUPRC score: 33.084 | FPR@TPR95: 86.463\n",
            "Dataset SMIYC RO-21    | AUPRC score: 13.214 | FPR@TPR95: 99.474\n",
            "Dataset FS L&F         | AUPRC score: 15.462 | FPR@TPR95: 52.662\n",
            "Dataset  FS Static     | AUPRC score: 30.864 | FPR@TPR95: 60.263\n",
            "Dataset Road Anomaly   | AUPRC score: 13.042 | FPR@TPR95: 93.016\n",
            "\n",
            "================================================================\n",
            "MODEL: ENET\n",
            "================================================================\n",
            "Dataset SMIYC RA-21    | AUPRC score: 21.616 | FPR@TPR95: 88.547\n",
            "Dataset SMIYC RO-21    | AUPRC score:  1.763 | FPR@TPR95: 95.070\n",
            "Dataset FS L&F         | AUPRC score:  0.624 | FPR@TPR95: 68.780\n",
            "Dataset  FS Static     | AUPRC score:  6.430 | FPR@TPR95: 68.681\n",
            "Dataset Road Anomaly   | AUPRC score: 18.122 | FPR@TPR95: 84.568\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to save the void folder in your local machine, create a ZIP file with the following command and then download it."
      ],
      "metadata": {
        "id": "nEpHELLP-XgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd ../plots\n",
        "# !zip -r void.zip void/"
      ],
      "metadata": {
        "id": "34F3Z0ml9uYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: `eval_iou`"
      ],
      "metadata": {
        "id": "IpoeUwLkiYVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the `eval_iou` script on the models trained from scratch. Set the parameter `void=True`."
      ],
      "metadata": {
        "id": "Ukfx7Y5ZoUqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "  print(f\"{'=' * 64}\")\n",
        "  print(f\"MODEL: {model.upper()}\")\n",
        "  print(f\"{'=' * 64}\")\n",
        "  load_dir = f\"../save/{model}_training_void/\"\n",
        "  training_folder = f\"model_best.pth\"\n",
        "\n",
        "  run_eval_iou(model, load_dir, training_folder, void=True)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OjMu8uQl2hy",
        "outputId": "c4e226d2-1091-4468-ba91-ba5e115c5a41"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================\n",
            "MODEL: ERFNET\n",
            "================================================================\n",
            "Loading model: ../save/erfnet_training_void/erfnet.py\n",
            "Loading weights: ../save/erfnet_training_void/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  76.34682869911194 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m91.38\u001b[0m Road\n",
            "\u001b[0m65.77\u001b[0m sidewalk\n",
            "\u001b[0m83.28\u001b[0m building\n",
            "\u001b[0m36.07\u001b[0m wall\n",
            "\u001b[0m38.81\u001b[0m fence\n",
            "\u001b[0m45.63\u001b[0m pole\n",
            "\u001b[0m40.41\u001b[0m traffic light\n",
            "\u001b[0m50.80\u001b[0m traffic sign\n",
            "\u001b[0m87.45\u001b[0m vegetation\n",
            "\u001b[0m51.84\u001b[0m terrain\n",
            "\u001b[0m86.88\u001b[0m sky\n",
            "\u001b[0m58.70\u001b[0m person\n",
            "\u001b[0m35.58\u001b[0m rider\n",
            "\u001b[0m85.85\u001b[0m car\n",
            "\u001b[0m40.57\u001b[0m truck\n",
            "\u001b[0m50.13\u001b[0m bus\n",
            "\u001b[0m28.49\u001b[0m train\n",
            "\u001b[0m25.68\u001b[0m motorcycle\n",
            "\u001b[0m54.59\u001b[0m bicycle\n",
            "\u001b[0m60.83\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m55.94\u001b[0m %\n",
            "\n",
            "================================================================\n",
            "MODEL: BISENET\n",
            "================================================================\n",
            "Loading model: ../save/bisenet_training_void/bisenet.py\n",
            "Loading weights: ../save/bisenet_training_void/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  73.1594660282135 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m92.31\u001b[0m Road\n",
            "\u001b[0m66.50\u001b[0m sidewalk\n",
            "\u001b[0m83.43\u001b[0m building\n",
            "\u001b[0m34.76\u001b[0m wall\n",
            "\u001b[0m35.86\u001b[0m fence\n",
            "\u001b[0m37.21\u001b[0m pole\n",
            "\u001b[0m42.08\u001b[0m traffic light\n",
            "\u001b[0m55.06\u001b[0m traffic sign\n",
            "\u001b[0m86.65\u001b[0m vegetation\n",
            "\u001b[0m51.33\u001b[0m terrain\n",
            "\u001b[0m87.12\u001b[0m sky\n",
            "\u001b[0m64.02\u001b[0m person\n",
            "\u001b[0m41.01\u001b[0m rider\n",
            "\u001b[0m88.21\u001b[0m car\n",
            "\u001b[0m51.31\u001b[0m truck\n",
            "\u001b[0m67.51\u001b[0m bus\n",
            "\u001b[0m52.67\u001b[0m train\n",
            "\u001b[0m31.72\u001b[0m motorcycle\n",
            "\u001b[0m58.62\u001b[0m bicycle\n",
            "\u001b[0m61.23\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m59.43\u001b[0m %\n",
            "\n",
            "================================================================\n",
            "MODEL: ENET\n",
            "================================================================\n",
            "Loading model: ../save/enet_training_void/enet.py\n",
            "Loading weights: ../save/enet_training_void/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  80.44643807411194 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m92.17\u001b[0m Road\n",
            "\u001b[0m62.30\u001b[0m sidewalk\n",
            "\u001b[0m80.05\u001b[0m building\n",
            "\u001b[0m18.72\u001b[0m wall\n",
            "\u001b[0m12.73\u001b[0m fence\n",
            "\u001b[0m32.42\u001b[0m pole\n",
            "\u001b[0m0.00\u001b[0m traffic light\n",
            "\u001b[0m34.84\u001b[0m traffic sign\n",
            "\u001b[0m85.73\u001b[0m vegetation\n",
            "\u001b[0m41.53\u001b[0m terrain\n",
            "\u001b[0m86.68\u001b[0m sky\n",
            "\u001b[0m46.96\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m82.51\u001b[0m car\n",
            "\u001b[0m21.15\u001b[0m truck\n",
            "\u001b[0m9.51\u001b[0m bus\n",
            "\u001b[0m5.34\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m40.15\u001b[0m bicycle\n",
            "\u001b[0m64.75\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m40.88\u001b[0m %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the `eval_iou` script on the fine-tuned versions of ERFNet, BiSeNet, and ENet. Compare the performance with the previous ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "q0VFu-iKqKH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in models:\n",
        "  print(f\"{'=' * 64}\")\n",
        "  print(f\"MODEL: {model.upper()}\")\n",
        "  print(f\"{'=' * 64}\")\n",
        "  load_dir = f\"../save/{model}_training_void_ft/\"\n",
        "  training_folder = f\"model_best.pth\"\n",
        "\n",
        "  run_eval_iou(model, load_dir, training_folder, void=True)\n",
        "  print()"
      ],
      "metadata": {
        "id": "pIgvMlwwJTW7",
        "outputId": "c3bddce8-2e82-4065-cf41-f34eee449316",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================\n",
            "MODEL: ERFNET\n",
            "================================================================\n",
            "Loading model: ../save/erfnet_training_void_ft/erfnet.py\n",
            "Loading weights: ../save/erfnet_training_void_ft/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  73.02247381210327 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m83.17\u001b[0m Road\n",
            "\u001b[0m66.71\u001b[0m sidewalk\n",
            "\u001b[0m83.86\u001b[0m building\n",
            "\u001b[0m35.33\u001b[0m wall\n",
            "\u001b[0m43.69\u001b[0m fence\n",
            "\u001b[0m55.00\u001b[0m pole\n",
            "\u001b[0m56.69\u001b[0m traffic light\n",
            "\u001b[0m62.44\u001b[0m traffic sign\n",
            "\u001b[0m88.13\u001b[0m vegetation\n",
            "\u001b[0m45.99\u001b[0m terrain\n",
            "\u001b[0m86.45\u001b[0m sky\n",
            "\u001b[0m68.59\u001b[0m person\n",
            "\u001b[0m52.79\u001b[0m rider\n",
            "\u001b[0m87.17\u001b[0m car\n",
            "\u001b[0m66.92\u001b[0m truck\n",
            "\u001b[0m74.83\u001b[0m bus\n",
            "\u001b[0m50.83\u001b[0m train\n",
            "\u001b[0m37.98\u001b[0m motorcycle\n",
            "\u001b[0m63.46\u001b[0m bicycle\n",
            "\u001b[0m14.08\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m61.21\u001b[0m %\n",
            "\n",
            "================================================================\n",
            "MODEL: BISENET\n",
            "================================================================\n",
            "Loading model: ../save/bisenet_training_void_ft/bisenet.py\n",
            "Loading weights: ../save/bisenet_training_void_ft/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  70.93619394302368 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m86.76\u001b[0m Road\n",
            "\u001b[0m63.12\u001b[0m sidewalk\n",
            "\u001b[0m83.16\u001b[0m building\n",
            "\u001b[0m42.88\u001b[0m wall\n",
            "\u001b[0m37.57\u001b[0m fence\n",
            "\u001b[0m44.78\u001b[0m pole\n",
            "\u001b[0m49.69\u001b[0m traffic light\n",
            "\u001b[0m58.74\u001b[0m traffic sign\n",
            "\u001b[0m86.15\u001b[0m vegetation\n",
            "\u001b[0m50.23\u001b[0m terrain\n",
            "\u001b[0m86.78\u001b[0m sky\n",
            "\u001b[0m66.56\u001b[0m person\n",
            "\u001b[0m48.65\u001b[0m rider\n",
            "\u001b[0m89.20\u001b[0m car\n",
            "\u001b[0m66.76\u001b[0m truck\n",
            "\u001b[0m70.13\u001b[0m bus\n",
            "\u001b[0m52.07\u001b[0m train\n",
            "\u001b[0m42.47\u001b[0m motorcycle\n",
            "\u001b[0m60.41\u001b[0m bicycle\n",
            "\u001b[0m38.62\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m61.24\u001b[0m %\n",
            "\n",
            "================================================================\n",
            "MODEL: ENET\n",
            "================================================================\n",
            "Loading model: ../save/enet_training_void_ft/enet.py\n",
            "Loading weights: ../save/enet_training_void_ft/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  75.40101766586304 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m63.02\u001b[0m Road\n",
            "\u001b[0m6.56\u001b[0m sidewalk\n",
            "\u001b[0m47.31\u001b[0m building\n",
            "\u001b[0m0.46\u001b[0m wall\n",
            "\u001b[0m7.55\u001b[0m fence\n",
            "\u001b[0m12.21\u001b[0m pole\n",
            "\u001b[0m2.15\u001b[0m traffic light\n",
            "\u001b[0m6.04\u001b[0m traffic sign\n",
            "\u001b[0m67.44\u001b[0m vegetation\n",
            "\u001b[0m2.81\u001b[0m terrain\n",
            "\u001b[0m49.41\u001b[0m sky\n",
            "\u001b[0m0.00\u001b[0m person\n",
            "\u001b[0m0.65\u001b[0m rider\n",
            "\u001b[0m53.33\u001b[0m car\n",
            "\u001b[0m0.68\u001b[0m truck\n",
            "\u001b[0m4.61\u001b[0m bus\n",
            "\u001b[0m5.65\u001b[0m train\n",
            "\u001b[0m0.01\u001b[0m motorcycle\n",
            "\u001b[0m11.64\u001b[0m bicycle\n",
            "\u001b[0m7.35\u001b[0m void\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m17.44\u001b[0m %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ The scratch-trained model shows a much better IoU on the void class (61.23 and 64.75 against 38.62 and 7.35), suggesting that starting from scratch may lead to stronger learning of unannotated/anomalous regions—possibly due to no bias from pretrained features."
      ],
      "metadata": {
        "id": "ANhvI53coMJ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n2SPIAyvU5g"
      },
      "source": [
        "## Effect of Training Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkwRPEbyVkKW"
      },
      "source": [
        "Analyze the effect of the training model along with losses that are specifically made for anomaly detection."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "VFHinGfUNwoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_erfnet_with_loss(base_dir: str, data_dir: str, loss: str, stop_epoch: int, num_epochs: int = 20,\n",
        "                           batch_size: int = 6, resume: bool = False, logit_norm: bool = False,\n",
        "                           iso_max: bool = False, class_weights: str = 'hard') -> None:\n",
        "\n",
        "    resume_flag = \"--resume\" if resume else \"\"\n",
        "    model = \"erfnet_isomaxplus\" if iso_max else \"erfnet\"\n",
        "    logit_suffix = \"_logit_norm\" if logit_norm else \"\"\n",
        "    logit_norm_flag = \"--logit_norm\" if logit_norm else \"\"\n",
        "    pretrained_encoder = \"../trained_models/erfnet_encoder_pretrained.pth.tar\"\n",
        "\n",
        "    !cd {base_dir} && python -W ignore main_v2.py \\\n",
        "      --savedir {model}_training_{loss}{logit_suffix}_final \\\n",
        "      --loss {loss} \\\n",
        "      --datadir {data_dir} \\\n",
        "      --model {model} \\\n",
        "      --cuda \\\n",
        "      --num-epochs={num_epochs} \\\n",
        "      --epochs-save=1 \\\n",
        "      --stop-epoch={stop_epoch} \\\n",
        "      --batch-size={batch_size} \\\n",
        "      {resume_flag} \\\n",
        "      {logit_norm_flag} \\\n",
        "      --decoder \\\n",
        "      --pretrainedEncoder={pretrained_encoder}"
      ],
      "metadata": {
        "id": "czCY94okoLWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different combinations of loss functions are experimented here."
      ],
      "metadata": {
        "id": "5Mga6fyvBlDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy"
      ],
      "metadata": {
        "id": "AlTcw1OiFVYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, loss=\"ce\")\n",
        "# !zip -r erfnet_training_ce.zip erfnet_training_ce/"
      ],
      "metadata": {
        "id": "LajyhEnOFX0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Focal Loss\n"
      ],
      "metadata": {
        "id": "hAHfpC8m2gpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, loss=\"f\")\n",
        "# !zip -r erfnet_training_f.zip erfnet_training_f/"
      ],
      "metadata": {
        "id": "KFvlW9ib2lit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy + Focal Loss"
      ],
      "metadata": {
        "id": "O4y689zwu8Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, loss=\"cef\")\n",
        "# !zip -r erfnet_training_ce_focal.zip erfnet_training_ce_focal/"
      ],
      "metadata": {
        "id": "usIMDNMxr748"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy + Logit Norm"
      ],
      "metadata": {
        "id": "5N439ifLw080"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, loss=\"ce\", logit_norm=True)\n",
        "# !zip -r erfnet_training_ce_logitnorm.zip erfnet_training_ce_logitnorm/"
      ],
      "metadata": {
        "id": "Q7OxFry4w4Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Loss + Focal + Logit Norm\n",
        "\n"
      ],
      "metadata": {
        "id": "DIpiNsBrqtib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, loss=\"cef\", logit_norm=True)\n",
        "# !zip -r erfnet_training_ce_focal_logitnorm.zip erfnet_training_ce_focal_logitnorm/"
      ],
      "metadata": {
        "id": "81l27VnRq_Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy + EIM"
      ],
      "metadata": {
        "id": "tF-Dqvu_yk9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, stop_epoch = 10, loss=\"ceim\", iso_max=True)\n",
        "# !zip -r erfnet_isomaxplus_training_ceim.zip erfnet_isomaxplus_training_ceim/"
      ],
      "metadata": {
        "id": "H6hWHrh3yol-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, stop_epoch = 20, loss=\"ceim\", iso_max=True, resume=True)"
      ],
      "metadata": {
        "id": "iF_R4WkYvqcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Focal + EIM"
      ],
      "metadata": {
        "id": "5xm7UsZ_JicM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, stop_epoch = 10, loss=\"feim\", iso_max=True)\n",
        "# !zip -r erfnet_isomaxplus_training_feim.zip erfnet_isomaxplus_training_feim/"
      ],
      "metadata": {
        "id": "hZJvLD01Igfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy + Focal + EIM"
      ],
      "metadata": {
        "id": "RuozrCquq19w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, stop_epoch = 10, loss=\"cefeim\", iso_max=True)\n",
        "# !zip -r erfnet_isomaxplus_training_cefeim.zip erfnet_isomaxplus_training_cefeim/"
      ],
      "metadata": {
        "id": "JO1uYD7DrC2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, stop_epoch = 20, loss=\"cefeim\", iso_max=True, resume=True)"
      ],
      "metadata": {
        "id": "mTiy4vbRsr44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy + Focal + EIM + Logit Norm"
      ],
      "metadata": {
        "id": "UmFeo57ErGNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, stop_epoch=10, loss=\"cefeim\", logit_norm=True, iso_max=True)\n",
        "# !zip -r erfnet_isomaxplus_training_cefeim_logit_norm.zip erfnet_isomaxplus_training_cefeim_logit_norm/"
      ],
      "metadata": {
        "id": "7QYGUg2arK6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_erfnet_with_loss(base_dir, data_dir, stop_epoch=20, loss=\"cefeim\", logit_norm=True, iso_max=True, resume=True)"
      ],
      "metadata": {
        "id": "ol8UQrGLvvi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zIp0mbgKafh"
      },
      "source": [
        "# Inference with different training losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dir = \"../save/\""
      ],
      "metadata": {
        "id": "y6L6-QrpHi33"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation_per_loss(losses):\n",
        "  for loss, (model, training_folder) in losses.items():\n",
        "    print(f\"ERFNet {loss.upper()}\")\n",
        "    plot_folder = training_folder.split(\"training_\")[1]\n",
        "    run_eval_anomaly(datasets, methods, model, training_folder, plot_folder, load_dir)\n",
        "    print()\n",
        "    load_dir_for_iou = f\"../save/{training_folder}/\"\n",
        "    model_best = f\"model_best.pth\"\n",
        "    run_eval_iou(model, load_dir_for_iou, model_best)\n",
        "    print()"
      ],
      "metadata": {
        "id": "DHrSZ-p3G6VM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the ERFNet model using various loss functions, such as Cross-Entropy, Focal Loss, Logit Normalization, and their combinations."
      ],
      "metadata": {
        "id": "idlZWXVZvEA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = {\"Cross-Entropy\": [\"erfnet\", \"erfnet_training_ce\"],\n",
        "          \"Focal\": [\"erfnet\", \"erfnet_training_f\"],\n",
        "          \"Cross-Entropy + Focal\": [\"erfnet\", \"erfnet_training_cef\"],\n",
        "          \"Cross-Entropy + LogitNorm\": [\"erfnet\", \"erfnet_training_ce_logit_norm\"],\n",
        "          \"Cross-Entropy + Focal + LogitNorm\": [\"erfnet\", \"erfnet_training_cef_logit_norm\"]}\n",
        "\n",
        "run_evaluation_per_loss(losses)"
      ],
      "metadata": {
        "id": "fmfRG8TNIr20",
        "outputId": "6f5bd7b6-9c12-4acb-8ad6-3c1a1cc42ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERFNet CROSS-ENTROPY\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 48.367 | FPR@TPR95: 45.054\n",
            " - MaxLogit   | AUPRC score: 54.100 | FPR@TPR95: 39.153\n",
            " - MaxEntropy | AUPRC score: 52.062 | FPR@TPR95: 44.448\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score:  9.054 | FPR@TPR95: 11.486\n",
            " - MaxLogit   | AUPRC score:  9.520 | FPR@TPR95: 10.512\n",
            " - MaxEntropy | AUPRC score:  9.609 | FPR@TPR95: 11.344\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  3.003 | FPR@TPR95: 59.098\n",
            " - MaxLogit   | AUPRC score:  3.377 | FPR@TPR95: 51.787\n",
            " - MaxEntropy | AUPRC score:  4.629 | FPR@TPR95: 58.176\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score:  8.448 | FPR@TPR95: 38.720\n",
            " - MaxLogit   | AUPRC score:  7.949 | FPR@TPR95: 48.521\n",
            " - MaxEntropy | AUPRC score:  9.671 | FPR@TPR95: 38.744\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 18.544 | FPR@TPR95: 66.442\n",
            " - MaxLogit   | AUPRC score: 20.919 | FPR@TPR95: 58.536\n",
            " - MaxEntropy | AUPRC score: 20.075 | FPR@TPR95: 66.105\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_training_ce/erfnet.py\n",
            "Loading weights: ../save/erfnet_training_ce/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  77.17487454414368 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.15\u001b[0m Road\n",
            "\u001b[0m78.09\u001b[0m sidewalk\n",
            "\u001b[0m89.21\u001b[0m building\n",
            "\u001b[0m45.12\u001b[0m wall\n",
            "\u001b[0m49.73\u001b[0m fence\n",
            "\u001b[0m53.74\u001b[0m pole\n",
            "\u001b[0m54.47\u001b[0m traffic light\n",
            "\u001b[0m65.15\u001b[0m traffic sign\n",
            "\u001b[0m89.96\u001b[0m vegetation\n",
            "\u001b[0m58.10\u001b[0m terrain\n",
            "\u001b[0m92.27\u001b[0m sky\n",
            "\u001b[0m70.27\u001b[0m person\n",
            "\u001b[0m47.19\u001b[0m rider\n",
            "\u001b[0m91.46\u001b[0m car\n",
            "\u001b[0m39.81\u001b[0m truck\n",
            "\u001b[0m30.78\u001b[0m bus\n",
            "\u001b[0m33.72\u001b[0m train\n",
            "\u001b[0m39.84\u001b[0m motorcycle\n",
            "\u001b[0m67.26\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m62.75\u001b[0m %\n",
            "\n",
            "ERFNet FOCAL\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 37.485 | FPR@TPR95: 54.910\n",
            " - MaxLogit   | AUPRC score: 43.686 | FPR@TPR95: 62.028\n",
            " - MaxEntropy | AUPRC score: 44.462 | FPR@TPR95: 52.363\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 20.835 | FPR@TPR95:  5.820\n",
            " - MaxLogit   | AUPRC score: 22.953 | FPR@TPR95:  7.701\n",
            " - MaxEntropy | AUPRC score: 31.205 | FPR@TPR95:  4.850\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  2.325 | FPR@TPR95: 75.484\n",
            " - MaxLogit   | AUPRC score:  2.337 | FPR@TPR95: 72.971\n",
            " - MaxEntropy | AUPRC score:  4.557 | FPR@TPR95: 74.240\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score:  8.493 | FPR@TPR95: 38.361\n",
            " - MaxLogit   | AUPRC score: 11.194 | FPR@TPR95: 33.608\n",
            " - MaxEntropy | AUPRC score: 11.298 | FPR@TPR95: 35.541\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 18.207 | FPR@TPR95: 68.824\n",
            " - MaxLogit   | AUPRC score: 25.019 | FPR@TPR95: 56.264\n",
            " - MaxEntropy | AUPRC score: 22.084 | FPR@TPR95: 66.254\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_training_f/erfnet.py\n",
            "Loading weights: ../save/erfnet_training_f/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  78.36598634719849 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.01\u001b[0m Road\n",
            "\u001b[0m75.46\u001b[0m sidewalk\n",
            "\u001b[0m88.68\u001b[0m building\n",
            "\u001b[0m45.83\u001b[0m wall\n",
            "\u001b[0m47.49\u001b[0m fence\n",
            "\u001b[0m51.26\u001b[0m pole\n",
            "\u001b[0m53.16\u001b[0m traffic light\n",
            "\u001b[0m63.19\u001b[0m traffic sign\n",
            "\u001b[0m89.40\u001b[0m vegetation\n",
            "\u001b[0m56.45\u001b[0m terrain\n",
            "\u001b[0m91.53\u001b[0m sky\n",
            "\u001b[0m68.47\u001b[0m person\n",
            "\u001b[0m44.37\u001b[0m rider\n",
            "\u001b[0m90.78\u001b[0m car\n",
            "\u001b[0m55.89\u001b[0m truck\n",
            "\u001b[0m59.27\u001b[0m bus\n",
            "\u001b[0m40.98\u001b[0m train\n",
            "\u001b[0m40.53\u001b[0m motorcycle\n",
            "\u001b[0m65.18\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m64.42\u001b[0m %\n",
            "\n",
            "ERFNet CROSS-ENTROPY + FOCAL\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 41.415 | FPR@TPR95: 55.714\n",
            " - MaxLogit   | AUPRC score: 42.035 | FPR@TPR95: 55.782\n",
            " - MaxEntropy | AUPRC score: 45.177 | FPR@TPR95: 55.524\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 11.890 | FPR@TPR95: 11.450\n",
            " - MaxLogit   | AUPRC score: 14.267 | FPR@TPR95: 11.690\n",
            " - MaxEntropy | AUPRC score: 14.888 | FPR@TPR95: 11.412\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  3.716 | FPR@TPR95: 54.579\n",
            " - MaxLogit   | AUPRC score:  2.146 | FPR@TPR95: 55.854\n",
            " - MaxEntropy | AUPRC score:  6.252 | FPR@TPR95: 54.157\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score:  8.131 | FPR@TPR95: 38.183\n",
            " - MaxLogit   | AUPRC score:  8.444 | FPR@TPR95: 34.208\n",
            " - MaxEntropy | AUPRC score:  9.588 | FPR@TPR95: 37.760\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 15.467 | FPR@TPR95: 68.555\n",
            " - MaxLogit   | AUPRC score: 15.901 | FPR@TPR95: 65.429\n",
            " - MaxEntropy | AUPRC score: 16.573 | FPR@TPR95: 68.251\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_training_cef/erfnet.py\n",
            "Loading weights: ../save/erfnet_training_cef/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  76.98322582244873 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.46\u001b[0m Road\n",
            "\u001b[0m77.17\u001b[0m sidewalk\n",
            "\u001b[0m88.80\u001b[0m building\n",
            "\u001b[0m42.67\u001b[0m wall\n",
            "\u001b[0m47.77\u001b[0m fence\n",
            "\u001b[0m51.95\u001b[0m pole\n",
            "\u001b[0m53.88\u001b[0m traffic light\n",
            "\u001b[0m64.39\u001b[0m traffic sign\n",
            "\u001b[0m89.65\u001b[0m vegetation\n",
            "\u001b[0m57.42\u001b[0m terrain\n",
            "\u001b[0m91.58\u001b[0m sky\n",
            "\u001b[0m69.61\u001b[0m person\n",
            "\u001b[0m45.01\u001b[0m rider\n",
            "\u001b[0m91.14\u001b[0m car\n",
            "\u001b[0m52.95\u001b[0m truck\n",
            "\u001b[0m49.53\u001b[0m bus\n",
            "\u001b[0m38.61\u001b[0m train\n",
            "\u001b[0m33.88\u001b[0m motorcycle\n",
            "\u001b[0m66.46\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m63.63\u001b[0m %\n",
            "\n",
            "ERFNet CROSS-ENTROPY + LOGITNORM\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 36.093 | FPR@TPR95: 47.531\n",
            " - MaxLogit   | AUPRC score: 48.474 | FPR@TPR95: 46.420\n",
            " - MaxEntropy | AUPRC score: 37.939 | FPR@TPR95: 47.013\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 10.391 | FPR@TPR95: 19.412\n",
            " - MaxLogit   | AUPRC score: 16.127 | FPR@TPR95: 18.533\n",
            " - MaxEntropy | AUPRC score: 11.777 | FPR@TPR95: 19.504\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  6.349 | FPR@TPR95: 49.044\n",
            " - MaxLogit   | AUPRC score:  4.376 | FPR@TPR95: 56.251\n",
            " - MaxEntropy | AUPRC score:  9.195 | FPR@TPR95: 48.266\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score:  9.107 | FPR@TPR95: 38.382\n",
            " - MaxLogit   | AUPRC score:  9.149 | FPR@TPR95: 39.494\n",
            " - MaxEntropy | AUPRC score: 11.434 | FPR@TPR95: 38.184\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 17.002 | FPR@TPR95: 68.158\n",
            " - MaxLogit   | AUPRC score: 18.763 | FPR@TPR95: 60.948\n",
            " - MaxEntropy | AUPRC score: 17.399 | FPR@TPR95: 68.132\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_training_ce_logit_norm/erfnet.py\n",
            "Loading weights: ../save/erfnet_training_ce_logit_norm/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  77.29137063026428 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.08\u001b[0m Road\n",
            "\u001b[0m77.96\u001b[0m sidewalk\n",
            "\u001b[0m89.40\u001b[0m building\n",
            "\u001b[0m48.44\u001b[0m wall\n",
            "\u001b[0m48.46\u001b[0m fence\n",
            "\u001b[0m53.79\u001b[0m pole\n",
            "\u001b[0m53.78\u001b[0m traffic light\n",
            "\u001b[0m65.04\u001b[0m traffic sign\n",
            "\u001b[0m90.22\u001b[0m vegetation\n",
            "\u001b[0m60.07\u001b[0m terrain\n",
            "\u001b[0m92.31\u001b[0m sky\n",
            "\u001b[0m70.24\u001b[0m person\n",
            "\u001b[0m46.15\u001b[0m rider\n",
            "\u001b[0m91.58\u001b[0m car\n",
            "\u001b[0m55.15\u001b[0m truck\n",
            "\u001b[0m55.14\u001b[0m bus\n",
            "\u001b[0m35.21\u001b[0m train\n",
            "\u001b[0m37.95\u001b[0m motorcycle\n",
            "\u001b[0m66.66\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m64.93\u001b[0m %\n",
            "\n",
            "ERFNet CROSS-ENTROPY + FOCAL + LOGITNORM\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 44.802 | FPR@TPR95: 58.457\n",
            " - MaxLogit   | AUPRC score: 54.712 | FPR@TPR95: 47.676\n",
            " - MaxEntropy | AUPRC score: 49.334 | FPR@TPR95: 57.904\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 26.276 | FPR@TPR95:  6.919\n",
            " - MaxLogit   | AUPRC score: 20.610 | FPR@TPR95:  7.444\n",
            " - MaxEntropy | AUPRC score: 27.854 | FPR@TPR95:  6.860\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  7.260 | FPR@TPR95: 45.452\n",
            " - MaxLogit   | AUPRC score:  2.668 | FPR@TPR95: 44.527\n",
            " - MaxEntropy | AUPRC score: 13.048 | FPR@TPR95: 45.276\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score:  9.656 | FPR@TPR95: 32.465\n",
            " - MaxLogit   | AUPRC score:  9.719 | FPR@TPR95: 25.432\n",
            " - MaxEntropy | AUPRC score: 13.122 | FPR@TPR95: 31.989\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 20.245 | FPR@TPR95: 64.278\n",
            " - MaxLogit   | AUPRC score: 22.655 | FPR@TPR95: 57.957\n",
            " - MaxEntropy | AUPRC score: 22.064 | FPR@TPR95: 63.866\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_training_cef_logit_norm/erfnet.py\n",
            "Loading weights: ../save/erfnet_training_cef_logit_norm/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  75.27469229698181 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.18\u001b[0m Road\n",
            "\u001b[0m77.43\u001b[0m sidewalk\n",
            "\u001b[0m88.48\u001b[0m building\n",
            "\u001b[0m48.25\u001b[0m wall\n",
            "\u001b[0m47.77\u001b[0m fence\n",
            "\u001b[0m51.02\u001b[0m pole\n",
            "\u001b[0m52.16\u001b[0m traffic light\n",
            "\u001b[0m63.75\u001b[0m traffic sign\n",
            "\u001b[0m89.52\u001b[0m vegetation\n",
            "\u001b[0m57.91\u001b[0m terrain\n",
            "\u001b[0m90.85\u001b[0m sky\n",
            "\u001b[0m69.64\u001b[0m person\n",
            "\u001b[0m46.25\u001b[0m rider\n",
            "\u001b[0m90.49\u001b[0m car\n",
            "\u001b[0m50.12\u001b[0m truck\n",
            "\u001b[0m50.09\u001b[0m bus\n",
            "\u001b[0m24.85\u001b[0m train\n",
            "\u001b[0m39.70\u001b[0m motorcycle\n",
            "\u001b[0m65.69\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m63.17\u001b[0m %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate ERFNet with EIM (Enhanced Isotropy Maximation) loss. The variant `erfnet_isomaxplus` is used here."
      ],
      "metadata": {
        "id": "RPgznr68vVma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = {\"CrossEntropy + EIM\": [\"erfnet_isomaxplus\", \"erfnet_isomaxplus_training_ceim\"],\n",
        "          \"Focal + EIM\": [\"erfnet_isomaxplus\", \"erfnet_isomaxplus_training_feim\"],\n",
        "          \"CrossEntropy + Focal + EIM\": [\"erfnet_isomaxplus\", \"erfnet_isomaxplus_training_cefeim\"],\n",
        "          \"CrossEntropy + Focal + EIM + LogitNorm\": [\"erfnet_isomaxplus\", \"erfnet_isomaxplus_training_cefeim_logit_norm\"]}\n",
        "\n",
        "run_evaluation_per_loss(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVSPBHKVsPLQ",
        "outputId": "af473a52-f5bd-44f0-ac54-b3f031c65582"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERFNet CROSSENTROPY + EIM\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 46.023 | FPR@TPR95: 82.052\n",
            " - MaxLogit   | AUPRC score: 37.634 | FPR@TPR95: 89.546\n",
            " - MaxEntropy | AUPRC score: 45.868 | FPR@TPR95: 81.474\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 22.832 | FPR@TPR95: 12.967\n",
            " - MaxLogit   | AUPRC score: 11.251 | FPR@TPR95: 45.394\n",
            " - MaxEntropy | AUPRC score: 24.655 | FPR@TPR95: 12.841\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  6.445 | FPR@TPR95: 61.162\n",
            " - MaxLogit   | AUPRC score:  6.515 | FPR@TPR95: 47.653\n",
            " - MaxEntropy | AUPRC score:  6.113 | FPR@TPR95: 59.712\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score: 12.735 | FPR@TPR95: 31.364\n",
            " - MaxLogit   | AUPRC score: 14.345 | FPR@TPR95: 33.941\n",
            " - MaxEntropy | AUPRC score: 12.769 | FPR@TPR95: 30.922\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 20.796 | FPR@TPR95: 69.273\n",
            " - MaxLogit   | AUPRC score: 19.166 | FPR@TPR95: 93.703\n",
            " - MaxEntropy | AUPRC score: 20.984 | FPR@TPR95: 68.918\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_isomaxplus_training_ceim/erfnet.py\n",
            "Loading weights: ../save/erfnet_isomaxplus_training_ceim/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  76.19119358062744 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m95.97\u001b[0m Road\n",
            "\u001b[0m75.50\u001b[0m sidewalk\n",
            "\u001b[0m88.56\u001b[0m building\n",
            "\u001b[0m47.32\u001b[0m wall\n",
            "\u001b[0m44.18\u001b[0m fence\n",
            "\u001b[0m46.16\u001b[0m pole\n",
            "\u001b[0m43.78\u001b[0m traffic light\n",
            "\u001b[0m59.74\u001b[0m traffic sign\n",
            "\u001b[0m89.44\u001b[0m vegetation\n",
            "\u001b[0m56.21\u001b[0m terrain\n",
            "\u001b[0m92.29\u001b[0m sky\n",
            "\u001b[0m68.31\u001b[0m person\n",
            "\u001b[0m37.09\u001b[0m rider\n",
            "\u001b[0m89.48\u001b[0m car\n",
            "\u001b[0m17.32\u001b[0m truck\n",
            "\u001b[0m30.38\u001b[0m bus\n",
            "\u001b[0m22.49\u001b[0m train\n",
            "\u001b[0m16.56\u001b[0m motorcycle\n",
            "\u001b[0m61.96\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m56.99\u001b[0m %\n",
            "\n",
            "ERFNet FOCAL + EIM\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 59.911 | FPR@TPR95: 52.233\n",
            " - MaxLogit   | AUPRC score: 56.706 | FPR@TPR95: 53.345\n",
            " - MaxEntropy | AUPRC score: 58.708 | FPR@TPR95: 43.802\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 14.233 | FPR@TPR95: 17.594\n",
            " - MaxLogit   | AUPRC score:  9.577 | FPR@TPR95: 18.503\n",
            " - MaxEntropy | AUPRC score: 11.432 | FPR@TPR95: 14.847\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  7.568 | FPR@TPR95: 65.506\n",
            " - MaxLogit   | AUPRC score:  9.589 | FPR@TPR95: 35.495\n",
            " - MaxEntropy | AUPRC score: 12.421 | FPR@TPR95: 65.239\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score: 14.640 | FPR@TPR95: 30.919\n",
            " - MaxLogit   | AUPRC score: 21.067 | FPR@TPR95: 26.309\n",
            " - MaxEntropy | AUPRC score: 19.327 | FPR@TPR95: 25.004\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 19.415 | FPR@TPR95: 68.645\n",
            " - MaxLogit   | AUPRC score: 19.651 | FPR@TPR95: 73.420\n",
            " - MaxEntropy | AUPRC score: 20.660 | FPR@TPR95: 66.848\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_isomaxplus_training_feim/erfnet.py\n",
            "Loading weights: ../save/erfnet_isomaxplus_training_feim/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  74.18534755706787 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m95.89\u001b[0m Road\n",
            "\u001b[0m77.32\u001b[0m sidewalk\n",
            "\u001b[0m88.86\u001b[0m building\n",
            "\u001b[0m47.06\u001b[0m wall\n",
            "\u001b[0m47.12\u001b[0m fence\n",
            "\u001b[0m49.91\u001b[0m pole\n",
            "\u001b[0m52.41\u001b[0m traffic light\n",
            "\u001b[0m65.31\u001b[0m traffic sign\n",
            "\u001b[0m89.82\u001b[0m vegetation\n",
            "\u001b[0m56.94\u001b[0m terrain\n",
            "\u001b[0m92.32\u001b[0m sky\n",
            "\u001b[0m69.47\u001b[0m person\n",
            "\u001b[0m43.80\u001b[0m rider\n",
            "\u001b[0m91.13\u001b[0m car\n",
            "\u001b[0m28.31\u001b[0m truck\n",
            "\u001b[0m34.15\u001b[0m bus\n",
            "\u001b[0m20.40\u001b[0m train\n",
            "\u001b[0m28.22\u001b[0m motorcycle\n",
            "\u001b[0m65.94\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m60.23\u001b[0m %\n",
            "\n",
            "ERFNet CROSSENTROPY + FOCAL + EIM\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 53.533 | FPR@TPR95: 54.502\n",
            " - MaxLogit   | AUPRC score: 43.595 | FPR@TPR95: 58.185\n",
            " - MaxEntropy | AUPRC score: 52.282 | FPR@TPR95: 53.747\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score: 13.074 | FPR@TPR95: 18.709\n",
            " - MaxLogit   | AUPRC score:  9.410 | FPR@TPR95: 18.609\n",
            " - MaxEntropy | AUPRC score: 13.840 | FPR@TPR95: 18.621\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score:  6.198 | FPR@TPR95: 68.507\n",
            " - MaxLogit   | AUPRC score:  6.991 | FPR@TPR95: 49.329\n",
            " - MaxEntropy | AUPRC score:  8.737 | FPR@TPR95: 68.145\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score:  8.674 | FPR@TPR95: 31.429\n",
            " - MaxLogit   | AUPRC score: 10.089 | FPR@TPR95: 33.423\n",
            " - MaxEntropy | AUPRC score: 10.094 | FPR@TPR95: 31.096\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score: 21.654 | FPR@TPR95: 70.301\n",
            " - MaxLogit   | AUPRC score: 19.870 | FPR@TPR95: 66.516\n",
            " - MaxEntropy | AUPRC score: 21.661 | FPR@TPR95: 70.426\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_isomaxplus_training_cefeim/erfnet.py\n",
            "Loading weights: ../save/erfnet_isomaxplus_training_cefeim/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  73.271235704422 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.08\u001b[0m Road\n",
            "\u001b[0m74.60\u001b[0m sidewalk\n",
            "\u001b[0m88.67\u001b[0m building\n",
            "\u001b[0m49.66\u001b[0m wall\n",
            "\u001b[0m45.39\u001b[0m fence\n",
            "\u001b[0m49.37\u001b[0m pole\n",
            "\u001b[0m53.55\u001b[0m traffic light\n",
            "\u001b[0m64.23\u001b[0m traffic sign\n",
            "\u001b[0m89.47\u001b[0m vegetation\n",
            "\u001b[0m54.51\u001b[0m terrain\n",
            "\u001b[0m91.81\u001b[0m sky\n",
            "\u001b[0m69.70\u001b[0m person\n",
            "\u001b[0m43.56\u001b[0m rider\n",
            "\u001b[0m90.90\u001b[0m car\n",
            "\u001b[0m34.85\u001b[0m truck\n",
            "\u001b[0m22.21\u001b[0m bus\n",
            "\u001b[0m22.81\u001b[0m train\n",
            "\u001b[0m35.53\u001b[0m motorcycle\n",
            "\u001b[0m65.58\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m60.13\u001b[0m %\n",
            "\n",
            "ERFNet CROSSENTROPY + FOCAL + EIM + LOGITNORM\n",
            "Dataset SMIYC RA-21\n",
            " - MSP        | AUPRC score: 26.173 | FPR@TPR95: 94.783\n",
            " - MaxLogit   | AUPRC score: 25.120 | FPR@TPR95: 94.822\n",
            " - MaxEntropy | AUPRC score: 21.299 | FPR@TPR95: 94.044\n",
            "=======================================================\n",
            "Dataset SMIYC RO-21\n",
            " - MSP        | AUPRC score:  0.667 | FPR@TPR95: 99.468\n",
            " - MaxLogit   | AUPRC score:  0.657 | FPR@TPR95: 99.470\n",
            " - MaxEntropy | AUPRC score:  0.589 | FPR@TPR95: 99.464\n",
            "=======================================================\n",
            "Dataset FS L&F\n",
            " - MSP        | AUPRC score: 20.409 | FPR@TPR95: 52.137\n",
            " - MaxLogit   | AUPRC score: 20.064 | FPR@TPR95: 57.528\n",
            " - MaxEntropy | AUPRC score: 21.229 | FPR@TPR95: 57.524\n",
            "=======================================================\n",
            "Dataset  FS Static\n",
            " - MSP        | AUPRC score: 10.008 | FPR@TPR95: 83.017\n",
            " - MaxLogit   | AUPRC score: 10.380 | FPR@TPR95: 77.004\n",
            " - MaxEntropy | AUPRC score:  9.850 | FPR@TPR95: 82.872\n",
            "=======================================================\n",
            "Dataset Road Anomaly\n",
            " - MSP        | AUPRC score:  9.458 | FPR@TPR95: 99.769\n",
            " - MaxLogit   | AUPRC score:  9.363 | FPR@TPR95: 99.765\n",
            " - MaxEntropy | AUPRC score:  9.231 | FPR@TPR95: 99.769\n",
            "=======================================================\n",
            "\n",
            "Loading model: ../save/erfnet_isomaxplus_training_cefeim_logit_norm/erfnet.py\n",
            "Loading weights: ../save/erfnet_isomaxplus_training_cefeim_logit_norm/model_best.pth\n",
            "Model and weights LOADED successfully\n",
            "---------------------------------------\n",
            "Took  74.36792993545532 seconds\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.00\u001b[0m Road\n",
            "\u001b[0m76.53\u001b[0m sidewalk\n",
            "\u001b[0m88.53\u001b[0m building\n",
            "\u001b[0m19.01\u001b[0m wall\n",
            "\u001b[0m21.62\u001b[0m fence\n",
            "\u001b[0m52.59\u001b[0m pole\n",
            "\u001b[0m20.91\u001b[0m traffic light\n",
            "\u001b[0m60.47\u001b[0m traffic sign\n",
            "\u001b[0m89.82\u001b[0m vegetation\n",
            "\u001b[0m57.33\u001b[0m terrain\n",
            "\u001b[0m91.69\u001b[0m sky\n",
            "\u001b[0m68.01\u001b[0m person\n",
            "\u001b[0m0.00\u001b[0m rider\n",
            "\u001b[0m90.45\u001b[0m car\n",
            "\u001b[0m29.17\u001b[0m truck\n",
            "\u001b[0m0.28\u001b[0m bus\n",
            "\u001b[0m0.00\u001b[0m train\n",
            "\u001b[0m0.00\u001b[0m motorcycle\n",
            "\u001b[0m57.20\u001b[0m bicycle\n",
            "=======================================\n",
            "MEAN IoU:  \u001b[0m48.40\u001b[0m %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "edzDv8bwzP4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example image to color\n",
        "# Nice images: RoadAnomaly/images/28, RoadAnomaly/images/58\n",
        "input = '/content/validation_dataset/RoadAnomaly/images/58.jpg'\n",
        "\n",
        "### Baseline models ###\n",
        "for method in [\"MSP\"]: # , \"MaxLogit\", \"MaxEntropy\"]:\n",
        "  print(f\"Method: {method}\")\n",
        "  save_image_path = f'/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/plots/images/{method}'\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    !python -W ignore /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method  {method} --save-colored-anomaly {save_image_path}  | tail -n 2\n",
        "  else:\n",
        "    !python -W ignore /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/evalAnomaly.py --input {input} --method {method} --save-colored-anomaly {save_image_path} --cpu | tail -n 2\n",
        "\n",
        "!python /content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/eval/plots.py --name_dir=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/baselines/images\" --name_output=\"/content/Real-Time-Anomaly-Segmentation-for-Road-Scenes/visualization/baseline_visualization.png\""
      ],
      "metadata": {
        "id": "Pk_KXrlIzbh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble"
      ],
      "metadata": {
        "id": "qc1jT1Sm2dBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform ensemble inference using ERFNet, ENet, and BiSeNet models trained on the Cityscapes dataset. Their predictions are combined via soft voting (averaging the class probabilities) to compute the per-class IoU and the mean IoU."
      ],
      "metadata": {
        "id": "3oQsbcMs0xwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd {base_dir} && python -W ignore main_v2.py --datadir {data_dir} --savedir dummy --ensemble"
      ],
      "metadata": {
        "id": "ngOX42Uk2fVA",
        "outputId": "8d9bd42f-7a90-44d3-f1cb-b96cd05a1de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== ENSEMBLE INFERENCE ===========\n",
            "Loading ensemble models...\n",
            "../cityscapes/leftImg8bit/val\n",
            "Running ensemble inference...\n",
            "=======================================\n",
            "Per-Class IoU:\n",
            "\u001b[0m96.90\u001b[0m Road\n",
            "\u001b[0m80.40\u001b[0m Sidewalk\n",
            "\u001b[0m90.19\u001b[0m Building\n",
            "\u001b[0m49.18\u001b[0m Wall\n",
            "\u001b[0m50.09\u001b[0m Fence\n",
            "\u001b[0m53.24\u001b[0m Pole\n",
            "\u001b[0m54.39\u001b[0m Traffic Light\n",
            "\u001b[0m68.93\u001b[0m Traffic Sign\n",
            "\u001b[0m90.99\u001b[0m Vegetation\n",
            "\u001b[0m61.78\u001b[0m Terrain\n",
            "\u001b[0m93.22\u001b[0m Sky\n",
            "\u001b[0m72.65\u001b[0m Person\n",
            "\u001b[0m48.62\u001b[0m Rider\n",
            "\u001b[0m91.96\u001b[0m Car\n",
            "\u001b[0m72.45\u001b[0m Truck\n",
            "\u001b[0m79.02\u001b[0m Bus\n",
            "\u001b[0m67.49\u001b[0m Train\n",
            "\u001b[0m39.95\u001b[0m Motorcycle\n",
            "\u001b[0m68.90\u001b[0m Bicycle\n",
            "=======================================\n",
            "Ensemble MEAN IoU: \u001b[0m70.02\u001b[0m %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}